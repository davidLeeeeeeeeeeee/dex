# Jellyfish Merkle Tree (JMT) 设计与实现问答录

## 1. 为什么项目需要从现有的 stateDB 升级到 JMT？

**问题**：当前的 stateDB 已经支持快速同步和 Epoch 快照，为什么还要重构引入 JMT？

**回答**：
如果你想将项目定位为**公链**，目前的“隐式一致性”是不够的。公链需要 **Trustless（去中心化信任）**。
- **状态证明**：JMT 可以为每个区块生成一个 `StateRoot` 哈希。没有这个哈希，轻节点无法验证余额，跨链桥也无法工作。
- **防止静默漂移**：JMT 确保所有节点在逻辑执行后，如果状态有 1 bit 的不同，最终的哈希就会完全对不上，从而立即发现 Bug 或攻击。
- **高效检索与证明**：JMT 提供了极速的“存在证明”和“不存在证明”，这对于 DEX 处理撤单和防双花至关重要。

## 2. JMT 与传统 MPT (以太坊) 的区别是什么？

| 特性 | MPT (以太坊) | JMT (Diem/Aptos) |
| :--- | :--- | :--- |
| **分支因子** | 16 进制 (动态路径) | **16 叉 (固定路径分段)** |
| **磁盘 I/O** | 较高（路径可能很深且复杂） | **极低（树更加扁平，通常 7 层即可覆盖亿级数据）** |
| **版本管理** | 外部实现 | **原生支持版本化（Versioned），天然支持 MVCC** |
| **实现复杂度** | 极高（多种节点类型和压缩逻辑） | **中等（逻辑统一，对存储引擎非常友好）** |

## 3. 关于 JMT 的时间复杂度 $O(\log_{16} N)$

**问题**：更新亿级数据真的只需要 6-7 次哈希运算吗？

**回答**：
是的。因为 JMT 是 16 叉树：
- 对于 1 亿（$10^8$）规模的数据，$\log_{16}(10^8) \approx 6.6$ 层。
- 即使物理深度是 256 位，但通过**路径压缩**技术，JMT 会跳过所有空的中间节点。
- 真正影响性能的是**磁盘 I/O**。JMT 通过减少树的高度（增加宽度），将读取磁盘的次数压到了最低。

## 4. JMT 是“内存树”还是“硬盘树”？

**问题**：以太坊数据几 TB 内存占用却很小，这是怎么做到的？JMT 会撑爆内存吗？

**回答**：
**JMT 本质上是“硬盘树”。**
- **持久化**：所有的节点（Node）都作为 KV 存储在 BadgerDB 中。
- **按需加载**：只有被交易触达的路径节点才会被读入内存。
- **缓存加速**：通过内存中的 LRU Cache 缓存热点节点（如靠近根部的几层），保证 90% 的请求不触发物理磁盘 IO。
- **扁平快照**：为了极速查询余额，通常会维持一份扁平的 `Address -> Balance` 快照（Flat State），而 JMT 则专门负责生成共识所需的执行证明。

## 5. 为什么引入 JMT 后可以考虑取消“双写”？

**问题**：现在的系统写一遍主库，再写一遍 stateDB，以后还需要吗？

**回答**：
这取决于架构选择。在成熟设计中：
- **逻辑合一**：JMT 节点本身就包含了账户数据。
- **存储方案**：你可以封装一个 `StateManager`，它在底层同时更新“用于查询的扁平索引”和“用于验证的 JMT 节点”。
- **单轨制**：所有的状态变更都由 JMT 逻辑统一驱动，避免了主库（存钱）和状态树（证明）之间出现数据不一致。

## 6. 技术选型建议：自研 Go 版本还是引入 Rust 库？

- **自研 Go 代码**：
    - **优点**：100% 控制权，无 CGO 性能损耗，易于调试。
    - **周期**：约 1 个月，核心是处理好 Nibble 路径和 Proof 生成。
- **引入 Rust 库 (Penumbra/Aptos) via CGO**：
    - **优点**：工业级强度，现成的针对大并发优化的逻辑。
    - **缺点**：处理 FFI 内存管理、交叉编译比较痛苦，跨语言调用有额外开销。

**结论**：如果追求长远稳定和代码纯粹性，**自研一个适配 BadgerDB 的 Go 语言 JMT** 是最佳的公链路线。

## 7. 系统保证一致性的核心公式

**公式**：`[ Genesis 状态 ] + [ 共识确定的区块序列 ] + [ 确定性 VM ] = [ 一致的状态快照 ]`

**深度评估**：
- **方案可靠性**：对于高性能实验项目，该方案基于“隐式一致性”，极大减少了开发开销。但作为公链，它无法防范“静默状态漂移”。
- **风险点**：
    - **静默漂移**：若 VM 逻辑有微小差异（如浮点数、Map 遍历），节点间余额可能出现分分钱的差别，但系统不会报错，直到很久以后提现失败才爆发。
    - **不可信同步**：由于区块头没有快照哈希，节点无法验证从 Peer 下载的快照是否被篡改。

## 8. 公链级架构演进路线图

为了让项目达到“公链级”健壮性，建议按以下阶段演进：

### 第一阶段：协议透明化（StateRoot 引入）
- 在 `pb.Block` 中增加 `state_root` 和 `receipt_root`。
- **意义**：让共识层强制对齐执行结果，支持轻客户端（SPV）验证。

### 第二阶段：存储可验证化（JMT 集成）
- 将 `StateDB` 从扁平存储升级为 **JMT 存储**。
- **意义**：提供 $O(\log N)$ 的存在/不存在证明，支持 Trustless 状态访问。

### 第三阶段：同步安全性检查
- 在每个 Epoch 结束时，将快照整体哈希锚定在区块中。
- **意义**：确保同步过程中的快照数据不可篡改。

不用看 JMT，单看现在的 BadgerDB (KV) 和 
StateDB
 的配合，确实存在大量没必要的逻辑交叉。如果你想让这两套存储“各司其职”，存储完全不一样的内容，这在架构上是非常合理的。

以下是目前冗余的症结所在，以及如何实现完全分离的思路：

1. 现状：低效的“双重记账”
在当前代码里，一个账户的更新是这样走的：

路径 A (KV)：VM 产生 
WriteOp
 -> 调用 manager.EnqueueSet -> 存入 Badger 的 v1_account_...。
路径 B (StateDB)：VM 产生同样的 
WriteOp
 -> 调用 manager.SyncToStateDB -> 存入 StateDB。
为什么说没必要？

数据冗余：同样的账户 Proto 字节码在物理磁盘上存了两份。
IO 浪费：一次交易写两次磁盘。
维护负担：StateDB 有自己的 WAL（预写日志），Badger 也有自己的 WAL。存一次账户数据甚至涉及三四次文件系统操作。
2. 思路：让存储“存完全不一样的内容”
如果你想彻底分离，可以采用 “账本与状态分离” (Ledger vs. State) 的模型：

BadgerDB 只存：账本 (Ledger / History)
它只存储不可变的、流水式的数据。一旦存入，几乎不改。

Block：区块头。
Transaction (TxRaw)：交易原文。
Receipt：交易执行收据（日志、状态码）。
Index：各种索引（高度到哈希的映射、TxID 到高度的映射）。
Meta：比如“当前处理到的最高高度”。
StateDB (或未来的 JMT) 只存：状态 (Current State)
它只存储最新的、“世界”当前是什么样子的数据。它是高度可变的。

Account / Balance：谁有多少钱。
Nonce：每个人的交易序号。
OrderState：尚未完全成交的订单状态。
3. 分离后的数据流是如何跑的？
如果这样设计，你的 VM 执行逻辑会发生根本变化（变的更清爽）：

读取时：
VM 需要读账户余额？只去 StateDB 读。
用户想通过 API 查某笔交易详情？只去 BadgerDB 读。
写入时 (Commit)：
applyResult
 收到一堆变更。
它把其中的“余额/Nonce”打包发给 StateDB。
它把其中的“交易收据/区块信息”发给 BadgerDB。
这两者异步或并行写入，互不干扰。
4. 这种分离的好处：
存储空间减半：账户数据只有一份，不再在 Badger 里占位。
性能提升：Badger 只需要顺序写（Append-only），性能极高；而涉及复杂查找和版本控制（Epoch）的随机读写压力全部转移给专门优化的 StateDB/JMT。
职责明确：你现在查 
keys.go
 里的前缀，不会再纠结这个 Key 到底是在“账户”里还是在“交易历史”里。
5. 你的思路结论：
“两套存储存完全不一样的内容”不仅可行，而且是系统走向规范化的必经之路。

目前的交叉是因为：我们一开始把 Badger 当成了“一筐箩”，什么都往里塞；后来为了加状态根和快速回滚功能，又临时搭了一个 StateDB，但没敢把 Badger 里的“账户备份”删掉。

如果你决定要用 JMT 替代 StateDB，那么这正好是一个绝佳的解耦机会：

JMT 负责可验证的“状态”。
Badger 负责不可变、无 Merkle 证明的“历史”。
你觉得把“账户”彻底从 Badger 的通用 KV 空间里踢出去，只留给 JMT，这个步子会不会迈得太大？或者你更担心这样会导致数据恢复变得困难？