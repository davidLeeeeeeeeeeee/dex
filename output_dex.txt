
Êñá‰ª∂Ë∑ØÂæÑ: consensus/block.go
Êñá‰ª∂ÂÜÖÂÆπ:
package consensus

import "fmt"

// Âå∫ÂùóÂÆö‰πâ
type Block struct {
	ID       string
	Height   uint64
	ParentID string
	Data     string
	Proposer int
	Round    int
}

func (b *Block) String() string {
	return fmt.Sprintf("Block{ID:%s, Height:%d, Parent:%s, Proposer:%d, Round:%d}",
		b.ID, b.Height, b.ParentID, b.Proposer, b.Round)
}


Êñá‰ª∂Ë∑ØÂæÑ: consensus/blockStore.go
Êñá‰ª∂ÂÜÖÂÆπ:
package consensus

import (
	"fmt"
	"sort"
	"sync"
	"time"
)

// ============================================
// Âå∫ÂùóÂ≠òÂÇ®Êé•Âè£ÔºàÂ¢ûÂº∫ÁâàÔºåÊîØÊåÅÂø´ÁÖßÔºâ
// ============================================

type MemoryBlockStore struct {
	mu                 sync.RWMutex
	blocks             map[string]*Block
	heightIndex        map[uint64][]*Block
	lastAccepted       *Block
	lastAcceptedHeight uint64
	finalizedBlocks    map[uint64]*Block
	maxHeight          uint64

	// Âø´ÁÖßÁõ∏ÂÖ≥
	snapshots       map[uint64]*Snapshot
	snapshotHeights []uint64 // ÊúâÂ∫èÁöÑÂø´ÁÖßÈ´òÂ∫¶ÂàóË°®
	maxSnapshots    int
}

func NewMemoryBlockStore() BlockStore {
	return NewMemoryBlockStoreWithConfig(10)
}

func NewMemoryBlockStoreWithConfig(maxSnapshots int) BlockStore {
	store := &MemoryBlockStore{
		blocks:          make(map[string]*Block),
		heightIndex:     make(map[uint64][]*Block),
		finalizedBlocks: make(map[uint64]*Block),
		maxHeight:       0,
		snapshots:       make(map[uint64]*Snapshot),
		snapshotHeights: make([]uint64, 0),
		maxSnapshots:    maxSnapshots,
	}

	// Âàõ‰∏ñÂå∫Âùó
	genesis := &Block{
		ID:       "genesis",
		Height:   0,
		ParentID: "",
		Proposer: -1,
	}
	store.blocks[genesis.ID] = genesis
	store.heightIndex[0] = []*Block{genesis}
	store.lastAccepted = genesis
	store.lastAcceptedHeight = 0
	store.finalizedBlocks[0] = genesis
	store.maxHeight = 0

	return store
}

func (s *MemoryBlockStore) Add(block *Block) (bool, error) {
	s.mu.Lock()
	defer s.mu.Unlock()

	if _, exists := s.blocks[block.ID]; exists {
		return false, nil
	}

	if err := s.validateBlock(block); err != nil {
		return false, err
	}

	s.blocks[block.ID] = block
	s.heightIndex[block.Height] = append(s.heightIndex[block.Height], block)

	if block.Height > s.maxHeight {
		s.maxHeight = block.Height
	}

	return true, nil
}

func (s *MemoryBlockStore) validateBlock(block *Block) error {
	if block == nil || block.ID == "" {
		return fmt.Errorf("invalid block")
	}
	if block.Height == 0 && block.ID != "genesis" {
		return fmt.Errorf("invalid genesis block")
	}
	if block.Height > 0 && block.ParentID == "" {
		return fmt.Errorf("non-genesis block must have parent")
	}
	return nil
}

func (s *MemoryBlockStore) Get(id string) (*Block, bool) {
	s.mu.RLock()
	defer s.mu.RUnlock()
	block, exists := s.blocks[id]
	return block, exists
}

func (s *MemoryBlockStore) GetByHeight(height uint64) []*Block {
	s.mu.RLock()
	defer s.mu.RUnlock()
	blocks := s.heightIndex[height]
	result := make([]*Block, len(blocks))
	copy(result, blocks)
	return result
}

func (s *MemoryBlockStore) GetLastAccepted() (string, uint64) {
	s.mu.RLock()
	defer s.mu.RUnlock()
	return s.lastAccepted.ID, s.lastAcceptedHeight
}

func (s *MemoryBlockStore) GetFinalizedAtHeight(height uint64) (*Block, bool) {
	s.mu.RLock()
	defer s.mu.RUnlock()
	block, exists := s.finalizedBlocks[height]
	return block, exists
}

func (s *MemoryBlockStore) GetBlocksFromHeight(from, to uint64) []*Block {
	s.mu.RLock()
	defer s.mu.RUnlock()

	blocks := make([]*Block, 0)
	for h := from; h <= to && h <= s.maxHeight; h++ {
		if heightBlocks, exists := s.heightIndex[h]; exists {
			blocks = append(blocks, heightBlocks...)
		}
	}
	return blocks
}

func (s *MemoryBlockStore) GetCurrentHeight() uint64 {
	s.mu.RLock()
	defer s.mu.RUnlock()
	return s.maxHeight
}

func (s *MemoryBlockStore) SetFinalized(height uint64, blockID string) {
	s.mu.Lock()
	defer s.mu.Unlock()

	if block, exists := s.blocks[blockID]; exists {
		s.finalizedBlocks[height] = block
		s.lastAccepted = block
		s.lastAcceptedHeight = height

		// Ê∏ÖÁêÜÂêåÈ´òÂ∫¶ÂÖ∂‰ªñÂå∫Âùó
		newBlocks := make([]*Block, 0, 1)
		for _, b := range s.heightIndex[height] {
			if b.ID == blockID {
				newBlocks = append(newBlocks, b)
			} else {
				delete(s.blocks, b.ID)
			}
		}
		s.heightIndex[height] = newBlocks
	}
}

// ÂàõÂª∫Âø´ÁÖß
func (s *MemoryBlockStore) CreateSnapshot(height uint64) (*Snapshot, error) {
	s.mu.Lock()
	defer s.mu.Unlock()

	// Âè™Âú®Â∑≤ÊúÄÁªàÂåñÁöÑÈ´òÂ∫¶ÂàõÂª∫Âø´ÁÖß
	if height > s.lastAcceptedHeight {
		return nil, fmt.Errorf("cannot create snapshot beyond last accepted height")
	}

	snapshot := &Snapshot{
		Height:             height,
		Timestamp:          time.Now(),
		FinalizedBlocks:    make(map[uint64]*Block),
		LastAcceptedID:     s.lastAccepted.ID,
		LastAcceptedHeight: s.lastAcceptedHeight,
		BlockHashes:        make(map[string]bool),
	}

	// Â§çÂà∂ÊâÄÊúâÂ∑≤ÊúÄÁªàÂåñÁöÑÂå∫ÂùóÔºàÂà∞ÊåáÂÆöÈ´òÂ∫¶Ôºâ
	for h := uint64(0); h <= height; h++ {
		if block, exists := s.finalizedBlocks[h]; exists {
			snapshot.FinalizedBlocks[h] = block
			snapshot.BlockHashes[block.ID] = true
		}
	}

	// Â≠òÂÇ®Âø´ÁÖß
	s.snapshots[height] = snapshot
	s.snapshotHeights = append(s.snapshotHeights, height)
	sort.Slice(s.snapshotHeights, func(i, j int) bool {
		return s.snapshotHeights[i] < s.snapshotHeights[j]
	})

	// ÈôêÂà∂Âø´ÁÖßÊï∞Èáè
	if len(s.snapshotHeights) > s.maxSnapshots {
		oldestHeight := s.snapshotHeights[0]
		delete(s.snapshots, oldestHeight)
		s.snapshotHeights = s.snapshotHeights[1:]
	}

	return snapshot, nil
}

// Âä†ËΩΩÂø´ÁÖßÔºàÊñ∞Â¢ûÔºâ
func (s *MemoryBlockStore) LoadSnapshot(snapshot *Snapshot) error {
	if snapshot == nil {
		return fmt.Errorf("nil snapshot")
	}

	s.mu.Lock()
	defer s.mu.Unlock()

	// Ê∏ÖÁ©∫Áé∞ÊúâÊï∞ÊçÆ
	s.blocks = make(map[string]*Block)
	s.heightIndex = make(map[uint64][]*Block)
	s.finalizedBlocks = make(map[uint64]*Block)

	// Âä†ËΩΩÂø´ÁÖßÊï∞ÊçÆ
	for height, block := range snapshot.FinalizedBlocks {
		s.blocks[block.ID] = block
		s.heightIndex[height] = []*Block{block}
		s.finalizedBlocks[height] = block

		if height > s.maxHeight {
			s.maxHeight = height
		}
	}

	// ÊÅ¢Â§çÊúÄÂêéÊé•ÂèóÁöÑÂå∫Âùó
	if lastBlock, exists := snapshot.FinalizedBlocks[snapshot.LastAcceptedHeight]; exists {
		s.lastAccepted = lastBlock
		s.lastAcceptedHeight = snapshot.LastAcceptedHeight
	}

	Logf("[Store] Loaded snapshot at height %d with %d blocks\n",
		snapshot.Height, len(snapshot.FinalizedBlocks))

	return nil
}

// Ëé∑ÂèñÊúÄÊñ∞Âø´ÁÖßÔºàÊñ∞Â¢ûÔºâ
func (s *MemoryBlockStore) GetLatestSnapshot() (*Snapshot, bool) {
	s.mu.RLock()
	defer s.mu.RUnlock()

	if len(s.snapshotHeights) == 0 {
		return nil, false
	}

	latestHeight := s.snapshotHeights[len(s.snapshotHeights)-1]
	snapshot, exists := s.snapshots[latestHeight]
	return snapshot, exists
}

// Ëé∑ÂèñÊåáÂÆöÈ´òÂ∫¶Êàñ‰πãÂâçÁöÑÊúÄËøëÂø´ÁÖßÔºàÊñ∞Â¢ûÔºâ
func (s *MemoryBlockStore) GetSnapshotAtHeight(height uint64) (*Snapshot, bool) {
	s.mu.RLock()
	defer s.mu.RUnlock()

	// ÊâæÂà∞Â∞è‰∫éÁ≠â‰∫éÊåáÂÆöÈ´òÂ∫¶ÁöÑÊúÄÂ§ßÂø´ÁÖßÈ´òÂ∫¶
	var bestHeight uint64
	found := false

	for i := len(s.snapshotHeights) - 1; i >= 0; i-- {
		if s.snapshotHeights[i] <= height {
			bestHeight = s.snapshotHeights[i]
			found = true
			break
		}
	}

	if !found {
		return nil, false
	}

	snapshot, exists := s.snapshots[bestHeight]
	return snapshot, exists
}


Êñá‰ª∂Ë∑ØÂæÑ: consensus/config.go
Êñá‰ª∂ÂÜÖÂÆπ:
package consensus

import "time"

// ============================================
// ÈÖçÁΩÆÁÆ°ÁêÜ
// ============================================

type Config struct {
	Network   NetworkConfig
	Consensus ConsensusConfig
	Node      NodeConfig
	Sync      SyncConfig
	Gossip    GossipConfig
	Snapshot  SnapshotConfig // Êñ∞Â¢û
}

type NetworkConfig struct {
	NumNodes          int
	NumByzantineNodes int
	NetworkLatency    time.Duration
}

type ConsensusConfig struct {
	K                    int
	Alpha                int
	Beta                 int
	QueryTimeout         time.Duration
	MaxConcurrentQueries int
	NumHeights           int
	BlocksPerHeight      int
}

type NodeConfig struct {
	ProposalInterval time.Duration
}

type SyncConfig struct {
	CheckInterval     time.Duration
	BehindThreshold   uint64
	BatchSize         uint64
	Timeout           time.Duration
	SnapshotThreshold uint64 // Êñ∞Â¢ûÔºöËêΩÂêéÂ§öÂ∞ëÈ´òÂ∫¶Êó∂‰ΩøÁî®Âø´ÁÖß
}

type GossipConfig struct {
	Fanout   int
	Interval time.Duration
}

// Êñ∞Â¢ûÔºöÂø´ÁÖßÈÖçÁΩÆ
type SnapshotConfig struct {
	Interval     uint64 // ÊØèÂ§öÂ∞ë‰∏™Âå∫ÂùóÂàõÂª∫‰∏ÄÊ¨°Âø´ÁÖß
	MaxSnapshots int    // ÊúÄÂ§ö‰øùÁïôÂ§öÂ∞ë‰∏™Âø´ÁÖß
	Enabled      bool   // ÊòØÂê¶ÂêØÁî®Âø´ÁÖßÂäüËÉΩ
}

func DefaultConfig() *Config {
	return &Config{
		Network: NetworkConfig{
			NumNodes:          100,
			NumByzantineNodes: 10,
			NetworkLatency:    100 * time.Millisecond,
		},
		Consensus: ConsensusConfig{
			K:                    20,
			Alpha:                15,
			Beta:                 15,
			QueryTimeout:         3 * time.Second,
			MaxConcurrentQueries: 20,
			NumHeights:           10,
			BlocksPerHeight:      5,
		},
		Node: NodeConfig{
			ProposalInterval: 1200 * time.Millisecond,
		},
		Sync: SyncConfig{
			CheckInterval:     2 * time.Second,
			BehindThreshold:   2,
			BatchSize:         10,
			Timeout:           5 * time.Second,
			SnapshotThreshold: 100, // Êñ∞Â¢ûÔºöËêΩÂêé100‰∏™È´òÂ∫¶Êó∂‰ΩøÁî®Âø´ÁÖß
		},
		Gossip: GossipConfig{
			Fanout:   15,
			Interval: 50 * time.Millisecond,
		},
		Snapshot: SnapshotConfig{ // Êñ∞Â¢û
			Interval:     100,  // ÊØè100‰∏™Âå∫Âùó‰∏Ä‰∏™Âø´ÁÖß
			MaxSnapshots: 10,   // ÊúÄÂ§ö‰øùÁïô10‰∏™Âø´ÁÖß
			Enabled:      true, // ÂêØÁî®Âø´ÁÖß
		},
	}
}


Êñá‰ª∂Ë∑ØÂæÑ: consensus/consensusEngine.go
Êñá‰ª∂ÂÜÖÂÆπ:
package consensus

import (
	"context"
	"fmt"
	"sync"
	"time"
)

// ============================================
// ÂÖ±ËØÜÂºïÊìé
// ============================================

type SnowmanEngine struct {
	mu            sync.RWMutex
	nodeID        NodeID
	store         BlockStore
	config        *ConsensusConfig
	events        EventBus
	snowballs     map[uint64]*Snowball
	activeQueries map[string]*QueryContext
	preferences   map[uint64]string
}

type QueryContext struct {
	queryKey  string
	blockID   string
	votes     map[string]int
	voters    map[NodeID]bool
	responded int
	startTime time.Time
	height    uint64
}

func NewSnowmanEngine(nodeID NodeID, store BlockStore, config *ConsensusConfig, events EventBus) ConsensusEngine {
	return &SnowmanEngine{
		nodeID:        nodeID,
		store:         store,
		config:        config,
		events:        events,
		snowballs:     make(map[uint64]*Snowball),
		activeQueries: make(map[string]*QueryContext),
		preferences:   make(map[uint64]string),
	}
}

func (e *SnowmanEngine) Start(ctx context.Context) error {
	// ÂàùÂßãÂåñÂàõ‰∏ñÂå∫ÂùóÁöÑSnowball
	e.mu.Lock()
	genesisSB := NewSnowball("genesis")
	genesisSB.Finalize()
	e.snowballs[0] = genesisSB
	e.preferences[0] = "genesis"
	e.mu.Unlock()

	// ÂÆöÊúüÊ£ÄÊü•Ë∂ÖÊó∂
	go func() {
		ticker := time.NewTicker(1 * time.Second)
		defer ticker.Stop()

		for {
			select {
			case <-ticker.C:
				e.checkTimeouts()
			case <-ctx.Done():
				return
			}
		}
	}()

	return nil
}

func (e *SnowmanEngine) RegisterQuery(nodeID NodeID, requestID uint32, blockID string, height uint64) string {
	e.mu.Lock()
	defer e.mu.Unlock()

	queryKey := fmt.Sprintf("%d-%d", nodeID, requestID)
	e.activeQueries[queryKey] = &QueryContext{
		queryKey:  queryKey,
		blockID:   blockID,
		votes:     make(map[string]int),
		voters:    make(map[NodeID]bool),
		responded: 0,
		startTime: time.Now(),
		height:    height,
	}

	return queryKey
}

func (e *SnowmanEngine) SubmitChit(nodeID NodeID, queryKey string, preferredID string) {
	e.mu.Lock()
	defer e.mu.Unlock()

	ctx, exists := e.activeQueries[queryKey]
	if !exists || ctx.voters[nodeID] {
		return
	}

	ctx.voters[nodeID] = true
	ctx.votes[preferredID]++
	ctx.responded++

	if ctx.responded >= e.config.Alpha {
		e.processVotes(ctx)
		delete(e.activeQueries, queryKey)
		e.events.PublishAsync(BaseEvent{
			eventType: EventQueryComplete,
			data:      ctx,
		})
	}
}

func (e *SnowmanEngine) processVotes(ctx *QueryContext) {
	sb, exists := e.snowballs[ctx.height]
	if !exists {
		sb = NewSnowball("")
		e.snowballs[ctx.height] = sb
	}

	candidates := make([]string, 0)
	blocks := e.store.GetByHeight(ctx.height)
	for _, block := range blocks {
		candidates = append(candidates, block.ID)
	}

	sb.RecordVote(candidates, ctx.votes, e.config.Alpha)

	newPreference := sb.GetPreference()
	if newPreference != "" {
		e.preferences[ctx.height] = newPreference
	}

	if sb.CanFinalize(e.config.Beta) && newPreference != "" {
		e.finalizeBlock(ctx.height, newPreference)
	}
}

func (e *SnowmanEngine) finalizeBlock(height uint64, blockID string) {
	if store, ok := e.store.(*MemoryBlockStore); ok {
		store.SetFinalized(height, blockID)
	}

	sb := e.snowballs[height]
	if sb != nil {
		sb.Finalize()
	}

	if block, exists := e.store.Get(blockID); exists {
		Logf("[Engine] üéâ Finalized block %s at height %d\n", blockID, height)
		e.events.PublishAsync(BaseEvent{
			eventType: EventBlockFinalized,
			data:      block,
		})
	}
}

func (e *SnowmanEngine) checkTimeouts() {
	e.mu.Lock()
	defer e.mu.Unlock()

	now := time.Now()
	toDelete := make([]string, 0)

	for queryKey, ctx := range e.activeQueries {
		if now.Sub(ctx.startTime) > e.config.QueryTimeout {
			toDelete = append(toDelete, queryKey)
		}
	}

	for _, queryKey := range toDelete {
		delete(e.activeQueries, queryKey)
	}

	if len(toDelete) > 0 {
		e.events.PublishAsync(BaseEvent{
			eventType: EventQueryComplete,
			data:      nil,
		})
	}
}

func (e *SnowmanEngine) GetActiveQueryCount() int {
	e.mu.RLock()
	defer e.mu.RUnlock()
	return len(e.activeQueries)
}

func (e *SnowmanEngine) GetPreference(height uint64) string {
	e.mu.RLock()
	defer e.mu.RUnlock()

	if pref, exists := e.preferences[height]; exists {
		return pref
	}

	if sb, exists := e.snowballs[height]; exists {
		return sb.GetPreference()
	}

	return ""
}


Êñá‰ª∂Ë∑ØÂæÑ: consensus/event.go
Êñá‰ª∂ÂÜÖÂÆπ:
package consensus

import "sync"

// ============================================
// ‰∫ã‰ª∂Á≥ªÁªü
// ============================================

type EventType string

const (
	EventBlockFinalized  EventType = "block.finalized"
	EventBlockReceived   EventType = "block.received"
	EventQueryComplete   EventType = "query.complete"
	EventSyncComplete    EventType = "sync.complete"
	EventNewBlock        EventType = "block.new"
	EventSnapshotCreated EventType = "snapshot.created" // Êñ∞Â¢û
	EventSnapshotLoaded  EventType = "snapshot.loaded"  // Êñ∞Â¢û
)

type BaseEvent struct {
	eventType EventType
	data      interface{}
}

func (e BaseEvent) Type() EventType   { return e.eventType }
func (e BaseEvent) Data() interface{} { return e.data }

type EventHandler func(Event)

type EventBus interface {
	Subscribe(topic EventType, handler EventHandler)
	Publish(event Event)
	PublishAsync(event Event)
}

type SimpleEventBus struct {
	mu       sync.RWMutex
	handlers map[EventType][]EventHandler
}

func NewEventBus() EventBus {
	return &SimpleEventBus{
		handlers: make(map[EventType][]EventHandler),
	}
}

func (eb *SimpleEventBus) Subscribe(topic EventType, handler EventHandler) {
	eb.mu.Lock()
	defer eb.mu.Unlock()
	eb.handlers[topic] = append(eb.handlers[topic], handler)
}

func (eb *SimpleEventBus) Publish(event Event) {
	eb.mu.RLock()
	handlers := eb.handlers[event.Type()]
	eb.mu.RUnlock()

	for _, handler := range handlers {
		handler(event)
	}
}

func (eb *SimpleEventBus) PublishAsync(event Event) {
	go eb.Publish(event)
}


Êñá‰ª∂Ë∑ØÂæÑ: consensus/gossipManager.go
Êñá‰ª∂ÂÜÖÂÆπ:
package consensus

import (
	"context"
	"math/rand"
	"sync"
	"time"
)

// ============================================
// GossipÁÆ°ÁêÜÂô®
// ============================================

type GossipManager struct {
	nodeID     NodeID
	node       *Node
	transport  Transport
	store      BlockStore
	config     *GossipConfig
	events     EventBus
	seenBlocks map[string]bool
	mu         sync.RWMutex
}

func NewGossipManager(nodeID NodeID, transport Transport, store BlockStore, config *GossipConfig, events EventBus) *GossipManager {
	gm := &GossipManager{
		nodeID:     nodeID,
		transport:  transport,
		store:      store,
		config:     config,
		events:     events,
		seenBlocks: make(map[string]bool),
	}

	events.Subscribe(EventNewBlock, func(e Event) {
		if block, ok := e.Data().(*Block); ok {
			gm.gossipBlock(block)
		}
	})

	return gm
}

func (gm *GossipManager) Start(ctx context.Context) {
	go func() {
		ticker := time.NewTicker(gm.config.Interval)
		defer ticker.Stop()

		for {
			select {
			case <-ticker.C:
				gm.gossipNewBlocks()
			case <-ctx.Done():
				return
			}
		}
	}()
}

func (gm *GossipManager) gossipNewBlocks() {
	_, currentHeight := gm.store.GetLastAccepted()

	blocks := make([]*Block, 0)
	blocks = append(blocks, gm.store.GetByHeight(currentHeight+1)...)
	blocks = append(blocks, gm.store.GetByHeight(currentHeight+2)...)

	for _, block := range blocks {
		if block.ID == "genesis" {
			continue
		}

		gm.mu.RLock()
		alreadyGossiped := gm.seenBlocks[block.ID]
		gm.mu.RUnlock()

		if !alreadyGossiped {
			gm.gossipBlock(block)
		}
	}
}

func (gm *GossipManager) gossipBlock(block *Block) {
	gm.mu.Lock()
	gm.seenBlocks[block.ID] = true
	gm.mu.Unlock()

	peers := gm.transport.SamplePeers(gm.nodeID, gm.config.Fanout)
	msg := Message{
		Type:    MsgGossip,
		From:    gm.nodeID,
		Block:   block,
		BlockID: block.ID,
		Height:  block.Height,
	}

	gm.transport.Broadcast(msg, peers)
}

func (gm *GossipManager) HandleGossip(msg Message) {
	if msg.Block == nil {
		return
	}

	if gm.node != nil {
		gm.node.stats.mu.Lock()
		gm.node.stats.gossipsReceived++
		gm.node.stats.mu.Unlock()
	}

	gm.mu.RLock()
	alreadySeen := gm.seenBlocks[msg.Block.ID]
	gm.mu.RUnlock()

	if alreadySeen {
		return
	}

	gm.mu.Lock()
	gm.seenBlocks[msg.Block.ID] = true
	gm.mu.Unlock()

	isNew, err := gm.store.Add(msg.Block)
	if err != nil {
		return
	}

	if isNew {
		Logf("[Node %d] Received new block %s via gossip from Node %d\n",
			gm.nodeID, msg.Block.ID, msg.From)

		gm.events.Publish(BaseEvent{
			eventType: EventBlockReceived,
			data:      msg.Block,
		})

		go func() {
			time.Sleep(time.Duration(50+rand.Intn(100)) * time.Millisecond)
			gm.gossipBlock(msg.Block)
		}()
	}
}


Êñá‰ª∂Ë∑ØÂæÑ: consensus/iface.go
Êñá‰ª∂ÂÜÖÂÆπ:
package consensus

import "context"

type BlockStore interface {
	Add(block *Block) (bool, error)
	Get(id string) (*Block, bool)
	GetByHeight(height uint64) []*Block
	GetLastAccepted() (string, uint64)
	GetFinalizedAtHeight(height uint64) (*Block, bool)
	GetBlocksFromHeight(from, to uint64) []*Block
	GetCurrentHeight() uint64

	// Âø´ÁÖßÁõ∏ÂÖ≥
	CreateSnapshot(height uint64) (*Snapshot, error)
	LoadSnapshot(snapshot *Snapshot) error
	GetLatestSnapshot() (*Snapshot, bool)
	GetSnapshotAtHeight(height uint64) (*Snapshot, bool)
}

type ConsensusEngine interface {
	Start(ctx context.Context) error
	RegisterQuery(nodeID NodeID, requestID uint32, blockID string, height uint64) string
	SubmitChit(nodeID NodeID, queryKey string, preferredID string)
	GetActiveQueryCount() int
	GetPreference(height uint64) string
}

type Event interface {
	Type() EventType
	Data() interface{}
}

// ============================================
// Âå∫ÂùóÊèêÊ°àÊé•Âè£ÂÆö‰πâ
// ============================================

// BlockProposer ÂÆö‰πâ‰∫ÜÂå∫ÂùóÊèêÊ°àÁöÑÊé•Âè£
type BlockProposer interface {
	// ProposeBlock ÁîüÊàê‰∏Ä‰∏™Êñ∞ÁöÑÂå∫ÂùóÊèêÊ°à
	// parentID: Áà∂Âå∫ÂùóID
	// height: Âå∫ÂùóÈ´òÂ∫¶
	// proposer: ÊèêÊ°àËÄÖID
	// round: ÊèêÊ°àËΩÆÊ¨°
	ProposeBlock(parentID string, height uint64, proposer NodeID, round int) (*Block, error)

	// ShouldPropose ÂÜ≥ÂÆöÊòØÂê¶Â∫îËØ•Âú®ÂΩìÂâçËΩÆÊ¨°ÊèêÂá∫Âå∫Âùó
	// nodeID: ËäÇÁÇπID
	// round: ÂΩìÂâçËΩÆÊ¨°
	// currentBlocks: ÂΩìÂâçÈ´òÂ∫¶Â∑≤Â≠òÂú®ÁöÑÂå∫ÂùóÊï∞Èáè
	ShouldPropose(nodeID NodeID, round int, currentBlocks int) bool
}

// ============================================
// ÁΩëÁªú‰º†ËæìÂ±ÇÊé•Âè£
// ============================================

type Transport interface {
	Send(to NodeID, msg Message) error
	Receive() <-chan Message
	Broadcast(msg Message, peers []NodeID)
	SamplePeers(exclude NodeID, count int) []NodeID
}


Êñá‰ª∂Ë∑ØÂæÑ: consensus/loop.go
Êñá‰ª∂ÂÜÖÂÆπ:
package consensus

import (
	"fmt"
	"math/rand"
	"time"
)

// ============================================
// ‰∏ªÂáΩÊï∞
// ============================================

func RunLoop() {
	rand.Seed(time.Now().UnixNano())

	config := DefaultConfig()

	// ÂèØ‰ª•Ë∞ÉÊï¥Âø´ÁÖßÈÖçÁΩÆËøõË°åÊµãËØï
	// config.Sync.SnapshotThreshold = 50  // ËêΩÂêé50‰∏™ÂùóÂ∞±Áî®Âø´ÁÖß
	// config.Snapshot.Interval = 50        // ÊØè50‰∏™ÂùóÂàõÂª∫‰∏Ä‰∏™Âø´ÁÖß

	fmt.Println("Starting Decoupled Snowman Consensus (With Snapshot Support)...")
	Logf("Network: %d nodes (%d honest, %d byzantine)\n",
		config.Network.NumNodes,
		config.Network.NumNodes-config.Network.NumByzantineNodes,
		config.Network.NumByzantineNodes)
	Logf("Heights: %d, Blocks per height: %d\n",
		config.Consensus.NumHeights,
		config.Consensus.BlocksPerHeight)

	if config.Snapshot.Enabled {
		Logf("Snapshot: Enabled (interval=%d, threshold=%d)\n",
			config.Snapshot.Interval,
			config.Sync.SnapshotThreshold)
	}

	network := NewNetworkManager(config)
	network.CreateNodes()

	programStart := time.Now()
	network.Start()

	ticker := time.NewTicker(1 * time.Second)
	defer ticker.Stop()

	lastHeight := uint64(0)
	for {
		select {
		case <-ticker.C:
			minHeight, allDone := network.CheckProgress()
			if minHeight > lastHeight {
				Logf("\n‚úÖ All honest nodes reached consensus on height %d\n", minHeight)
				lastHeight = minHeight
			}

			if allDone {
				totalTime := time.Since(programStart)
				Logf("\nüéâ All heights completed! Total time: %v\n", totalTime)

				time.Sleep(1 * time.Second)

				fmt.Println("\n\n===== FINAL RESULTS =====")
				network.PrintStatus()
				network.PrintFinalResults()

				fmt.Println("\n--- Time Statistics ---")
				Logf("Total Time: %v\n", totalTime)
				Logf("Average/Height: %v\n", totalTime/time.Duration(config.Consensus.NumHeights))

				return
			}
		}
	}
}


Êñá‰ª∂Ë∑ØÂæÑ: consensus/message.go
Êñá‰ª∂ÂÜÖÂÆπ:
package consensus

type NodeID int

// Ê∂àÊÅØÁ±ªÂûã
type MessageType int

const (
	MsgPullQuery MessageType = iota
	MsgPushQuery
	MsgChits
	MsgGet
	MsgPut
	MsgGossip
	MsgSyncRequest
	MsgSyncResponse
	MsgHeightQuery
	MsgHeightResponse
	MsgSnapshotRequest  // Êñ∞Â¢ûÔºöËØ∑Ê±ÇÂø´ÁÖß
	MsgSnapshotResponse // Êñ∞Â¢ûÔºöÂø´ÁÖßÂìçÂ∫î
)

// Âü∫Á°ÄÊ∂àÊÅØÁªìÊûÑ
type Message struct {
	Type      MessageType
	From      NodeID
	RequestID uint32
	BlockID   string
	Block     *Block
	Height    uint64
	// For Chits
	PreferredID       string
	PreferredIDHeight uint64
	AcceptedID        string
	AcceptedHeight    uint64
	// For Sync
	FromHeight uint64
	ToHeight   uint64
	Blocks     []*Block
	SyncID     uint32
	// For Height Query
	CurrentHeight uint64
	// For Snapshot
	Snapshot        *Snapshot
	SnapshotHeight  uint64
	RequestSnapshot bool
}


Êñá‰ª∂Ë∑ØÂæÑ: consensus/messageHandler.go
Êñá‰ª∂ÂÜÖÂÆπ:
package consensus

import "sort"

// ============================================
// Ê∂àÊÅØÂ§ÑÁêÜÂô®
// ============================================

type MessageHandler struct {
	nodeID          NodeID
	node            *Node
	isByzantine     bool
	transport       Transport
	store           BlockStore
	engine          ConsensusEngine
	queryManager    *QueryManager
	gossipManager   *GossipManager
	syncManager     *SyncManager
	snapshotManager *SnapshotManager // Êñ∞Â¢û
	events          EventBus
	config          *ConsensusConfig
}

func NewMessageHandler(nodeID NodeID, isByzantine bool, transport Transport, store BlockStore, engine ConsensusEngine, events EventBus, config *ConsensusConfig) *MessageHandler {
	return &MessageHandler{
		nodeID:      nodeID,
		isByzantine: isByzantine,
		transport:   transport,
		store:       store,
		engine:      engine,
		events:      events,
		config:      config,
	}
}

func (h *MessageHandler) SetManagers(qm *QueryManager, gm *GossipManager, sm *SyncManager, snapMgr *SnapshotManager) {
	h.queryManager = qm
	h.gossipManager = gm
	h.syncManager = sm
	h.snapshotManager = snapMgr
}

func (h *MessageHandler) Handle(msg Message) {
	if h.isByzantine && (msg.Type == MsgPullQuery || msg.Type == MsgPushQuery) {
		if h.node != nil {
			h.node.stats.mu.Lock()
			h.node.stats.queriesReceived++
			h.node.stats.mu.Unlock()
		}
		return
	}

	switch msg.Type {
	case MsgPullQuery:
		h.handlePullQuery(msg)
	case MsgPushQuery:
		h.handlePushQuery(msg)
	case MsgChits:
		h.queryManager.HandleChit(msg)
	case MsgGet:
		h.handleGet(msg)
	case MsgPut:
		h.handlePut(msg)
	case MsgGossip:
		h.gossipManager.HandleGossip(msg)
	case MsgSyncRequest:
		h.syncManager.HandleSyncRequest(msg)
	case MsgSyncResponse:
		h.syncManager.HandleSyncResponse(msg)
	case MsgHeightQuery:
		h.syncManager.HandleHeightQuery(msg)
	case MsgHeightResponse:
		h.syncManager.HandleHeightResponse(msg)
	case MsgSnapshotRequest: // Êñ∞Â¢û
		h.syncManager.HandleSnapshotRequest(msg)
	case MsgSnapshotResponse: // Êñ∞Â¢û
		h.syncManager.HandleSnapshotResponse(msg)
	}
}

func (h *MessageHandler) handlePullQuery(msg Message) {
	if h.node != nil {
		h.node.stats.mu.Lock()
		h.node.stats.queriesReceived++
		h.node.stats.mu.Unlock()
	}

	block, exists := h.store.Get(msg.BlockID)
	if !exists {
		h.transport.Send(NodeID(msg.From), Message{
			Type:      MsgGet,
			From:      h.nodeID,
			RequestID: msg.RequestID,
			BlockID:   msg.BlockID,
		})
		return
	}

	h.sendChits(NodeID(msg.From), msg.RequestID, block.Height)
}

func (h *MessageHandler) handlePushQuery(msg Message) {
	if h.node != nil {
		h.node.stats.mu.Lock()
		h.node.stats.queriesReceived++
		h.node.stats.mu.Unlock()
	}

	if msg.Block != nil {
		isNew, err := h.store.Add(msg.Block)
		if err != nil {
			return
		}

		if isNew {
			Logf("[Node %d] Received new block %s via PushQuery\n", h.nodeID, msg.Block.ID)
			h.events.Publish(BaseEvent{
				eventType: EventNewBlock,
				data:      msg.Block,
			})
		}

		h.sendChits(NodeID(msg.From), msg.RequestID, msg.Block.Height)
	}
}

func (h *MessageHandler) sendChits(to NodeID, requestID uint32, queryHeight uint64) {
	preferred := h.engine.GetPreference(queryHeight)

	if preferred == "" {
		if block, ok := h.store.GetFinalizedAtHeight(queryHeight); ok {
			preferred = block.ID
		}
	}

	if preferred == "" {
		blocks := h.store.GetByHeight(queryHeight)
		if len(blocks) > 0 {
			ids := make([]string, 0, len(blocks))
			for _, b := range blocks {
				ids = append(ids, b.ID)
			}
			sort.Strings(ids)
			preferred = ids[len(ids)-1]
		}
	}

	if h.node != nil {
		h.node.stats.mu.Lock()
		h.node.stats.chitsResponded++
		h.node.stats.mu.Unlock()
	}

	accepted, acceptedHeight := h.store.GetLastAccepted()
	h.transport.Send(to, Message{
		Type: MsgChits, From: h.nodeID, RequestID: requestID,
		PreferredID: preferred, PreferredIDHeight: queryHeight,
		AcceptedID: accepted, AcceptedHeight: acceptedHeight,
	})
}

func (h *MessageHandler) handleGet(msg Message) {
	if block, exists := h.store.Get(msg.BlockID); exists {
		h.transport.Send(NodeID(msg.From), Message{
			Type:      MsgPut,
			From:      h.nodeID,
			RequestID: msg.RequestID,
			Block:     block,
			Height:    block.Height,
		})
	}
}

func (h *MessageHandler) handlePut(msg Message) {
	if msg.Block != nil {
		isNew, err := h.store.Add(msg.Block)
		if err != nil {
			return
		}

		if isNew {
			Logf("[Node %d] Received new block %s via Put from Node %d, gossiping it\n",
				h.nodeID, msg.Block.ID, msg.From)
			h.events.Publish(BaseEvent{
				eventType: EventBlockReceived,
				data:      msg.Block,
			})
		}
	}
}


Êñá‰ª∂Ë∑ØÂæÑ: consensus/networkManager.go
Êñá‰ª∂ÂÜÖÂÆπ:
package consensus

import (
	"context"
	"fmt"
	"math/rand"
	"strings"
	"sync"
	"time"
)

// ============================================
// ÁΩëÁªúÁÆ°ÁêÜÂô®
// ============================================

type NetworkManager struct {
	nodes      map[NodeID]*Node
	transports map[NodeID]Transport
	config     *Config
	startTime  time.Time
	mu         sync.RWMutex
}

func NewNetworkManager(config *Config) *NetworkManager {
	return &NetworkManager{
		nodes:      make(map[NodeID]*Node),
		transports: make(map[NodeID]Transport),
		config:     config,
	}
}

func (nm *NetworkManager) CreateNodes() {
	byzantineMap := make(map[NodeID]bool)
	indices := rand.Perm(nm.config.Network.NumNodes)
	for i := 0; i < nm.config.Network.NumByzantineNodes; i++ {
		byzantineMap[NodeID(indices[i])] = true
	}

	ctx := context.Background()
	for i := 0; i < nm.config.Network.NumNodes; i++ {
		nodeID := NodeID(i)
		transport := NewSimulatedTransport(nodeID, nm, ctx, nm.config.Network.NetworkLatency)
		nm.transports[nodeID] = transport
	}

	for i := 0; i < nm.config.Network.NumNodes; i++ {
		nodeID := NodeID(i)
		node := NewNode(nodeID, nm.transports[nodeID], byzantineMap[nodeID], nm.config)
		nm.nodes[nodeID] = node
	}
}

func (nm *NetworkManager) GetTransport(nodeID NodeID) Transport {
	nm.mu.RLock()
	defer nm.mu.RUnlock()
	return nm.transports[nodeID]
}

func (nm *NetworkManager) SamplePeers(exclude NodeID, count int) []NodeID {
	nm.mu.RLock()
	defer nm.mu.RUnlock()

	peers := make([]NodeID, 0, len(nm.nodes)-1)
	for id := range nm.nodes {
		if id != exclude {
			peers = append(peers, id)
		}
	}

	rand.Shuffle(len(peers), func(i, j int) {
		peers[i], peers[j] = peers[j], peers[i]
	})

	if count > len(peers) {
		count = len(peers)
	}

	return peers[:count]
}

func (nm *NetworkManager) Start() {
	nm.startTime = time.Now()
	for _, node := range nm.nodes {
		node.Start()
	}
}

func (nm *NetworkManager) CheckProgress() (minHeight uint64, allDone bool) {
	minHeight = ^uint64(0)
	honestCount := 0

	for _, node := range nm.nodes {
		if !node.IsByzantine {
			honestCount++
			_, height := node.store.GetLastAccepted()
			if height < minHeight {
				minHeight = height
			}
		}
	}

	allDone = minHeight >= uint64(nm.config.Consensus.NumHeights)
	return minHeight, allDone
}

func (nm *NetworkManager) PrintStatus() {
	fmt.Println("\n===== Network Status =====")
	consensusMap := make(map[uint64]map[string]int)

	for id, node := range nm.nodes {
		lastAccepted, lastHeight := node.store.GetLastAccepted()
		currentHeight := node.store.GetCurrentHeight()

		nodeType := "Honest"
		if node.IsByzantine {
			nodeType = "Byzantine"
		}

		Logf("Node %d (%s): LastAccepted=%d, Current=%d, Block=%s\n",
			id, nodeType, lastHeight, currentHeight, lastAccepted)

		if lastHeight > 0 {
			if consensusMap[lastHeight] == nil {
				consensusMap[lastHeight] = make(map[string]int)
			}
			consensusMap[lastHeight][lastAccepted]++
		}
	}

	fmt.Println("\n--- Consensus by Height ---")
	for height := uint64(1); height <= uint64(nm.config.Consensus.NumHeights); height++ {
		if blocks, exists := consensusMap[height]; exists {
			Logf("Height %d: ", height)
			for blockID, count := range blocks {
				fmt.Printf("%s(%d nodes) ", blockID, count)
			}
			fmt.Println()
		}
	}
}

func (nm *NetworkManager) PrintFinalResults() {
	chains := make(map[NodeID][]string)

	for id, node := range nm.nodes {
		chain := make([]string, 0, nm.config.Consensus.NumHeights)
		for h := uint64(1); h <= uint64(nm.config.Consensus.NumHeights); h++ {
			if b, ok := node.store.GetFinalizedAtHeight(h); ok {
				chain = append(chain, b.ID)
			} else {
				chain = append(chain, "<none>")
			}
		}
		chains[id] = chain
	}

	allEqual := true
	var refChain []string
	for _, chain := range chains {
		if refChain == nil {
			refChain = chain
		} else {
			for i := range chain {
				if chain[i] != refChain[i] {
					allEqual = false
					break
				}
			}
		}
		if !allEqual {
			break
		}
	}

	fmt.Println("\n--- Global Agreement Check ---")
	if allEqual {
		Logf("All nodes have identical finalized chains: ‚úÖ YES\n")
		fmt.Println("Consensus chain:")
		fmt.Println(strings.Join(refChain, " -> "))
	} else {
		Logf("All nodes have identical finalized chains: ‚ùå NO\n")
		for id, chain := range chains {
			nodeType := "Honest"
			if nm.nodes[id].IsByzantine {
				nodeType = "Byzantine"
			}
			Logf("Node %3d (%s): %s\n", id, nodeType, strings.Join(chain, " -> "))
		}
	}

	nm.PrintQueryStatistics()
}

func (nm *NetworkManager) PrintQueryStatistics() {
	fmt.Println("\n--- Query & Sync Statistics ---")

	totalQueriesSent := uint32(0)
	totalQueriesReceived := uint32(0)
	totalChitsResponded := uint32(0)
	totalGossipsReceived := uint32(0)
	totalBlocksProposed := uint32(0)
	totalSnapshotsUsed := uint32(0)   // Êñ∞Â¢û
	totalSnapshotsServed := uint32(0) // Êñ∞Â¢û

	queriesByHeight := make(map[uint64]uint32)

	honestNodeCount := 0
	for _, node := range nm.nodes {
		if node.IsByzantine {
			continue
		}
		honestNodeCount++

		node.stats.mu.Lock()
		totalQueriesSent += node.stats.queriesSent
		totalQueriesReceived += node.stats.queriesReceived
		totalChitsResponded += node.stats.chitsResponded
		totalGossipsReceived += node.stats.gossipsReceived
		totalBlocksProposed += node.stats.blocksProposed
		totalSnapshotsUsed += node.stats.snapshotsUsed
		totalSnapshotsServed += node.stats.snapshotsServed

		for height, count := range node.stats.queriesPerHeight {
			queriesByHeight[height] += count
		}
		node.stats.mu.Unlock()
	}

	if honestNodeCount > 0 {
		avgQueriesSent := float64(totalQueriesSent) / float64(honestNodeCount)
		avgQueriesReceived := float64(totalQueriesReceived) / float64(honestNodeCount)
		avgChitsResponded := float64(totalChitsResponded) / float64(honestNodeCount)

		Logf("Average queries sent per honest node: %.2f\n", avgQueriesSent)
		Logf("Average queries received per honest node: %.2f\n", avgQueriesReceived)
		Logf("Average chits responded per honest node: %.2f\n", avgChitsResponded)
		Logf("Total blocks proposed: %d\n", totalBlocksProposed)
		Logf("Total gossips received: %d\n", totalGossipsReceived)

		// Êñ∞Â¢ûÂø´ÁÖßÁªüËÆ°
		if nm.config.Snapshot.Enabled {
			Logf("\n--- Snapshot Statistics ---\n")
			Logf("Total snapshots used: %d\n", totalSnapshotsUsed)
			Logf("Total snapshots served: %d\n", totalSnapshotsServed)
		}

		fmt.Println("\nQueries per height (total across all honest nodes):")
		for h := uint64(1); h <= uint64(nm.config.Consensus.NumHeights); h++ {
			if count, exists := queriesByHeight[h]; exists {
				avgPerNode := float64(count) / float64(honestNodeCount)
				Logf("  Height %d: %d total queries (%.2f avg per node)\n", h, count, avgPerNode)
			}
		}

		totalHeightQueries := uint32(0)
		for _, count := range queriesByHeight {
			totalHeightQueries += count
		}
		if nm.config.Consensus.NumHeights > 0 {
			avgQueriesPerHeight := float64(totalHeightQueries) / float64(nm.config.Consensus.NumHeights)
			Logf("\nAverage queries per height (all nodes): %.2f\n", avgQueriesPerHeight)
			avgQueriesPerHeightPerNode := avgQueriesPerHeight / float64(honestNodeCount)
			Logf("Average queries per height per node: %.2f\n", avgQueriesPerHeightPerNode)
		}
	}
}


Êñá‰ª∂Ë∑ØÂæÑ: consensus/node.go
Êñá‰ª∂ÂÜÖÂÆπ:
package consensus

import "context"

// ============================================
// ËäÇÁÇπÂÆûÁé∞
// ============================================

type Node struct {
	ID              NodeID
	IsByzantine     bool
	transport       Transport
	store           BlockStore
	engine          ConsensusEngine
	events          EventBus
	messageHandler  *MessageHandler
	queryManager    *QueryManager
	gossipManager   *GossipManager
	syncManager     *SyncManager
	snapshotManager *SnapshotManager
	proposalManager *ProposalManager
	ctx             context.Context
	cancel          context.CancelFunc
	config          *Config
	stats           *NodeStats
}

func NewNode(id NodeID, transport Transport, byzantine bool, config *Config) *Node {
	ctx, cancel := context.WithCancel(context.Background())

	store := NewMemoryBlockStoreWithConfig(config.Snapshot.MaxSnapshots)
	events := NewEventBus()
	engine := NewSnowmanEngine(id, store, &config.Consensus, events)

	node := &Node{
		ID:          id,
		IsByzantine: byzantine,
		transport:   transport,
		store:       store,
		engine:      engine,
		events:      events,
		ctx:         ctx,
		cancel:      cancel,
		config:      config,
		stats:       NewNodeStats(),
	}

	messageHandler := NewMessageHandler(id, byzantine, transport, store, engine, events, &config.Consensus)
	messageHandler.node = node

	queryManager := NewQueryManager(id, transport, store, engine, &config.Consensus, events)
	queryManager.node = node

	gossipManager := NewGossipManager(id, transport, store, &config.Gossip, events)
	gossipManager.node = node

	syncManager := NewSyncManager(id, transport, store, &config.Sync, &config.Snapshot, events)
	syncManager.node = node

	snapshotManager := NewSnapshotManager(id, store, &config.Snapshot, events) // Êñ∞Â¢û

	proposalManager := NewProposalManager(id, transport, store, &config.Node, events)
	proposalManager.node = node

	messageHandler.SetManagers(queryManager, gossipManager, syncManager, snapshotManager)

	node.messageHandler = messageHandler
	node.queryManager = queryManager
	node.gossipManager = gossipManager
	node.syncManager = syncManager
	node.snapshotManager = snapshotManager // Êñ∞Â¢û
	node.proposalManager = proposalManager

	return node
}

func (n *Node) Start() {
	go func() {
		for {
			select {
			case msg := <-n.transport.Receive():
				n.messageHandler.Handle(msg)
			case <-n.ctx.Done():
				return
			}
		}
	}()

	n.engine.Start(n.ctx)
	n.queryManager.Start(n.ctx)
	n.gossipManager.Start(n.ctx)
	n.syncManager.Start(n.ctx)
	n.snapshotManager.Start(n.ctx) // Êñ∞Â¢û

	if !n.IsByzantine {
		n.proposalManager.Start(n.ctx)
	}
}

func (n *Node) Stop() {
	n.cancel()
}


Êñá‰ª∂Ë∑ØÂæÑ: consensus/nodeStats.go
Êñá‰ª∂ÂÜÖÂÆπ:
package consensus

import "sync"

// ============================================
// ËäÇÁÇπÁªüËÆ°
// ============================================

type NodeStats struct {
	mu               sync.Mutex
	queriesSent      uint32
	queriesReceived  uint32
	chitsResponded   uint32
	queriesPerHeight map[uint64]uint32
	blocksProposed   uint32
	gossipsReceived  uint32
	snapshotsUsed    uint32 // Êñ∞Â¢û
	snapshotsServed  uint32 // Êñ∞Â¢û
}

func NewNodeStats() *NodeStats {
	return &NodeStats{
		queriesPerHeight: make(map[uint64]uint32),
	}
}


Êñá‰ª∂Ë∑ØÂæÑ: consensus/proposalManager.go
Êñá‰ª∂ÂÜÖÂÆπ:
package consensus

import (
	"context"
	"fmt"
	"sync"
	"time"
)

// DefaultBlockProposer ÈªòËÆ§ÁöÑÂå∫ÂùóÊèêÊ°àËÄÖÂÆûÁé∞Ôºà‰øùÊåÅÂéüÊúâÈÄªËæëÔºâ
type DefaultBlockProposer struct {
	maxBlocksPerHeight int
	proposalDenom      int
}

func NewDefaultBlockProposer() BlockProposer {
	return &DefaultBlockProposer{
		maxBlocksPerHeight: 3,
		proposalDenom:      33,
	}
}

func (p *DefaultBlockProposer) ProposeBlock(parentID string, height uint64, proposer NodeID, round int) (*Block, error) {
	blockID := fmt.Sprintf("block-%d-%d-r%d", height, proposer, round)

	block := &Block{
		ID:       blockID,
		Height:   height,
		ParentID: parentID,
		Data:     fmt.Sprintf("Height %d, Proposer %d, Round %d", height, proposer, round),
		Proposer: int(proposer),
		Round:    round,
	}

	return block, nil
}

func (p *DefaultBlockProposer) ShouldPropose(nodeID NodeID, round int, currentBlocks int) bool {
	// Â¶ÇÊûúÂΩìÂâçÈ´òÂ∫¶Â∑≤ÊúâË∂≥Â§üÂ§öÁöÑÂå∫ÂùóÔºå‰∏çÂÜçÊèêÊ°à
	if currentBlocks >= p.maxBlocksPerHeight {
		return false
	}

	// ‰ΩøÁî®ÂéüÊúâÁöÑÈöèÊú∫ÈÄâÊã©ÈÄªËæë
	denom := p.proposalDenom
	if denom <= 0 {
		denom = 1
	}

	return int(nodeID+NodeID(round))%denom == 0
}

type ProposalManager struct {
	nodeID         NodeID
	node           *Node
	transport      Transport
	store          BlockStore
	config         *NodeConfig
	events         EventBus
	proposedBlocks map[string]bool
	proposalRound  int
	mu             sync.Mutex
	proposer       BlockProposer // Êñ∞Â¢ûÔºöÊ≥®ÂÖ•ÁöÑÊèêÊ°àËÄÖÊé•Âè£
}

// NewProposalManager ÂàõÂª∫Êñ∞ÁöÑÊèêÊ°àÁÆ°ÁêÜÂô®Ôºà‰ΩøÁî®ÈªòËÆ§ÊèêÊ°àËÄÖÔºâ
func NewProposalManager(nodeID NodeID, transport Transport, store BlockStore, config *NodeConfig, events EventBus) *ProposalManager {
	return NewProposalManagerWithProposer(nodeID, transport, store, config, events, NewDefaultBlockProposer())
}

// NewProposalManagerWithProposer ÂàõÂª∫Êñ∞ÁöÑÊèêÊ°àÁÆ°ÁêÜÂô®ÔºàÂèØÊ≥®ÂÖ•Ëá™ÂÆö‰πâÊèêÊ°àËÄÖÔºâ
func NewProposalManagerWithProposer(nodeID NodeID, transport Transport, store BlockStore, config *NodeConfig, events EventBus, proposer BlockProposer) *ProposalManager {
	return &ProposalManager{
		nodeID:         nodeID,
		transport:      transport,
		store:          store,
		config:         config,
		events:         events,
		proposedBlocks: make(map[string]bool),
		proposer:       proposer,
	}
}

func (pm *ProposalManager) Start(ctx context.Context) {
	go func() {
		ticker := time.NewTicker(pm.config.ProposalInterval)
		defer ticker.Stop()

		for {
			select {
			case <-ticker.C:
				pm.proposeBlock()
			case <-ctx.Done():
				return
			}
		}
	}()
}

func (pm *ProposalManager) proposeBlock() {
	pm.mu.Lock()
	pm.proposalRound++
	currentRound := pm.proposalRound
	pm.mu.Unlock()

	lastAcceptedID, lastHeight := pm.store.GetLastAccepted()
	targetHeight := lastHeight + 1

	// Ëé∑ÂèñÂΩìÂâçÈ´òÂ∫¶ÁöÑÂå∫ÂùóÊï∞Èáè
	currentBlocks := len(pm.store.GetByHeight(targetHeight))

	// ‰ΩøÁî®Êé•Âè£Âà§Êñ≠ÊòØÂê¶Â∫îËØ•ÊèêÊ°à
	if !pm.proposer.ShouldPropose(pm.nodeID, currentRound, currentBlocks) {
		return
	}

	// ‰ΩøÁî®Êé•Âè£ÁîüÊàêÂå∫Âùó
	block, err := pm.proposer.ProposeBlock(lastAcceptedID, targetHeight, pm.nodeID, currentRound)
	if err != nil {
		Logf("[Node %d] Failed to propose block: %v\n", pm.nodeID, err)
		return
	}

	pm.mu.Lock()
	if pm.proposedBlocks[block.ID] {
		pm.mu.Unlock()
		return
	}
	pm.proposedBlocks[block.ID] = true
	pm.mu.Unlock()

	isNew, err := pm.store.Add(block)
	if err != nil || !isNew {
		return
	}

	if pm.node != nil {
		pm.node.stats.mu.Lock()
		pm.node.stats.blocksProposed++
		pm.node.stats.mu.Unlock()
	}

	Logf("[Node %d] Proposing %s on parent %s\n", pm.nodeID, block, lastAcceptedID)

	pm.events.Publish(BaseEvent{
		eventType: EventNewBlock,
		data:      block,
	})
}

// SetProposer ÂÖÅËÆ∏ËøêË°åÊó∂Êõ¥Êç¢ÊèêÊ°àËÄÖÂÆûÁé∞
func (pm *ProposalManager) SetProposer(proposer BlockProposer) {
	pm.mu.Lock()
	defer pm.mu.Unlock()
	pm.proposer = proposer
}


Êñá‰ª∂Ë∑ØÂæÑ: consensus/queryManager.go
Êñá‰ª∂ÂÜÖÂÆπ:
package consensus

import (
	"context"
	"sort"
	"sync"
	"sync/atomic"
	"time"
)

// ============================================
// Êü•ËØ¢ÁÆ°ÁêÜÂô®
// ============================================

type QueryManager struct {
	nodeID      NodeID
	node        *Node
	transport   Transport
	store       BlockStore
	engine      ConsensusEngine
	config      *ConsensusConfig
	events      EventBus
	activePolls sync.Map
	nextReqID   uint32
	mu          sync.Mutex
}

type Poll struct {
	requestID uint32
	blockID   string
	queryKey  string
	startTime time.Time
	height    uint64
}

func NewQueryManager(nodeID NodeID, transport Transport, store BlockStore, engine ConsensusEngine, config *ConsensusConfig, events EventBus) *QueryManager {
	qm := &QueryManager{
		nodeID:    nodeID,
		transport: transport,
		store:     store,
		engine:    engine,
		config:    config,
		events:    events,
	}

	events.Subscribe(EventQueryComplete, func(e Event) {
		qm.tryIssueQuery()
	})

	events.Subscribe(EventBlockFinalized, func(e Event) {
		qm.tryIssueQuery()
	})

	events.Subscribe(EventSyncComplete, func(e Event) {
		qm.tryIssueQuery()
	})

	// Êñ∞Â¢ûÔºöÂø´ÁÖßÂä†ËΩΩÂêé‰πüËß¶ÂèëÊü•ËØ¢
	events.Subscribe(EventSnapshotLoaded, func(e Event) {
		qm.tryIssueQuery()
	})

	events.Subscribe(EventBlockReceived, func(e Event) {
		qm.tryIssueQuery()
	})

	events.Subscribe(EventNewBlock, func(e Event) {
		qm.tryIssueQuery()
	})

	return qm
}

func (qm *QueryManager) tryIssueQuery() {
	qm.mu.Lock()
	defer qm.mu.Unlock()

	_, currentHeight := qm.store.GetLastAccepted()
	nextHeight := currentHeight + 1

	blocks := qm.store.GetByHeight(nextHeight)
	if len(blocks) == 0 {
		return
	}

	if qm.engine.GetActiveQueryCount() >= qm.config.MaxConcurrentQueries {
		return
	}

	qm.issueQuery()
}

func (qm *QueryManager) issueQuery() {
	_, currentHeight := qm.store.GetLastAccepted()
	nextHeight := currentHeight + 1

	blockID := qm.engine.GetPreference(nextHeight)
	if blockID == "" {
		blocks := qm.store.GetByHeight(nextHeight)
		if len(blocks) == 0 {
			return
		}

		candidates := make([]string, 0, len(blocks))
		for _, b := range blocks {
			candidates = append(candidates, b.ID)
		}
		sort.Strings(candidates)
		blockID = candidates[len(candidates)-1]
	}

	block, exists := qm.store.Get(blockID)
	if !exists {
		return
	}

	requestID := atomic.AddUint32(&qm.nextReqID, 1)
	queryKey := qm.engine.RegisterQuery(qm.nodeID, requestID, blockID, block.Height)

	poll := &Poll{
		requestID: requestID,
		blockID:   blockID,
		queryKey:  queryKey,
		startTime: time.Now(),
		height:    block.Height,
	}
	qm.activePolls.Store(requestID, poll)

	peers := qm.transport.SamplePeers(qm.nodeID, qm.config.K)
	msg := Message{
		Type:      MsgPushQuery,
		From:      qm.nodeID,
		RequestID: requestID,
		BlockID:   blockID,
		Block:     block,
		Height:    block.Height,
	}

	qm.transport.Broadcast(msg, peers)

	if qm.node != nil {
		qm.node.stats.mu.Lock()
		qm.node.stats.queriesSent++
		qm.node.stats.queriesPerHeight[block.Height]++
		qm.node.stats.mu.Unlock()
	}
}

func (qm *QueryManager) HandleChit(msg Message) {
	if poll, ok := qm.activePolls.Load(msg.RequestID); ok {
		p := poll.(*Poll)
		qm.engine.SubmitChit(NodeID(msg.From), p.queryKey, msg.PreferredID)
	}
}

func (qm *QueryManager) Start(ctx context.Context) {
	go func() {
		time.Sleep(100 * time.Millisecond)
		for i := 0; i < qm.config.MaxConcurrentQueries; i++ {
			qm.tryIssueQuery()
		}
	}()

	go func() {
		ticker := time.NewTicker(107 * time.Millisecond)
		defer ticker.Stop()

		for {
			select {
			case <-ticker.C:
				qm.tryIssueQuery()
			case <-ctx.Done():
				return
			}
		}
	}()
}


Êñá‰ª∂Ë∑ØÂæÑ: consensus/snapshot.go
Êñá‰ª∂ÂÜÖÂÆπ:
package consensus

import "time"

type Snapshot struct {
	Height             uint64            `json:"height"`
	Timestamp          time.Time         `json:"timestamp"`
	FinalizedBlocks    map[uint64]*Block `json:"finalized_blocks"`
	LastAcceptedID     string            `json:"last_accepted_id"`
	LastAcceptedHeight uint64            `json:"last_accepted_height"`
	BlockHashes        map[string]bool   `json:"block_hashes"` // Áî®‰∫éÂø´ÈÄüÂéªÈáç
}


Êñá‰ª∂Ë∑ØÂæÑ: consensus/snapshotManager.go
Êñá‰ª∂ÂÜÖÂÆπ:
package consensus

import (
	"context"
	"sync"
)

// ============================================
// Âø´ÁÖßÁÆ°ÁêÜÂô®
// ============================================

type SnapshotManager struct {
	nodeID NodeID
	store  BlockStore
	config *SnapshotConfig
	events EventBus
	mu     sync.Mutex
}

func NewSnapshotManager(nodeID NodeID, store BlockStore, config *SnapshotConfig, events EventBus) *SnapshotManager {
	return &SnapshotManager{
		nodeID: nodeID,
		store:  store,
		config: config,
		events: events,
	}
}

func (sm *SnapshotManager) Start(ctx context.Context) {
	if !sm.config.Enabled {
		return
	}

	// ÁõëÂê¨Âå∫ÂùóÊúÄÁªàÂåñ‰∫ã‰ª∂ÔºåÂÆöÊúüÂàõÂª∫Âø´ÁÖß
	sm.events.Subscribe(EventBlockFinalized, func(e Event) {
		if block, ok := e.Data().(*Block); ok {
			sm.checkAndCreateSnapshot(block.Height)
		}
	})
}

func (sm *SnapshotManager) checkAndCreateSnapshot(height uint64) {
	sm.mu.Lock()
	defer sm.mu.Unlock()

	// Ê£ÄÊü•ÊòØÂê¶Âà∞‰∫ÜÂàõÂª∫Âø´ÁÖßÁöÑÈ´òÂ∫¶
	if height > 0 && height%sm.config.Interval == 0 {
		snapshot, err := sm.store.CreateSnapshot(height)
		if err != nil {
			Logf("[Node %d] Failed to create snapshot at height %d: %v\n",
				sm.nodeID, height, err)
			return
		}

		Logf("[Node %d] üì∏ Created snapshot at height %d\n", sm.nodeID, height)

		sm.events.PublishAsync(BaseEvent{
			eventType: EventSnapshotCreated,
			data:      snapshot,
		})
	}
}


Êñá‰ª∂Ë∑ØÂæÑ: consensus/snowball.go
Êñá‰ª∂ÂÜÖÂÆπ:
package consensus

import (
	"sort"
	"sync"
)

// ============================================
// Snowball ÂÖ±ËØÜÁÆóÊ≥ïÊ†∏ÂøÉ
// ============================================

type Snowball struct {
	mu         sync.RWMutex
	blockID    string
	preference string
	confidence int
	finalized  bool
	lastVotes  map[string]int
}

func NewSnowball(blockID string) *Snowball {
	return &Snowball{
		blockID:   blockID,
		lastVotes: make(map[string]int),
	}
}

func (sb *Snowball) RecordVote(candidates []string, votes map[string]int, alpha int) {
	sb.mu.Lock()
	defer sb.mu.Unlock()

	sb.lastVotes = votes

	var winner string
	maxVotes := 0
	for cid, v := range votes {
		if v > maxVotes {
			maxVotes = v
			winner = cid
		}
	}

	if maxVotes >= alpha {
		if winner != sb.preference {
			sb.preference = winner
			sb.confidence = 1
		} else {
			sb.confidence++
		}
	} else {
		if len(candidates) > 0 {
			sort.Strings(candidates)
			largestBlock := candidates[len(candidates)-1]
			if largestBlock != sb.preference {
				sb.preference = largestBlock
				sb.confidence = 0
			}
		}
	}
}

func (sb *Snowball) GetPreference() string {
	sb.mu.RLock()
	defer sb.mu.RUnlock()
	return sb.preference
}

func (sb *Snowball) GetConfidence() int {
	sb.mu.RLock()
	defer sb.mu.RUnlock()
	return sb.confidence
}

func (sb *Snowball) CanFinalize(beta int) bool {
	sb.mu.RLock()
	defer sb.mu.RUnlock()
	return sb.confidence >= beta && !sb.finalized
}

func (sb *Snowball) Finalize() {
	sb.mu.Lock()
	defer sb.mu.Unlock()
	sb.finalized = true
}

func (sb *Snowball) IsFinalized() bool {
	sb.mu.RLock()
	defer sb.mu.RUnlock()
	return sb.finalized
}


Êñá‰ª∂Ë∑ØÂæÑ: consensus/syncManager.go
Êñá‰ª∂ÂÜÖÂÆπ:
package consensus

import (
	"context"
	"sync"
	"sync/atomic"
	"time"
)

// ============================================
// ÂêåÊ≠•ÁÆ°ÁêÜÂô® - Â¢ûÂº∫ÁâàÔºàÊîØÊåÅÂø´ÁÖßÔºâ
// ============================================

type SyncManager struct {
	nodeID         NodeID
	node           *Node // Êñ∞Â¢û
	transport      Transport
	store          BlockStore
	config         *SyncConfig
	snapshotConfig *SnapshotConfig // Êñ∞Â¢û
	events         EventBus
	syncRequests   map[uint32]time.Time
	nextSyncID     uint32
	syncing        bool
	mu             sync.RWMutex
	peerHeights    map[NodeID]uint64
	lastPoll       time.Time
	usingSnapshot  bool // Êñ∞Â¢ûÔºöÊ†áËÆ∞ÊòØÂê¶Ê≠£Âú®‰ΩøÁî®Âø´ÁÖßÂêåÊ≠•
}

func NewSyncManager(nodeID NodeID, transport Transport, store BlockStore, config *SyncConfig, snapshotConfig *SnapshotConfig, events EventBus) *SyncManager {
	return &SyncManager{
		nodeID:         nodeID,
		transport:      transport,
		store:          store,
		config:         config,
		snapshotConfig: snapshotConfig,
		events:         events,
		syncRequests:   make(map[uint32]time.Time),
		peerHeights:    make(map[NodeID]uint64),
		lastPoll:       time.Now(),
	}
}

func (sm *SyncManager) Start(ctx context.Context) {
	go func() {
		ticker := time.NewTicker(sm.config.CheckInterval)
		defer ticker.Stop()

		for {
			select {
			case <-ticker.C:
				sm.checkAndSync()
			case <-ctx.Done():
				return
			}
		}
	}()

	go func() {
		ticker := time.NewTicker(1 * time.Second)
		defer ticker.Stop()

		for {
			select {
			case <-ticker.C:
				sm.pollPeerHeights()
			case <-ctx.Done():
				return
			}
		}
	}()
}

func (sm *SyncManager) pollPeerHeights() {
	peers := sm.transport.SamplePeers(sm.nodeID, 10)
	for _, peer := range peers {
		sm.transport.Send(peer, Message{
			Type: MsgHeightQuery,
			From: sm.nodeID,
		})
	}
}

func (sm *SyncManager) HandleHeightQuery(msg Message) {
	_, height := sm.store.GetLastAccepted()
	currentHeight := sm.store.GetCurrentHeight()

	sm.transport.Send(NodeID(msg.From), Message{
		Type:          MsgHeightResponse,
		From:          sm.nodeID,
		Height:        height,
		CurrentHeight: currentHeight,
	})
}

func (sm *SyncManager) HandleHeightResponse(msg Message) {
	sm.mu.Lock()
	defer sm.mu.Unlock()
	sm.peerHeights[NodeID(msg.From)] = msg.CurrentHeight
}

func (sm *SyncManager) checkAndSync() {
	sm.mu.Lock()
	if sm.syncing {
		sm.mu.Unlock()
		return
	}

	maxPeerHeight := uint64(0)
	for _, height := range sm.peerHeights {
		if height > maxPeerHeight {
			maxPeerHeight = height
		}
	}
	sm.mu.Unlock()

	localCurrentHeight := sm.store.GetCurrentHeight()
	heightDiff := uint64(0)
	if maxPeerHeight > localCurrentHeight {
		heightDiff = maxPeerHeight - localCurrentHeight
	}

	// Âà§Êñ≠ÊòØÂê¶ÈúÄË¶Å‰ΩøÁî®Âø´ÁÖßÂêåÊ≠•
	if sm.snapshotConfig.Enabled && heightDiff > sm.config.SnapshotThreshold {
		// ‰ΩøÁî®Âø´ÁÖßÂêåÊ≠•
		sm.requestSnapshotSync(maxPeerHeight)
	} else if heightDiff > sm.config.BehindThreshold {
		// ‰ΩøÁî®ÊôÆÈÄöÂêåÊ≠•
		sm.requestSync(localCurrentHeight+1, minUint64(localCurrentHeight+sm.config.BatchSize, maxPeerHeight))
	}
}

// ËØ∑Ê±ÇÂø´ÁÖßÂêåÊ≠•ÔºàÊñ∞Â¢ûÔºâ
func (sm *SyncManager) requestSnapshotSync(targetHeight uint64) {
	sm.mu.Lock()
	if sm.syncing {
		sm.mu.Unlock()
		return
	}
	sm.syncing = true
	sm.usingSnapshot = true
	syncID := atomic.AddUint32(&sm.nextSyncID, 1)
	sm.syncRequests[syncID] = time.Now()
	sm.mu.Unlock()

	// Êâæ‰∏Ä‰∏™È´òÂ∫¶Ë∂≥Â§üÁöÑËäÇÁÇπ
	sm.mu.RLock()
	var targetPeer NodeID = -1
	for peer, height := range sm.peerHeights {
		if height >= targetHeight {
			targetPeer = peer
			break
		}
	}
	sm.mu.RUnlock()

	if targetPeer == -1 {
		peers := sm.transport.SamplePeers(sm.nodeID, 5)
		if len(peers) > 0 {
			targetPeer = peers[0]
		}
	}

	if targetPeer != -1 {
		Logf("[Node %d] üì∏ Requesting SNAPSHOT sync from Node %d (behind by %d blocks)\n",
			sm.nodeID, targetPeer, targetHeight-sm.store.GetCurrentHeight())

		msg := Message{
			Type:            MsgSnapshotRequest,
			From:            sm.nodeID,
			SyncID:          syncID,
			RequestSnapshot: true,
			Height:          targetHeight,
		}
		sm.transport.Send(targetPeer, msg)
	} else {
		sm.mu.Lock()
		sm.syncing = false
		sm.usingSnapshot = false
		delete(sm.syncRequests, syncID)
		sm.mu.Unlock()
	}
}

func (sm *SyncManager) requestSync(fromHeight, toHeight uint64) {
	sm.mu.Lock()
	if sm.syncing {
		sm.mu.Unlock()
		return
	}
	sm.syncing = true
	syncID := atomic.AddUint32(&sm.nextSyncID, 1)
	sm.syncRequests[syncID] = time.Now()
	sm.mu.Unlock()

	sm.mu.RLock()
	var targetPeer NodeID = -1
	for peer, height := range sm.peerHeights {
		if height >= toHeight {
			targetPeer = peer
			break
		}
	}
	sm.mu.RUnlock()

	if targetPeer == -1 {
		peers := sm.transport.SamplePeers(sm.nodeID, 5)
		if len(peers) > 0 {
			targetPeer = peers[0]
		}
	}

	if targetPeer != -1 {
		Logf("[Node %d] Requesting sync from Node %d for heights %d-%d\n",
			sm.nodeID, targetPeer, fromHeight, toHeight)

		msg := Message{
			Type:       MsgSyncRequest,
			From:       sm.nodeID,
			SyncID:     syncID,
			FromHeight: fromHeight,
			ToHeight:   toHeight,
		}
		sm.transport.Send(targetPeer, msg)
	} else {
		sm.mu.Lock()
		sm.syncing = false
		delete(sm.syncRequests, syncID)
		sm.mu.Unlock()
	}
}

// Â§ÑÁêÜÂø´ÁÖßËØ∑Ê±ÇÔºàÊñ∞Â¢ûÔºâ
func (sm *SyncManager) HandleSnapshotRequest(msg Message) {
	// Ëé∑ÂèñÊúÄËøëÁöÑÂø´ÁÖß
	snapshot, exists := sm.store.GetLatestSnapshot()
	if !exists {
		// Â¶ÇÊûúÊ≤°ÊúâÂø´ÁÖßÔºåÈôçÁ∫ßÂà∞ÊôÆÈÄöÂêåÊ≠•
		sm.HandleSyncRequest(Message{
			Type:       MsgSyncRequest,
			From:       msg.From,
			SyncID:     msg.SyncID,
			FromHeight: 1,
			ToHeight:   minUint64(100, sm.store.GetCurrentHeight()),
		})
		return
	}

	Logf("[Node %d] üì∏ Sending snapshot (height %d) to Node %d\n",
		sm.nodeID, snapshot.Height, msg.From)

	// Êõ¥Êñ∞ÁªüËÆ°
	if sm.node != nil {
		sm.node.stats.mu.Lock()
		sm.node.stats.snapshotsServed++
		sm.node.stats.mu.Unlock()
	}

	response := Message{
		Type:           MsgSnapshotResponse,
		From:           sm.nodeID,
		SyncID:         msg.SyncID,
		Snapshot:       snapshot,
		SnapshotHeight: snapshot.Height,
	}

	sm.transport.Send(NodeID(msg.From), response)
}

// Â§ÑÁêÜÂø´ÁÖßÂìçÂ∫îÔºàÊñ∞Â¢ûÔºâ
func (sm *SyncManager) HandleSnapshotResponse(msg Message) {
	sm.mu.Lock()
	defer sm.mu.Unlock()

	if _, ok := sm.syncRequests[msg.SyncID]; !ok {
		return
	}

	delete(sm.syncRequests, msg.SyncID)

	if msg.Snapshot == nil {
		sm.syncing = false
		sm.usingSnapshot = false
		return
	}

	// Âä†ËΩΩÂø´ÁÖß
	err := sm.store.LoadSnapshot(msg.Snapshot)
	if err != nil {
		Logf("[Node %d] Failed to load snapshot: %v\n", sm.nodeID, err)
		sm.syncing = false
		sm.usingSnapshot = false
		return
	}

	// Êõ¥Êñ∞ÁªüËÆ°
	if sm.node != nil {
		sm.node.stats.mu.Lock()
		sm.node.stats.snapshotsUsed++
		sm.node.stats.mu.Unlock()
	}

	Logf("[Node %d] üì∏ Successfully loaded snapshot at height %d\n",
		sm.nodeID, msg.SnapshotHeight)

	// ÂèëÂ∏ÉÂø´ÁÖßÂä†ËΩΩ‰∫ã‰ª∂
	sm.events.PublishAsync(BaseEvent{
		eventType: EventSnapshotLoaded,
		data:      msg.Snapshot,
	})

	// ÁªßÁª≠ÂêåÊ≠•Âø´ÁÖß‰πãÂêéÁöÑÂå∫Âùó
	currentHeight := sm.store.GetCurrentHeight()
	maxPeerHeight := uint64(0)
	for _, height := range sm.peerHeights {
		if height > maxPeerHeight {
			maxPeerHeight = height
		}
	}

	sm.syncing = false
	sm.usingSnapshot = false

	// Â¶ÇÊûúËøòÈúÄË¶ÅÊõ¥Â§öÂå∫ÂùóÔºåÁªßÁª≠ÊôÆÈÄöÂêåÊ≠•
	if maxPeerHeight > currentHeight+1 {
		go func() {
			time.Sleep(100 * time.Millisecond)
			sm.requestSync(currentHeight+1, minUint64(currentHeight+sm.config.BatchSize, maxPeerHeight))
		}()
	}
}

func (sm *SyncManager) HandleSyncRequest(msg Message) {
	blocks := sm.store.GetBlocksFromHeight(msg.FromHeight, msg.ToHeight)

	if len(blocks) == 0 {
		return
	}

	Logf("[Node %d] Sending %d blocks to Node %d for sync\n",
		sm.nodeID, len(blocks), msg.From)

	response := Message{
		Type:       MsgSyncResponse,
		From:       sm.nodeID,
		SyncID:     msg.SyncID,
		Blocks:     blocks,
		FromHeight: msg.FromHeight,
		ToHeight:   msg.ToHeight,
	}

	sm.transport.Send(NodeID(msg.From), response)
}

func (sm *SyncManager) HandleSyncResponse(msg Message) {
	sm.mu.Lock()
	defer sm.mu.Unlock()

	if _, ok := sm.syncRequests[msg.SyncID]; !ok {
		return
	}

	delete(sm.syncRequests, msg.SyncID)
	sm.syncing = false

	added := 0
	for _, block := range msg.Blocks {
		isNew, err := sm.store.Add(block)
		if err != nil {
			continue
		}
		if isNew {
			added++
		}
	}

	if added > 0 {
		Logf("[Node %d] üì¶ Successfully synced %d new blocks (heights %d-%d)\n",
			sm.nodeID, added, msg.FromHeight, msg.ToHeight)
	}

	sm.events.PublishAsync(BaseEvent{
		eventType: EventSyncComplete,
		data:      added,
	})
}


Êñá‰ª∂Ë∑ØÂæÑ: consensus/transport.go
Êñá‰ª∂ÂÜÖÂÆπ:
package consensus

import (
	"context"
	"math/rand"
	"time"
)

type SimulatedTransport struct {
	nodeID         NodeID
	inbox          chan Message
	network        *NetworkManager
	ctx            context.Context
	networkLatency time.Duration
}

func NewSimulatedTransport(nodeID NodeID, network *NetworkManager, ctx context.Context, latency time.Duration) Transport {
	return &SimulatedTransport{
		nodeID:         nodeID,
		inbox:          make(chan Message, 1000000),
		network:        network,
		ctx:            ctx,
		networkLatency: latency,
	}
}

func (t *SimulatedTransport) Send(to NodeID, msg Message) error {
	go func() {
		// Âø´ÁÖß‰º†ËæìÂª∂ËøüÊõ¥Èïø
		delay := t.networkLatency
		if msg.Type == MsgSnapshotResponse && msg.Snapshot != nil {
			delay = delay * 3 // Âø´ÁÖßÊï∞ÊçÆÊõ¥Â§ßÔºå‰º†ËæìÊó∂Èó¥Êõ¥Èïø
		}
		delay += time.Duration(rand.Intn(int(delay / 2)))
		time.Sleep(delay)

		select {
		case t.network.GetTransport(to).(*SimulatedTransport).inbox <- msg:
		case <-time.After(100 * time.Millisecond):
		case <-t.ctx.Done():
		}
	}()
	return nil
}

func (t *SimulatedTransport) Receive() <-chan Message {
	return t.inbox
}

func (t *SimulatedTransport) Broadcast(msg Message, peers []NodeID) {
	for _, peer := range peers {
		t.Send(peer, msg)
	}
}

func (t *SimulatedTransport) SamplePeers(exclude NodeID, count int) []NodeID {
	return t.network.SamplePeers(exclude, count)
}


Êñá‰ª∂Ë∑ØÂæÑ: consensus/utils.go
Êñá‰ª∂ÂÜÖÂÆπ:
package consensus

import (
	"fmt"
	"time"
)

// ============================================
// Â∑•ÂÖ∑ÂáΩÊï∞
// ============================================

func Logf(format string, args ...interface{}) {
	now := time.Now()
	timestamp := now.Format("15:04:05.999")
	fmt.Printf("[%s] %s", timestamp, fmt.Sprintf(format, args...))
}

func minUint64(a, b uint64) uint64 {
	if a < b {
		return a
	}
	return b
}


Êñá‰ª∂Ë∑ØÂæÑ: db/data.proto
Êñá‰ª∂ÂÜÖÂÆπ:
syntax = "proto3";
// protoc --go_out=. --go_opt=paths=source_relative db/data.proto
package pb;

option go_package = "dex/db;db";

// --------------------- Token & Registry ---------------------
message Token {
  string address = 1; // tokenÂú∞ÂùÄÔºåÊâÄÊúâtokenÈÉΩÊòØÂπ≥Á∫ßÔºåÂå∫Âà´ÊòØÁ≥ªÁªütoken‰ºöÊúâ‰ª£Á†ÅÂØπÂÆÉÁöÑÂú∞ÂùÄÂÅöÁâπÊÆäÂ§ÑÁêÜÔºåÊØîÂ¶ÇÂéüÁîü‰ª£Â∏ÅFB‰ºöÊääÂú∞ÂùÄÁ°¨ÁºñÁ†ÅÂà∞‰ª£Á†ÅÈáåÈù¢
  string symbol = 2; // ‰æãÂ¶Ç "USDT"„ÄÅ"MYT"Á≠â
  string name = 3;   // ‰æãÂ¶Ç "Tether USD"
  string owner = 4;  // Êã•ÊúâËÄÖÂú∞ÂùÄÔºåÂèØÂÅöÂÜªÁªìÁ≠âÁÆ°ÁêÜÊùÉÈôêÔºåÂ¶ÇÊûúÁ≠â‰∫é 0x0 Ë°®Á§∫Ê≤°ÊúâÊã•ÊúâËÄÖ
  string totalSupply = 5; // ÂèëË°åÊÄªÈáè
  bool canMint = 6;       // ÊòØÂê¶ÂèØÂ¢ûÂèëÔºåÈîÄÊØÅÁöÑËØùÁõ¥Êé•ÊâìÂÖ• 0x0 Âú∞ÂùÄÂç≥ÂèØÔºåÊü•ËØ¢0x0‰πüÂèØÊü•Âà∞Ë¢´ÈîÄÊØÅÁöÑtokenÊï∞Èáè
}

message TokenRegistry {
  map<string, Token> tokens = 1;
}

// --------------------- ÂÖ¨ÂÖ±Â≠óÊÆµÂÆö‰πâ ---------------------
message BaseMessage {
  string tx_id = 1;         // ‰∫§Êòì ID Âç≥hash
  string from_address = 2;  // ÂèëËµ∑ËÄÖÂú∞ÂùÄ
  uint64 target_height = 3; // ÊúüÊúõÊâßË°å‰∫§ÊòìÁöÑÈ´òÂ∫¶
  uint64 executed_height = 4; // ÂÆûÈôÖÊâßË°åÁöÑÂå∫ÂùóÈ´òÂ∫¶
  string public_key = 5;
  string signature = 6;     // Á≠æÂêç
  Status status = 7;
  uint64 nonce = 8;
}

// --------------------- Ë¥¶Êà∑ & Âå∫Âùó‰ø°ÊÅØ ---------------------
message Account {
  string address = 1; // Ë¥¶Êà∑Âú∞ÂùÄ
  string public_key = 2; // Âú∞ÂùÄÂØπÂ∫îÁöÑÂÖ¨Èí•
  uint64 nonce = 3;   // Ë¥¶Êà∑ÁöÑÊìç‰ΩúÂ∫èÂè∑
  repeated string orders = 4; // Êú™ÊâßË°åÂÆåÁöÑ order Áä∂ÊÄÅÁöÑ hash ÂàóË°®
  map<string, TokenBalance> balances = 5; // key‰∏∫tokenÂú∞ÂùÄ
  string receive_votes = 6; // Ê≠§Ë¥¶Êà∑Êî∂Âà∞ÁöÑÊäïÁ•®
  string candidate = 7;     // Ë¢´Ê≠§Ë¥¶Êà∑ÈÄâ‰∏æÁöÑ‰∫∫Ôºå‰∏Ä‰∏™Ë¥¶Êà∑Âè™ËÉΩÂêåÊó∂ÈÄâ‰∏æ‰∏Ä‰∏™‰∫∫
  string unclaimed_reward = 8;  // Á¥ØËÆ°‰ΩÜÂ∞öÊú™È¢ÜÂèñÁöÑÂ•ñÂä±
  string last_acc_reward = 9;   // ‰∏äÊ¨°Êõ¥Êñ∞Êó∂ÁöÑÁ¥ØÁßØÂõ†Â≠ê
  bool   is_miner = 10;     // ÊòØÂê¶Ê≠£Âú®ÂèÇ‰∏éÂÖ±ËØÜÊåñÁüø
  string ip = 11;
  uint64 index = 12; // ÊØè‰∏™ÁüøÂ∑•ÈÉΩÊúâÂîØ‰∏ÄÁöÑindexÔºåÁî®‰∫ébitmap
  string code = 13;    // ‰∏∫ÂêéÁª≠ÂêàÁ∫¶ÂäüËÉΩ‰øùÁïôÂ≠óÊÆµ
}

message TokenBalance {
  string balance = 1;       // ÂèØÁî®‰ΩôÈ¢ù
  string candidate_locked_balance = 2; // ÊäïÁ•®ÈîÅÂÆö‰ΩôÈ¢ù
  string miner_locked_balance = 3; // ÂÖ±ËØÜÊåñÁüøÈîÅÂÆö‰ΩôÈ¢ù
  string liquid_locked_balance = 4; // ÊµÅÂä®ÊÄßÊåñÁüøÈîÅÂÆö‰ΩôÈ¢ù
  string witness_locked_balance = 5; // ËßÅËØÅËÄÖÊåñÁüøÈîÅÂÆö‰ΩôÈ¢ù
  string leverage_locked_balance = 6; // Êù†ÊùÜ‰∫§ÊòìÈîÅÂÆö‰ΩôÈ¢ù
}

message Block {
  uint64 height = 1;
  string txs_hash = 2;
  string block_hash = 3;
  string prev_block_hash = 4;
  string accumulated_reward = 5;// Âå∫ÂùóÂ•ñÂä±Á¥ØËÆ°Âõ†Â≠ê
  bytes bit_map = 6; // ÁüøÂ∑•Âú®Á∫ø‰ΩçÂõæ
  bytes miner_signature = 7;
  bytes short_txs = 8;
  string miner = 9;
  repeated AnyTx body = 10;// ÂÖ®ÈÉ®‰∫§Êòì
}
message RewordInfo {

}
message OrderPriceIndex {// Âø´ÈÄüÊ£ÄÁ¥¢Âà∞‰ª∑Ê†ºÂå∫Èó¥ÁöÑËÆ¢ÂçïÔºå‰∏éorderTx‰∏ÄÂπ∂Â≠òÂÖ•
  bool ok = 1; //Âç†‰ΩçÁ¨¶,‰∏çÈúÄË¶ÅÔºå‰∏ªË¶ÅÊòØÈúÄË¶ÅËøô‰∏™Â≠óÊÆµÁöÑkey(pair:...|price:...|is_filledÔºö...|order_id:...)Êù•Âø´ÈÄüÊü•ËØ¢Âà∞‰ª∑Ê†ºÂå∫Èó¥ÂÜÖÔºåËøòÊú™ÂÆåÂÖ®Êàê‰∫§Ôºàis_filled:falseÔºâÁöÑtx
}
message CandidateIndex {//Âø´ÈÄüÈÅçÂéÜÂà∞ËÆÆÂëòÁöÑÊâÄÊúâÂßîÊâò‰∫∫
  bool ok = 1;// key(candidate:...|user:...)
}
// --------------------- ‰∫§ÊòìÁõ∏ÂÖ≥Ê∂àÊÅØ ---------------------
message IssueTokenTx {// ÂèëÂ∏Åtx
  BaseMessage base = 1;
  string token_name = 2;
  string token_symbol = 3;
  string total_supply = 4;
  bool canMint = 5;
}

message FreezeTx {//ÂÜªÁªì„ÄÅËß£ÂÜªToken tx
  BaseMessage base = 1;
  string token_addr = 2;
  string target_addr = 3;
  bool freeze = 4; // true ‰∏∫ÂÜªÁªìÔºåfalse ‰∏∫Ëß£ÂÜª
}

message Transaction {//ËΩ¨Ë¥¶tx
  BaseMessage base = 1;
  string to = 2;
  string token_address = 3;
  string amount = 4;
}

message OrderTx {//‰∏ãÂçïtx
  BaseMessage base = 1;
  string base_token = 2;  // ‰æãÂ¶Ç "bc1q6156"‰ª£Ë°®USDT
  string quote_token = 3; // ‰æãÂ¶Ç "bc1q0000"‰ª£Ë°®BTC
  OrderOp op = 4; // ADDÊòØÊõ¥Êñ∞ËÆ¢ÂçïÁä∂ÊÄÅREMOVEÊòØÊí§ÂçïÔºå‰π∞ÂçïÂíå‰π∞ÂçïÊòØÊåâbase_tokenÂíåquote_tokenÊù•ÂÜ≥ÂÆöÁöÑ„ÄÇÊØîÂ¶Ç‰π∞BTCÔºåÈÇ£‰πàbase_token=USDTÁöÑÂú∞ÂùÄ,quote_token=BTCÁöÑÂú∞ÂùÄ
  string op_target_id = 5;   // Â¶ÇÊûúË¶ÅÊõ¥Êñ∞/ÁßªÈô§ËÆ¢Âçïid
  string amount = 6;
  string price = 7;
  string filled_base = 8;  // Ë°®Á§∫ËØ•ËÆ¢ÂçïÂ∑≤ÁªèÊàê‰∫§ÁöÑbaseÊï∞Èáè
  string filled_quote = 9; // Ë°®Á§∫ËØ•ËÆ¢ÂçïÂ∑≤ÁªèÊàê‰∫§ÁöÑquoteÊï∞Èáè
  bool is_filled = 10; // ÊòØÂê¶Â∑≤ÁªèÂÆåÂÖ®Êàê‰∫§Ê†áÁ≠æ
}

message RechargeTx {//‰∏äË¥¶TX,Áî®Êà∑Ëá™Â∑±Âú®Êú¨Âú∞Ê†πÊçÆÂÖ¨Èí•+tweakÁîüÊàêÂÖÖÂÄºÂú∞ÂùÄ„ÄÇÂÖÖÂÄºÂÆåÊàêÂêéÂπøÊí≠RechargeTx
  BaseMessage base = 1;
  string token_address = 2;//ÂØπÂ∫îÊú¨ÈìæÁöÑÂì™‰∏™ËµÑ‰∫ß
  //Ëøô‰∏™Â≠óÊÆµÂú®ÊâìÂåÖÂå∫ÂùóÁöÑÊó∂ÂÄôÁîüÊàêÔºåÁî®Êà∑Áõ¥Êé•ÈÄöËøáÂå∫ÂùóÈìæÊµèËßàÂô®Êü•ËØ¢Âç≥ÂèØ
  string generated_address = 3;//ÁîüÊàêÁöÑËÅöÂêàÂú∞ÂùÄÔºåÂ¶ÇÊûúÊòØbtcÂ∞±Áõ¥Êé•Áî®ÔºåÂ¶ÇÊûúÊòØÂÖ∂‰ªñÈìæÂ∞±ÂØπÂ∫îÂÖ∂ÂêàÁ∫¶ÂÜÖÁöÑÊâÄÊúâËÄÖÂ≠óÊÆµ
  string tweak = 4;//Âú®Èí±ÂåÖÁ´ØÔºåÁîüÊàêÂú∞ÂùÄÁöÑÊó∂ÂÄôËÆ∞ÂΩï‰∏ãÊù•Ôºå
  //Âêå‰∏Ä‰∏™Âú∞ÂùÄÂ§öÊ¨°‰∏äË¥¶ÂíãÊêûÔºü
  //‰∏çË°åÔºå‰∏Ä‰∏™AddressTxÂØπÂ∫î‰∏ÄÊ¨°‰∏äË¥¶ËØ∑Ê±Ç
}

message CandidateTx {//ÂßîÊâò‰∫∫tx
  BaseMessage base = 1;
  OrderOp op = 2;
  string candidate_address = 3;
  string amount = 4;
}
message MinerTx {//ÂêØÂä®ÊàñÂÅúÊ≠¢ÊåñÁüø
  BaseMessage base = 1;
  OrderOp op = 2;//Â¶ÇÊûúÊòØADDÔºåÂàô‰ºöÁõ¥Êé•Êääis_minerÁΩÆ‰∏∫true,miner_locked_balance‰ºöË¢´amountÁ¥ØÂä†„ÄÇ
  string amount = 3;// Â¶ÇÊûúÊòØREMOVE‰∏çÈúÄË¶Å‰º†Ëøô‰∏™ÂèÇÊï∞Ôºåis_minerÁΩÆ‰∏∫false,miner_locked_balanceÁöÑÂÄºÁõ¥Êé•Á¥ØÂä†Âà∞TokenBalance.balance,miner_locked_balanceÁΩÆ0

}

// Êñ∞Â¢ûÔºö Áî® oneof Â∞ÅË£ÖÂêÑÁßç Tx
message AnyTx {
  oneof content {
    IssueTokenTx issue_token_tx = 1;
    FreezeTx freeze_tx = 2;
    Transaction transaction = 3;
    OrderTx order_tx = 4;
    RechargeTx address_tx = 5;
    CandidateTx candidate_tx = 6;
    MinerTx miner_tx = 8;
  }
}
// --------------------- ËÆ¢Âçï & Áä∂ÊÄÅÁõ∏ÂÖ≥ ---------------------
enum OrderOp {
  ADD = 0;
  REMOVE = 1;
}

enum Status {
  PENDING = 0;
  FAILED = 1;
  SUCCEED = 2;
}

// --------------------- ËäÇÁÇπ==ÂÆ¢Êà∑Á´Ø‰ø°ÊÅØ ---------------------
message NodeInfo {
  string public_key = 1;
  string ip = 2;
  bool isOnline = 4;
}

message NodeList {
  repeated NodeInfo nodes = 1;
}

message ClientInfo {
  string ip = 1;
  bool authed = 2;
  string public_key_pem = 3; // ÂÅáËÆæ‰ª• PEM Ê†ºÂºèÂ≠òÂÇ®ÂÖ¨Èí•Ôºå‰πüÂèØ‰ª•Â≠òÂÖ∂ÂÆÉÂΩ¢Âºè
}

// --------------------- Handshake & Áä∂ÊÄÅËØ∑Ê±Ç ---------------------
message HandshakeRequest {
  string client_id = 1; // Â∞±ÊòØÁüøÂ∑•Âú∞ÂùÄ
  string public_key = 2; // PEM
  string signature = 3;
}

message HandshakeResponse {
  string status = 1; // "handshake_ok" ÊàñÂÖ∂ÂÆÉ
}

message StatusRequest {
  // Â¶ÇÊûúËØ∑Ê±ÇÁ´ØÂï•ÈÉΩ‰∏çÈúÄË¶ÅÔºå‰πüÂèØ‰ª•ÁïôÁ©∫
}

message StatusResponse {
  string status = 1; // "ok"
  string info = 2;   // "Server is running"
}

// --------------------- ËäÇÁÇπÈó¥ÈÄöËÆØ‰ø°ÊÅØ ---------------------

// Áî®‰∫éËØ∑Ê±ÇÂÆåÊï¥‰∫§ÊòìÊï∞ÊçÆ
message GetData {
  string tx_id = 1;
}

message GetBlockRequest {
  uint64 height = 1;   // ÊÉ≥Ë¶ÅËé∑ÂèñÂì™‰∏™È´òÂ∫¶ÁöÑÂå∫Âùó
}
message GetBlockResponse {
  Block block = 1;     // Ëã•ÊàêÂäüÔºåÂàôËøîÂõûÂÆåÊï¥Âå∫Âùó
  string error = 2;    // Ëã•Âá∫ÈîôÔºåÊääÈîôËØØ‰ø°ÊÅØËøîÂõûÁªôËØ∑Ê±ÇÊñπ
}

message BatchGetShortTxRequest {
  repeated bytes short_hashes = 1; // ÊØè‰∏™ 8 Â≠óËäÇÁöÑÁü≠ hash
}

message BatchGetShortTxResponse {
  repeated AnyTx transactions = 1;
}
message CheckPointInfo {
  uint64 height = 1;
  string txs_hash = 2;
  bytes aggregate_signature = 3;
  bytes bit_map = 4;
  bytes txs = 5;// ÊâÄÊúâ‰∫§ÊòìÁöÑÁÆÄÁü≠tx_hash
}
// ÊãâÂèñÂÖ±ËØÜÁä∂ÊÄÅÁöÑËØ∑Ê±ÇÔºàÁõÆÂâç‰∏∫Á©∫ÔºåÂèØÊâ©Â±ïÔºâ
message GetConsensusStateRequest {}
// ---------------- Snowman networking ----------------
message PushQuery {
  string address = 1;
  // Timeout (ns) for this request
  uint64 deadline = 2;
  bool container_is_block = 3;// ‰∏∫trueË°®Á§∫ÊòØÂÆåÊï¥Êï∞ÊçÆÔºåfalseË°®Á§∫Âè™Êúâtxs
  bytes container = 4;// ÂΩìtxÊï∞ÈáèÂ∞è‰∫é2500Êó∂ÂÄôÔºåËØ•Â≠óÊÆµ‰∏∫ÊâÄÊúâ‰∫§ÊòìÁöÑÂÆåÊï¥‰∫åËøõÂà∂Êï∞ÊçÆÔºå>=2500ÁöÑÊó∂ÂÄô‰∏∫ ÊâÄÊúâ‰∫§ÊòìÁöÑÁÆÄÁü≠tx_hash
  // Requesting peer's last accepted height
  uint64 requested_height = 5;
  string block_id = 6;
}

// PullQuery requests the preferences of a remote peer given a container id.
//
// Remote peers should respond to a PullQuery with a Chits message
message PullQuery {
  string address = 1;
  // Timeout (ns) for this request
  uint64 deadline = 2;
  // Container id being gossiped
  string block_id = 3;
  // Requesting peer's last accepted height
  uint64 requested_height = 4;
}

// Chits contains the preferences of a peer in response to a PushQuery or
// PullQuery message.
message Chits {
  // ID of the currently preferred block
  string preferred_block = 2;
  // ID of the last accepted block
  string accepted_block = 3;
  // ID of the currently preferred block at the requested height
  string preferred_block_at_height = 4;
  // Last accepted block's height
  uint64 accepted_height = 5;
  bytes bitmap = 6;//‰∏ä‰∏Ä‰∏™Âå∫Âùó‰∏éËá™Â∑±ËÅîÁ≥ªËøáÁöÑËäÇÁÇπ
}

Êñá‰ª∂Ë∑ØÂæÑ: logs/log.go
Êñá‰ª∂ÂÜÖÂÆπ:
package logs

import (
	"log"
	"os"
)

// ÂÆö‰πâÊó•ÂøóÁ∫ßÂà´Â∏∏ÈáèÔºàÊï∞ÂÄºË∂äÂ§ßÔºåÁ∫ßÂà´Ë∂äÈ´òÔºâ
const (
	LevelTrace   = iota // 0ÔºàÊúÄ‰ΩéÔºåÊúÄËØ¶ÁªÜÔºâ
	LevelDebug          // 1
	LevelVerbose        // 2ÔºàÊñ∞Â¢ûÁ∫ßÂà´Ôºâ
	LevelInfo           // 3
	LevelWarning        // 4
	LevelError          // 5ÔºàÊúÄÈ´òÔºåÊúÄ‰∏•ÈáçÔºâ
)

var logLevel = LevelTrace // ÂÖ®Â±ÄÊó•ÂøóÁ∫ßÂà´ÔºàÁ§∫‰æãËÆæÁΩÆ‰∏∫ LevelVerboseÔºâ

// ÂÖ®Â±Ä Logger ÂÆû‰æã
var logger *Logger
var MyAddress = "0x0000000"
var IsCurrentLeader = ""

// Logger ÁªìÊûÑ‰Ωì
type Logger struct {
	traceLogger   *log.Logger
	debugLogger   *log.Logger
	verboseLogger *log.Logger
	infoLogger    *log.Logger
	warnLogger    *log.Logger
	errorLogger   *log.Logger
}

// ÂàùÂßãÂåñÂÖ®Â±Ä Logger ÂÆû‰æã
func init() {
	logger = &Logger{
		traceLogger:   log.New(os.Stdout, "[TRACE]   ", log.Ldate|log.Ltime|log.Lmicroseconds|log.Lshortfile),
		debugLogger:   log.New(os.Stdout, "[DEBUG]   ", log.Ldate|log.Ltime|log.Lmicroseconds|log.Lshortfile),
		verboseLogger: log.New(os.Stdout, "[VERBOSE] ", log.Ldate|log.Ltime|log.Lmicroseconds|log.Lshortfile),
		infoLogger:    log.New(os.Stdout, "[INFO]    ", log.Ldate|log.Ltime|log.Lmicroseconds|log.Lshortfile),
		warnLogger:    log.New(os.Stdout, "[WARN]    ", log.Ldate|log.Ltime|log.Lmicroseconds|log.Lshortfile),
		errorLogger:   log.New(os.Stderr, "[ERROR]   ", log.Ldate|log.Ltime|log.Lmicroseconds|log.Lshortfile),
	}
}

// ÂåÖÁ∫ßÂà´ÁöÑÊó•ÂøóÊñπÊ≥ï
func Trace(format string, v ...interface{}) {
	if logLevel <= LevelTrace {
		logger.traceLogger.Printf(IsCurrentLeader+" "+MyAddress[:7]+format, v...)
	}
}

func Debug(format string, v ...interface{}) {
	if logLevel <= LevelDebug {
		logger.debugLogger.Printf(IsCurrentLeader+" "+MyAddress[:7]+" "+format, v...)
	}
}

func Verbose(format string, v ...interface{}) {
	if logLevel <= LevelVerbose {
		logger.verboseLogger.Printf(IsCurrentLeader+" "+MyAddress[:7]+" "+format, v...)
	}
}

func Info(format string, v ...interface{}) {
	if logLevel <= LevelInfo {
		logger.infoLogger.Printf(IsCurrentLeader+" "+MyAddress[:7]+" "+format, v...)
	}
}

func Warn(format string, v ...interface{}) {
	if logLevel <= LevelWarning {
		logger.warnLogger.Printf(IsCurrentLeader+" "+MyAddress[:7]+" "+format, v...)
	}
}

func Error(format string, v ...interface{}) {
	if logLevel <= LevelError {
		logger.errorLogger.Printf(IsCurrentLeader+" "+MyAddress[:7]+" "+format, v...)
	}
}


Êñá‰ª∂Ë∑ØÂæÑ: main.go
Êñá‰ª∂ÂÜÖÂÆπ:
package main

import "dex/consensus"

func main() {
	consensus.RunLoop()
}


Êñá‰ª∂Ë∑ØÂæÑ: print/print_files.go
Êñá‰ª∂ÂÜÖÂÆπ:
package main

import (
	"dex/logs"
	"fmt"
	"os"
	"path/filepath"
	"strings"
)

// Á°¨ÁºñÁ†ÅÂºÄÂÖ≥ÔºöÊòØÂê¶ÂåÖÂê´ _test.go Êñá‰ª∂„ÄÇ
// ‰øÆÊîπ‰∏∫ true ÂàôÊâìÂç∞ _test.go Êñá‰ª∂Ôºå‰øÆÊîπ‰∏∫ false ÂàôË∑≥Ëøá _test.go Êñá‰ª∂„ÄÇ
var includeTestFiles = false

func main() {
	// ÂàõÂª∫(ÊàñË¶ÜÁõñ)‰∏Ä‰∏™ËæìÂá∫Êñá‰ª∂
	file, err := os.Create("output_dex.txt")
	if err != nil {
		logs.Error("Êó†Ê≥ïÂàõÂª∫ËæìÂá∫Êñá‰ª∂: %v", err)
	}
	defer file.Close()

	// Â¶ÇÊûúÂ∏åÊúõÊó•Âøó‰πü‰∏ÄËµ∑ÈáçÂÆöÂêëÂà∞Êñá‰ª∂ÔºåÂèØ‰ª•ÂèñÊ∂à‰∏ãÈù¢ÁöÑÊ≥®Èáä
	// log.SetOutput(file)

	dir := `./` // ÊåáÂÆöË¶ÅÈÅçÂéÜÁöÑÁõÆÂΩï
	printGoFiles(dir, file)
}

func printGoFiles(dir string, outFile *os.File) {
	files, err := os.ReadDir(dir)
	if err != nil {
		// Â∞ÜÈîôËØØ‰ø°ÊÅØÂÜôÂÖ• outFile
		fmt.Fprintf(outFile, "Êó†Ê≥ïËØªÂèñÁõÆÂΩï %s: %v\n", dir, err)
		return
	}

	for _, f := range files {
		path := filepath.Join(dir, f.Name())
		if f.IsDir() {
			// Â¶ÇÊûúÊòØÁõÆÂΩïÔºåÂàôÈÄíÂΩíË∞ÉÁî®
			printGoFiles(path, outFile)
		} else {
			// Â§ÑÁêÜ .proto Êñá‰ª∂ÔºåÁõ¥Êé•ÊâìÂç∞
			if strings.HasSuffix(f.Name(), ".proto") {
				printFile(path, outFile)
				continue
			}

			// Â§ÑÁêÜ .go Êñá‰ª∂ÔºàÊéíÈô§ .pb.go Êñá‰ª∂Ôºâ
			if strings.HasSuffix(f.Name(), ".go") && !strings.HasSuffix(f.Name(), ".pb.go") {
				// Ê†πÊçÆÂºÄÂÖ≥ÂÜ≥ÂÆöÊòØÂê¶Ë∑≥Ëøá _test.go Êñá‰ª∂
				if !includeTestFiles && strings.HasSuffix(f.Name(), "_test.go") && !strings.HasSuffix(f.Name(), "main_test.go") {
					continue
				}
				printFile(path, outFile)
			}
		}
	}
}

func printFile(path string, outFile *os.File) {
	// Â∞ÜÊñá‰ª∂Ë∑ØÂæÑÂÜôÂÖ•ËæìÂá∫Êñá‰ª∂
	fmt.Fprintf(outFile, "\nÊñá‰ª∂Ë∑ØÂæÑ: %s\n", path)

	// ËØªÂèñÂπ∂ÂÜôÂÖ•Êñá‰ª∂ÂÜÖÂÆπ
	content, err := os.ReadFile(path)
	if err != nil {
		fmt.Fprintf(outFile, "Êó†Ê≥ïËØªÂèñÊñá‰ª∂ %s: %v\n", path, err)
		return
	}
	fmt.Fprintf(outFile, "Êñá‰ª∂ÂÜÖÂÆπ:\n%s\n", string(content))
}

