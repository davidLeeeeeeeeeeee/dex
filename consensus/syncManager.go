package consensus

import (
	"context"
	"dex/interfaces"
	"dex/logs"
	"dex/pb"
	"dex/types"
	"fmt"
	"strings"
	"sync"
	"sync/atomic"
	"time"

	"google.golang.org/protobuf/proto"
)

// ============================================
// åŒæ­¥ç®¡ç†å™¨ - å¢å¼ºç‰ˆï¼ˆæ”¯æŒå¿«ç…§ï¼‰
// ============================================

type SyncManager struct {
	nodeID         types.NodeID
	node           *Node // æ–°å¢
	transport      interfaces.Transport
	store          interfaces.BlockStore
	config         *SyncConfig
	snapshotConfig *SnapshotConfig // æ–°å¢
	events         interfaces.EventBus
	Logger         logs.Logger
	SyncRequests   map[uint32]time.Time
	nextSyncID     uint32
	Syncing        bool
	Mu             sync.RWMutex
	PeerHeights    map[types.NodeID]uint64
	lastPoll       time.Time
	usingSnapshot  bool // æ ‡è®°æ˜¯å¦æ­£åœ¨ä½¿ç”¨å¿«ç…§åŒæ­¥
	// é‡‡æ ·éªŒè¯ç›¸å…³å­—æ®µ
	sampling        bool                    // æ˜¯å¦æ­£åœ¨é‡‡æ ·éªŒè¯
	sampleResponses map[types.NodeID]uint64 // é‡‡æ ·å“åº”: nodeID -> acceptedHeight
	sampleStartTime time.Time               // é‡‡æ ·å¼€å§‹æ—¶é—´
	// äº‹ä»¶é©±åŠ¨åŒæ­¥ç›¸å…³
	pendingBlockBuffer    *PendingBlockBuffer  // å¾…å¤„ç†åŒºå—ç¼“å†²åŒºï¼ˆç”¨äºè¡¥è¯¾ï¼‰
	consecutiveStallCount uint32               // è¿ç»­åŒæ­¥åœæ»è®¡æ•°ï¼ˆé«˜é£é™©ä¿®å¤ï¼šæ­»å¾ªç¯ä¿æŠ¤ï¼‰
	InFlightSyncRanges    map[string]time.Time // æ–°å¢ï¼šæ­£åœ¨è¿›è¡Œçš„åŒæ­¥é«˜åº¦èŒƒå›´ï¼ˆå»é‡ï¼‰
}

func NewSyncManager(id types.NodeID, transport interfaces.Transport, store interfaces.BlockStore, config *SyncConfig, snapshotConfig *SnapshotConfig, events interfaces.EventBus, logger logs.Logger) *SyncManager {
	return &SyncManager{
		nodeID:             id,
		transport:          transport,
		store:              store,
		config:             config,
		snapshotConfig:     snapshotConfig,
		events:             events,
		Logger:             logger,
		SyncRequests:       make(map[uint32]time.Time),
		PeerHeights:        make(map[types.NodeID]uint64),
		lastPoll:           time.Now(),
		sampleResponses:    make(map[types.NodeID]uint64),
		InFlightSyncRanges: make(map[string]time.Time),
	}
}

// SetPendingBlockBuffer è®¾ç½® PendingBlockBufferï¼ˆåœ¨åˆå§‹åŒ–åæ³¨å…¥ï¼‰
func (sm *SyncManager) SetPendingBlockBuffer(buffer *PendingBlockBuffer) {
	sm.pendingBlockBuffer = buffer
}

func (sm *SyncManager) Start(ctx context.Context) {
	// 1. æ¯éš” config.CheckInterval è¿›è¡Œä¸€æ¬¡å¸¸è§„åŒæ­¥æ£€æŸ¥ï¼ˆå…œåº•ï¼‰
	go func() {
		logs.SetThreadNodeContext(string(sm.nodeID))
		ticker := time.NewTicker(sm.config.CheckInterval)
		defer ticker.Stop()

		for {
			select {
			case <-ticker.C:
				sm.checkAndSync()
			case <-ctx.Done():
				return
			}
		}
	}()

	// 2. æ¯éš” config.CheckInterval è¿›è¡Œä¸€æ¬¡å¸¸è§„é«˜åº¦é‡‡æ ·ï¼ˆå…œåº•ï¼‰
	go func() {
		ticker := time.NewTicker(sm.config.CheckInterval)
		defer ticker.Stop()

		for {
			select {
			case <-ticker.C:
				sm.pollPeerHeights()
			case <-ctx.Done():
				return
			}
		}
	}()

	// 3. æ–°å¢ï¼šé«˜é¢‘å¥åº·æ£€æŸ¥å¾ªç¯ï¼ˆæ¯1ç§’ï¼‰ï¼Œç”¨äºå¤„ç†è¶…æ—¶æ¸…ç†å’Œä¸¢åŒ…æ¢å¤
	go func() {
		ticker := time.NewTicker(1 * time.Second)
		defer ticker.Stop()

		for {
			select {
			case <-ticker.C:
				sm.processTimeouts()
			case <-ctx.Done():
				return
			}
		}
	}()
}

// å®šæ—¶æŸ¥è¯¢å…¶ä»–èŠ‚ç‚¹é«˜åº¦
func (sm *SyncManager) pollPeerHeights() {
	// å¦‚æœæ­£åœ¨åŒæ­¥ä¸­ï¼Œæš‚åœè½®è¯¢é«˜åº¦ï¼Œé¿å…ç½‘ç»œæ‹¥å µ
	sm.Mu.RLock()
	if sm.Syncing {
		sm.Mu.RUnlock()
		return
	}
	sm.Mu.RUnlock()

	// é™åˆ¶è½®è¯¢é¢‘ç‡ï¼Œè½åæ—¶å¢åŠ æ¢æµ‹é¢‘ç‡
	cooldown := 2 * time.Second
	sm.Mu.RLock()
	_, accepted := sm.store.GetLastAccepted()
	maxPeer := uint64(0)
	for _, h := range sm.PeerHeights {
		if h > maxPeer {
			maxPeer = h
		}
	}
	if maxPeer > accepted+2 {
		cooldown = 500 * time.Millisecond // è½åè¾ƒå¤šæ—¶ï¼Œæ¯ 500ms æ¢æµ‹ä¸€æ¬¡
	}
	sm.Mu.RUnlock()

	if time.Since(sm.lastPoll) < cooldown {
		return
	}
	sm.lastPoll = time.Now()

	peers := sm.transport.SamplePeers(sm.nodeID, 10)
	for _, peer := range peers {
		sm.transport.Send(peer, types.Message{
			Type: types.MsgHeightQuery,
			From: sm.nodeID,
		})
	}
}

func (sm *SyncManager) HandleHeightQuery(msg types.Message) {
	_, height := sm.store.GetLastAccepted()
	currentHeight := sm.store.GetCurrentHeight()

	err := sm.transport.Send(types.NodeID(msg.From), types.Message{
		Type:          types.MsgHeightResponse,
		From:          sm.nodeID,
		Height:        height,
		CurrentHeight: currentHeight,
	})
	if err != nil {
		return
	}
}

func (sm *SyncManager) HandleHeightResponse(msg types.Message) {
	sm.Mu.Lock()
	defer sm.Mu.Unlock()
	// åŒæ­¥åº”è¯¥åŸºäºå¯¹æ–¹â€œå·²æœ€ç»ˆåŒ–/å·²æ¥å—â€çš„é«˜åº¦ï¼Œè€Œä¸æ˜¯å…¶å½“å‰æœ€å¤§é«˜åº¦ï¼ˆå¯èƒ½åŒ…å«å¤§é‡æœªæœ€ç»ˆåŒ–å—ï¼‰ã€‚
	// å¦åˆ™ä¼šå¯¼è‡´æœ¬èŠ‚ç‚¹ä¸æ–­å°è¯•åŒæ­¥ä¸€äº›å…¶å®å…¨ç½‘éƒ½è¿˜æ²¡æœ€ç»ˆåŒ–çš„é«˜åº¦ï¼Œå‡ºç°é‡å¤ sync ä¸” added=0 çš„æƒ…å†µã€‚
	sm.PeerHeights[types.NodeID(msg.From)] = msg.Height

	// å¦‚æœæ­£åœ¨é‡‡æ ·éªŒè¯ï¼Œæ”¶é›†å“åº”
	if sm.sampling {
		sm.sampleResponses[types.NodeID(msg.From)] = msg.Height
	}
}

// processTimeouts å¿«é€Ÿæ¸…ç†è¶…æ—¶çš„è¯·æ±‚ï¼ˆç”± 1s çš„å¥åº·å¾ªç¯è°ƒç”¨ï¼‰
func (sm *SyncManager) processTimeouts() {
	sm.Mu.Lock()
	defer sm.Mu.Unlock()

	if !sm.Syncing && !sm.sampling {
		return
	}

	now := time.Now()

	// æ£€æŸ¥åŒæ­¥è¯·æ±‚è¶…æ—¶
	if sm.Syncing {
		hasTimeout := false
		for syncID, startTime := range sm.SyncRequests {
			if now.Sub(startTime) > sm.config.Timeout {
				Logf("[Node %s] âš ï¸ Sync request %d timed out (started at %v)\n",
					sm.nodeID, syncID, startTime.Format("15:04:05"))
				delete(sm.SyncRequests, syncID)
				hasTimeout = true
			}
		}

		if hasTimeout && len(sm.SyncRequests) == 0 {
			logs.Warn("[Node %s] All sync requests timed out, resetting Syncing flag", sm.nodeID)
			sm.Syncing = false
			sm.usingSnapshot = false
		}
	}

	// æ£€æŸ¥é«˜åº¦é‡‡æ ·è¶…æ—¶
	if sm.sampling {
		if now.Sub(sm.sampleStartTime) > sm.config.SampleTimeout {
			logs.Debug("[Node %s] Sample verification timed out, resetting sampling flag", sm.nodeID)
			sm.sampling = false
		}
	}

	// æ–°å¢ï¼šæ£€æŸ¥å¹¶æ¸…ç†è¿‡æœŸçš„å¤„ç†ä¸­åŒæ­¥èŒƒå›´ï¼ˆå…œåº•ï¼‰
	for rangeKey, startTime := range sm.InFlightSyncRanges {
		if now.Sub(startTime) > sm.config.Timeout*2 {
			delete(sm.InFlightSyncRanges, rangeKey)
		}
	}
}

// æ£€æŸ¥æ˜¯å¦æœ‰å¿…è¦å¯åŠ¨åŒæ­¥ç¨‹åº
func (sm *SyncManager) checkAndSync() {
	sm.Mu.Lock()

	// æ­£åœ¨åŒæ­¥æœŸé—´ä¸å¯åŠ¨æ–°çš„æ£€æŸ¥
	if sm.Syncing {
		sm.Mu.Unlock()
		return
	}

	// 1. ä¼˜å…ˆæ£€æŸ¥é‡‡æ ·éªŒè¯çŠ¶æ€ï¼ˆé‡‡æ ·å‘å‡ºåéœ€è¦åœ¨æ­¤è¯„ä¼°ç»“æœï¼‰
	if sm.sampling {
		// æ£€æŸ¥é‡‡æ ·æ˜¯å¦è¶…æ—¶
		if time.Since(sm.sampleStartTime) > sm.config.SampleTimeout {
			logs.Debug("[Sync] Sample verification timed out, responses=%d", len(sm.sampleResponses))
			sm.sampling = false
			sm.Mu.Unlock()
			return
		}

		// è¯„ä¼° Quorum
		quorumHeight, ok := sm.evaluateSampleQuorum()
		if !ok {
			// å°šæœªè¾¾åˆ° Quorumï¼Œç­‰å¾…æ›´å¤šå“åº”
			sm.Mu.Unlock()
			return
		}

		// Quorum è¾¾æˆï¼Œå¯ä»¥å®‰å…¨åŒæ­¥
		sm.sampling = false
		_, localAcceptedHeight := sm.store.GetLastAccepted()

		if quorumHeight > localAcceptedHeight {
			logs.Info("[Sync] âœ… Quorum verified: target height %d confirmed by %.0f%% nodes",
				quorumHeight, sm.config.QuorumRatio*100)
			sm.Mu.Unlock()

			// åˆ¤æ–­ä½¿ç”¨å¿«ç…§è¿˜æ˜¯æ™®é€šåŒæ­¥
			heightDiff := quorumHeight - localAcceptedHeight
			if sm.snapshotConfig.Enabled && heightDiff > sm.config.SnapshotThreshold {
				sm.requestSnapshotSync(quorumHeight)
			} else {
				sm.requestSync(localAcceptedHeight+1, minUint64(localAcceptedHeight+sm.config.BatchSize, quorumHeight))
			}
			return
		}

		sm.Mu.Unlock()
		return
	}

	// 2. åˆæ­¥æ£€æŸ¥æ˜¯å¦è½å
	maxPeerHeight := uint64(0)
	for _, height := range sm.PeerHeights {
		if height > maxPeerHeight {
			maxPeerHeight = height
		}
	}

	_, localAcceptedHeight := sm.store.GetLastAccepted()
	heightDiff := uint64(0)
	if maxPeerHeight > localAcceptedHeight {
		heightDiff = maxPeerHeight - localAcceptedHeight
	}

	// 3. å¦‚æœè½åè¶…è¿‡é˜ˆå€¼ï¼Œå¯åŠ¨é‡‡æ ·éªŒè¯
	if heightDiff > sm.config.BehindThreshold {
		logs.Debug("[Sync] Detected lag of %d blocks, starting sample verification (target=%d)",
			heightDiff, maxPeerHeight)
		sm.startHeightSampling()
	}

	sm.Mu.Unlock()
}

// TriggerSyncFromChit ç”± Chits æ¶ˆæ¯é©±åŠ¨çš„åŒæ­¥è§¦å‘å…¥å£ï¼ˆäº‹ä»¶é©±åŠ¨æ¨¡å¼ï¼‰
// æ— éœ€å¤æ‚çš„é‡‡æ ·éªŒè¯ï¼Œå› ä¸º Chits æœ¬èº«å°±æ˜¯å…±è¯†é‡‡æ ·çš„ä¸€éƒ¨åˆ†
func (sm *SyncManager) TriggerSyncFromChit(peerAcceptedHeight uint64, from types.NodeID) {
	sm.Mu.Lock()

	// æ›´æ–° PeerHeights
	if peerAcceptedHeight > sm.PeerHeights[from] {
		sm.PeerHeights[from] = peerAcceptedHeight
	}

	// æ™ºèƒ½åˆ¤å®šæ˜¯å¦éœ€è¦æ–°ä¸€è½®åŒæ­¥ï¼š
	// è‹¥å·²å¿™ç¢Œï¼Œåˆ™æ£€æŸ¥å½“å‰æ˜¯å¦æœ‰æ­£åœ¨è¿›è¡Œçš„è¯·æ±‚ã€‚å¦‚æœæ‰€æœ‰è¯·æ±‚éƒ½è¶…æ—¶äº†ï¼Œå…è®¸ç»§ç»­ã€‚
	if sm.Syncing || sm.sampling {
		if len(sm.SyncRequests) > 0 {
			// æœ‰åœ¨é€”è¯·æ±‚ï¼Œç­‰å¾…å®Œæˆ
			sm.Mu.Unlock()
			return
		}
		// P0 ä¿®å¤ï¼šSyncing=true ä½† SyncRequests ä¸ºç©º â†’ ä¸Šä¸€è½®åŒæ­¥å·²å®è´¨ç»“æŸ
		// ï¼ˆå¯èƒ½æ˜¯å“åº”ä¸¢åŒ…ã€è¶…æ—¶æ¸…ç†åé—ç•™çš„è„çŠ¶æ€ï¼‰ã€‚
		// å¿…é¡»é‡ç½® Syncingï¼Œå¦åˆ™åç»­çš„ requestSync/requestSyncParallel
		// ä¼šæ£€æŸ¥ Syncing=true åç›´æ¥ returnï¼Œå¯¼è‡´åŒæ­¥æ°¸ä¹…å¡æ­»ã€‚
		logs.Info("[SyncManager] TriggerSyncFromChit: resetting stale Syncing flag (Requests=0)")
		sm.Syncing = false
		sm.sampling = false
	}

	_, localAccepted := sm.store.GetLastAccepted()
	if peerAcceptedHeight <= localAccepted {
		sm.Mu.Unlock()
		return
	}

	heightDiff := peerAcceptedHeight - localAccepted
	sm.Mu.Unlock()

	logs.Debug("[SyncManager] TriggerSyncFromChit: peer=%s peerHeight=%d localAccepted=%d diff=%d",
		from, peerAcceptedHeight, localAccepted, heightDiff)

	// ä¸­é£é™©ä¿®å¤ï¼šè½»é‡çº§é«˜åº¦é‡‡æ ·éªŒè¯
	// é‡‡æ · 2 ä¸ªéšæœºèŠ‚ç‚¹ï¼ˆä¸åŒ…æ‹¬å‘é€è€…ï¼‰æ¥äº¤å‰ç¡®è®¤é«˜åº¦ã€‚
	// è¿™èƒ½é˜²æ­¢è¢«å•ä¸ªæ¶æ„æˆ–æ•…éšœèŠ‚ç‚¹æ‹‰å…¥é”™è¯¯çš„åŒæ­¥è½¨é“ã€‚
	go func() {
		logs.Debug("[SyncManager] Starting lightweight safety sampling for TriggerSyncFromChit (target=%d)", peerAcceptedHeight)

		peers := sm.transport.SamplePeers(sm.nodeID, 2)
		// å¦‚æœç½‘ç»œå¤ªå°æ²¡æ³•é‡‡æ ·ï¼Œåˆ™ä¿¡ä»»è¯¥ Chitï¼ˆæ­¤æ—¶å…±è¯†å®‰å…¨æ€§ç”± BFT ä¿è¯ï¼‰
		if len(peers) <= 1 {
			sm.performTriggeredSync(peerAcceptedHeight, localAccepted, heightDiff)
			return
		}

		// å‘èµ·é«˜åº¦æŸ¥è¯¢
		for _, p := range peers {
			if p == from {
				continue
			}
			sm.transport.Send(p, types.Message{
				Type: types.MsgHeightQuery,
				From: sm.nodeID,
			})
		}

		// ç­‰å¾… 300ms è§‚å¯Ÿå“åº”ï¼ˆé€šè¿‡ HandleHeightResponse æ›´æ–° PeerHeightsï¼‰
		time.Sleep(300 * time.Millisecond)

		// æ£€æŸ¥æ˜¯å¦æœ‰å…¶ä»–èŠ‚ç‚¹ä¹Ÿè¾¾åˆ°äº†è¯¥é«˜åº¦é™„è¿‘
		sm.Mu.RLock()
		maxOtherPeerHeight := uint64(0)
		for pID, h := range sm.PeerHeights {
			if pID != from && h > maxOtherPeerHeight {
				maxOtherPeerHeight = h
			}
		}
		sm.Mu.RUnlock()

		// å®‰å…¨é˜ˆå€¼ï¼šåªè¦æœ‰ä»»ä½•ä¸€ä¸ªå…¶ä»–èŠ‚ç‚¹ä¹Ÿå¤„äºç±»ä¼¼é«˜åº¦ï¼ˆæˆ–æ›´é«˜ï¼‰ï¼Œå³è®¤ä¸ºå®‰å…¨
		if maxOtherPeerHeight >= peerAcceptedHeight-1 {
			sm.performTriggeredSync(peerAcceptedHeight, localAccepted, heightDiff)
		} else {
			logs.Warn("[SyncManager] ğŸ›¡ï¸ Lightweight sampling failed: no other peer confirmed height %d (maxOther=%d). Sync cancelled.",
				peerAcceptedHeight, maxOtherPeerHeight)
		}
	}()
}

// performTriggeredSync æ‰§è¡Œè¢«è§¦å‘çš„åŒæ­¥åŠ¨ä½œ
func (sm *SyncManager) performTriggeredSync(peerAcceptedHeight, localAccepted, heightDiff uint64) {
	if sm.snapshotConfig.Enabled && heightDiff > sm.config.SnapshotThreshold {
		sm.requestSnapshotSync(peerAcceptedHeight)
	} else {
		// ä½¿ç”¨åˆ†ç‰‡å¹¶è¡ŒåŒæ­¥
		sm.requestSyncParallel(localAccepted+1, minUint64(localAccepted+sm.config.BatchSize, peerAcceptedHeight))
	}
}

// requestSyncParallel åˆ†ç‰‡å¹¶è¡ŒåŒæ­¥ï¼šå°†é«˜åº¦èŒƒå›´åˆ†é…ç»™å¤šä¸ªèŠ‚ç‚¹åŒæ—¶è¯·æ±‚
func (sm *SyncManager) requestSyncParallel(fromHeight, toHeight uint64) {
	sm.Mu.Lock()
	if sm.Syncing {
		sm.Mu.Unlock()
		return
	}

	// å…¨å±€èŒƒå›´å»é‡
	rangeKey := fmt.Sprintf("%d-%d", fromHeight, toHeight)
	if startTime, exists := sm.InFlightSyncRanges[rangeKey]; exists {
		if time.Since(startTime) < sm.config.Timeout {
			sm.Mu.Unlock()
			return
		}
	}

	sm.Syncing = true
	sm.InFlightSyncRanges[rangeKey] = time.Now()
	sm.Mu.Unlock()

	// è·å–å¯ç”¨èŠ‚ç‚¹
	peers := sm.transport.SamplePeers(sm.nodeID, sm.config.ParallelPeers)
	if len(peers) == 0 {
		sm.Mu.Lock()
		sm.Syncing = false
		sm.Mu.Unlock()
		return
	}

	totalBlocks := toHeight - fromHeight + 1

	// å¦‚æœè¯·æ±‚èŒƒå›´å°æˆ–åªæœ‰1ä¸ªèŠ‚ç‚¹ï¼Œé€€åŒ–åˆ°æ™®é€šåŒæ­¥
	if totalBlocks <= 5 || len(peers) == 1 {
		sm.Mu.Lock()
		sm.Syncing = false
		delete(sm.InFlightSyncRanges, rangeKey) // æ¸…ç† keyï¼Œå¦åˆ™ requestSync ä¼šè¢«åŒä¸€ä¸ª key é˜»å¡
		sm.Mu.Unlock()
		sm.requestSync(fromHeight, toHeight)
		return
	}

	// è®¡ç®—æ¯ä¸ªèŠ‚ç‚¹è´Ÿè´£çš„é«˜åº¦èŒƒå›´
	rangePerPeer := totalBlocks / uint64(len(peers))

	logs.Info("[SyncManager] Starting parallel sync: heights %d-%d across %d peers",
		fromHeight, toHeight, len(peers))

	for i, peer := range peers {
		start := fromHeight + uint64(i)*rangePerPeer
		end := start + rangePerPeer - 1
		if i == len(peers)-1 {
			end = toHeight // æœ€åä¸€ä¸ªèŠ‚ç‚¹è´Ÿè´£å‰©ä½™
		}

		// ä¸ºæ¯ä¸ªåˆ†ç‰‡åˆ›å»ºç‹¬ç«‹çš„ SyncID
		syncID := atomic.AddUint32(&sm.nextSyncID, 1)
		sm.Mu.Lock()
		sm.SyncRequests[syncID] = time.Now()
		sm.Mu.Unlock()

		// åˆ¤æ–­æ˜¯å¦ä½¿ç”¨ ShortTxs æ¨¡å¼ï¼ˆåŸºäºæ€»è½åé‡è€Œéåˆ†ç‰‡å¤§å°ï¼‰
		useShortMode := totalBlocks <= sm.config.ShortSyncThreshold

		go func(p types.NodeID, s, e uint64, id uint32, shortMode bool) {
			logs.Debug("[SyncManager] Parallel shard: peer=%s heights=%d-%d shortMode=%v", p, s, e, shortMode)

			msg := types.Message{
				Type:          types.MsgSyncRequest,
				From:          sm.nodeID,
				SyncID:        id,
				FromHeight:    s,
				ToHeight:      e,
				SyncShortMode: shortMode,
			}
			sm.transport.Send(p, msg)
		}(peer, start, end, syncID, useShortMode)
	}
}

// startHeightSampling å¯åŠ¨é‡‡æ ·éªŒè¯ï¼ˆå¿…é¡»æŒæœ‰å†™é”è°ƒç”¨ï¼‰
func (sm *SyncManager) startHeightSampling() {
	sm.sampling = true
	sm.sampleResponses = make(map[types.NodeID]uint64)
	sm.sampleStartTime = time.Now()

	// é‡‡æ · K ä¸ªèŠ‚ç‚¹
	peers := sm.transport.SamplePeers(sm.nodeID, sm.config.SampleSize)
	for _, peer := range peers {
		sm.transport.Send(peer, types.Message{
			Type: types.MsgHeightQuery,
			From: sm.nodeID,
		})
	}
}

// evaluateSampleQuorum è¯„ä¼°é‡‡æ · Quorumï¼ˆå¿…é¡»æŒæœ‰è¯»é”è°ƒç”¨ï¼‰
// è¿”å›æ»¡è¶³ Quorum çš„æœ€é«˜å·²æœ€ç»ˆåŒ–é«˜åº¦
func (sm *SyncManager) evaluateSampleQuorum() (uint64, bool) {
	responseCount := len(sm.sampleResponses)
	if responseCount == 0 {
		return 0, false
	}

	// è®¡ç®— Quorum é˜ˆå€¼
	required := int(float64(sm.config.SampleSize) * sm.config.QuorumRatio)
	if required < 1 {
		required = 1
	}

	// å¦‚æœå“åº”ä¸è¶³ï¼Œç»§ç»­ç­‰å¾…
	if responseCount < required {
		return 0, false
	}

	// æ”¶é›†æ‰€æœ‰é«˜åº¦å¹¶æ’åº
	heights := make([]uint64, 0, responseCount)
	for _, h := range sm.sampleResponses {
		heights = append(heights, h)
	}

	// ä»é«˜åˆ°ä½æ‰¾åˆ°æ»¡è¶³ Quorum çš„æœ€é«˜é«˜åº¦
	// å¯¹äºæ¯ä¸ªå€™é€‰é«˜åº¦ï¼Œç»Ÿè®¡æœ‰å¤šå°‘èŠ‚ç‚¹çš„ acceptedHeight >= è¯¥é«˜åº¦
	var maxQuorumHeight uint64
	for _, candidateHeight := range heights {
		supportCount := 0
		for _, h := range sm.sampleResponses {
			if h >= candidateHeight {
				supportCount++
			}
		}
		if supportCount >= required && candidateHeight > maxQuorumHeight {
			maxQuorumHeight = candidateHeight
		}
	}

	if maxQuorumHeight > 0 {
		logs.Debug("[Sync] Quorum check: %d/%d nodes support height %d (required=%d)",
			len(sm.sampleResponses), sm.config.SampleSize, maxQuorumHeight, required)
		return maxQuorumHeight, true
	}

	return 0, false
}

// è¯·æ±‚å¿«ç…§åŒæ­¥
func (sm *SyncManager) requestSnapshotSync(targetHeight uint64) {
	sm.Mu.Lock()
	if sm.Syncing {
		sm.Mu.Unlock()
		return
	}
	sm.Syncing = true
	sm.usingSnapshot = true
	syncID := atomic.AddUint32(&sm.nextSyncID, 1)
	sm.SyncRequests[syncID] = time.Now()
	// è®°å½•å¤„ç†ä¸­èŒƒå›´
	rangeKey := fmt.Sprintf("snapshot_%d", targetHeight)
	sm.InFlightSyncRanges[rangeKey] = time.Now()
	sm.Mu.Unlock()

	// æ‰¾ä¸€ä¸ªé«˜åº¦è¶³å¤Ÿçš„èŠ‚ç‚¹
	sm.Mu.RLock()
	var targetPeer types.NodeID = "-1"
	for peer, height := range sm.PeerHeights {
		if height >= targetHeight {
			targetPeer = peer
			break
		}
	}
	sm.Mu.RUnlock()

	if targetPeer == "-1" {
		peers := sm.transport.SamplePeers(sm.nodeID, 5)
		if len(peers) > 0 {
			targetPeer = peers[0]
		}
	}

	if targetPeer != "-1" {
		Logf("[Node %s] ğŸ“¸ Requesting SNAPSHOT sync from Node %s (behind by %d blocks)\n",
			sm.nodeID, targetPeer, targetHeight-sm.store.GetCurrentHeight())

		msg := types.Message{
			Type:            types.MsgSnapshotRequest,
			From:            sm.nodeID,
			SyncID:          syncID,
			RequestSnapshot: true,
			Height:          targetHeight,
		}
		sm.transport.Send(targetPeer, msg)
	} else {
		sm.Mu.Lock()
		sm.Syncing = false
		sm.usingSnapshot = false
		delete(sm.SyncRequests, syncID)
		sm.Mu.Unlock()
	}
}

func (sm *SyncManager) requestSync(fromHeight, toHeight uint64) {
	sm.Mu.Lock()
	if sm.Syncing {
		sm.Mu.Unlock()
		return
	}

	// å»é‡æ£€æŸ¥
	rangeKey := fmt.Sprintf("%d-%d", fromHeight, toHeight)
	if startTime, exists := sm.InFlightSyncRanges[rangeKey]; exists {
		if time.Since(startTime) < sm.config.Timeout {
			sm.Mu.Unlock()
			return
		}
	}

	sm.Syncing = true
	syncID := atomic.AddUint32(&sm.nextSyncID, 1)
	sm.SyncRequests[syncID] = time.Now()
	sm.InFlightSyncRanges[rangeKey] = time.Now()
	sm.Mu.Unlock()

	sm.Mu.RLock()
	var targetPeer types.NodeID = "-1"
	for peer, height := range sm.PeerHeights {
		if height >= toHeight {
			targetPeer = peer
			break
		}
	}
	sm.Mu.RUnlock()

	if targetPeer == "-1" {
		peers := sm.transport.SamplePeers(sm.nodeID, 5)
		if len(peers) > 0 {
			targetPeer = peers[0]
		}
	}

	if targetPeer != "-1" {
		Logf("[Node %s] Requesting sync from Node %s for heights %d-%d\n",
			sm.nodeID, targetPeer, fromHeight, toHeight)

		msg := types.Message{
			Type:       types.MsgSyncRequest,
			From:       sm.nodeID,
			SyncID:     syncID,
			FromHeight: fromHeight,
			ToHeight:   toHeight,
		}
		sm.transport.Send(targetPeer, msg)
	} else {
		sm.Mu.Lock()
		sm.Syncing = false
		delete(sm.SyncRequests, syncID)
		sm.Mu.Unlock()
	}
}

// å¤„ç†å¿«ç…§è¯·æ±‚ï¼ˆæ–°å¢ï¼‰
func (sm *SyncManager) HandleSnapshotRequest(msg types.Message) {
	// è·å–æœ€è¿‘çš„å¿«ç…§
	snapshot, exists := sm.store.GetLatestSnapshot()
	if !exists {
		// å¦‚æœæ²¡æœ‰å¿«ç…§ï¼Œé™çº§åˆ°æ™®é€šåŒæ­¥
		sm.HandleSyncRequest(types.Message{
			Type:       types.MsgSyncRequest,
			From:       msg.From,
			SyncID:     msg.SyncID,
			FromHeight: 1,
			ToHeight:   minUint64(100, sm.store.GetCurrentHeight()),
		})
		return
	}

	Logf("[Node %s] ğŸ“¸ Sending snapshot (height %d) to Node %s\n",
		sm.nodeID, snapshot.Height, msg.From)

	// æ›´æ–°ç»Ÿè®¡
	if sm.node != nil {
		sm.node.Stats.Mu.Lock()
		sm.node.Stats.SnapshotsServed++
		sm.node.Stats.Mu.Unlock()
	}

	response := types.Message{
		Type:           types.MsgSnapshotResponse,
		From:           sm.nodeID,
		SyncID:         msg.SyncID,
		Snapshot:       snapshot,
		SnapshotHeight: snapshot.Height,
	}

	sm.transport.Send(types.NodeID(msg.From), response)
}

// å¤„ç†å¿«ç…§å“åº”ï¼ˆæ–°å¢ï¼‰
func (sm *SyncManager) HandleSnapshotResponse(msg types.Message) {
	sm.Mu.Lock()
	defer sm.Mu.Unlock()

	if _, ok := sm.SyncRequests[msg.SyncID]; !ok {
		return
	}

	delete(sm.SyncRequests, msg.SyncID)

	if msg.Snapshot == nil {
		sm.Syncing = false
		sm.usingSnapshot = false
		return
	}

	// åŠ è½½å¿«ç…§
	err := sm.store.LoadSnapshot(msg.Snapshot)
	if err != nil {
		Logf("[Node %s] Failed to load snapshot: %v\n", sm.nodeID, err)
		sm.Syncing = false
		sm.usingSnapshot = false
		return
	}

	// æ›´æ–°ç»Ÿè®¡
	if sm.node != nil {
		sm.node.Stats.Mu.Lock()
		sm.node.Stats.SnapshotsUsed++
		sm.node.Stats.Mu.Unlock()
	}

	Logf("[Node %s] ğŸ“¸ Successfully loaded snapshot at height %d\n",
		sm.nodeID, msg.SnapshotHeight)

	// å‘å¸ƒå¿«ç…§åŠ è½½äº‹ä»¶
	sm.events.PublishAsync(types.BaseEvent{
		EventType: types.EventSnapshotLoaded,
		EventData: msg.Snapshot,
	})

	// ç»§ç»­åŒæ­¥å¿«ç…§ä¹‹åçš„åŒºå—
	currentHeight := sm.store.GetCurrentHeight()
	maxPeerHeight := uint64(0)
	for _, height := range sm.PeerHeights {
		if height > maxPeerHeight {
			maxPeerHeight = height
		}
	}

	sm.Syncing = false
	sm.usingSnapshot = false

	// å¦‚æœè¿˜éœ€è¦æ›´å¤šåŒºå—ï¼Œç»§ç»­æ™®é€šåŒæ­¥
	if maxPeerHeight > currentHeight+1 {
		go func() {
			time.Sleep(100 * time.Millisecond)
			sm.requestSync(currentHeight+1, minUint64(currentHeight+sm.config.BatchSize, maxPeerHeight))
		}()
	}
}

func (sm *SyncManager) HandleSyncRequest(msg types.Message) {
	blocks := sm.store.GetBlocksFromHeight(msg.FromHeight, msg.ToHeight)

	Logf("[Node %s] Received sync request from Node %s for heights %d-%d (found %d blocks, shortMode=%v)\n",
		sm.nodeID, msg.From, msg.FromHeight, msg.ToHeight, len(blocks), msg.SyncShortMode)

	response := types.Message{
		Type:          types.MsgSyncResponse,
		From:          sm.nodeID,
		SyncID:        msg.SyncID,
		Blocks:        blocks,
		FromHeight:    msg.FromHeight,
		ToHeight:      msg.ToHeight,
		SyncShortMode: msg.SyncShortMode,
	}

	// é™„å¸¦ç­¾åé›†åˆï¼ˆVRF å…±è¯†è¯æ®ï¼‰
	if realStore, ok := sm.store.(*RealBlockStore); ok {
		sigSets := make(map[uint64][]byte)
		for _, block := range blocks {
			if block == nil {
				continue
			}
			if sigSet, exists := realStore.GetSignatureSet(block.Header.Height); exists {
				data, err := proto.Marshal(sigSet)
				if err == nil {
					sigSets[block.Header.Height] = data
				}
			}
		}
		if len(sigSets) > 0 {
			response.SignatureSets = sigSets
		}
	}

	// è‡ªé€‚åº”ä¼ è¾“æ¨¡å¼
	if msg.SyncShortMode {
		// çŸ­æœŸè½åæ¨¡å¼ï¼šé™„å¸¦ ShortTxs ç”¨äºå¿«é€Ÿè¿˜åŸ
		response.BlocksShortTxs = make(map[string][]byte)
		for _, block := range blocks {
			if block == nil {
				continue
			}
			if cachedBlock, exists := GetCachedBlock(block.ID); exists && cachedBlock != nil {
				response.BlocksShortTxs[block.ID] = cachedBlock.ShortTxs
			}
		}
	} else {
		for _, block := range blocks {
			if block == nil {
				continue
			}
			if cachedBlock, exists := GetCachedBlock(block.ID); exists && cachedBlock != nil {
				_ = cachedBlock // å·²æœ‰ç¼“å­˜æ•°æ®
			}
		}
	}

	sm.transport.Send(types.NodeID(msg.From), response)
}

func (sm *SyncManager) HandleSyncResponse(msg types.Message) {
	sm.Mu.Lock()
	if _, ok := sm.SyncRequests[msg.SyncID]; !ok {
		sm.Mu.Unlock()
		return
	}
	delete(sm.SyncRequests, msg.SyncID)
	// å¤„ç†å®Œæˆåæ¸…ç† rangeï¼ˆæˆ–è€…ä¿ç•™ä¸€æ®µæ—¶é—´ç”± timeout æ¸…ç†ï¼‰
	rangeKey := fmt.Sprintf("%d-%d", msg.FromHeight, msg.ToHeight)
	// ç”±äºå¹¶è¡ŒåŒæ­¥ä¼šæœ‰å­èŒƒå›´ï¼Œè¿™é‡Œç®€å•å¤„ç†ï¼Œå®é™…æ¨èç”± timeout è‡ªåŠ¨è¿‡æœŸï¼Œè¿™é‡Œä»…ç¤ºä¾‹
	delete(sm.InFlightSyncRanges, rangeKey)
	sm.Mu.Unlock()

	added := 0
	addErrs := 0
	var firstAddErr error
	var firstAddErrBlockID string
	for _, block := range msg.Blocks {
		if block == nil {
			continue
		}

		// å¦‚æœæ˜¯ ShortMode ä¸”æœ‰ ShortTxs æ•°æ®ï¼Œæå‰äº¤ç»™ PendingBlockBuffer å¤„ç†
		if msg.SyncShortMode && len(msg.BlocksShortTxs) > 0 {
			shortTxs := msg.BlocksShortTxs[block.ID]
			if len(shortTxs) > 0 && sm.pendingBlockBuffer != nil {
				// ä½¿ç”¨ ShortTxs å°è¯•è¿˜åŸåŒºå—
				sm.pendingBlockBuffer.AddPendingBlockForConsensus(block, shortTxs, types.NodeID(msg.From), 0, nil)
				added++ // ä¹è§‚è®¡æ•°ï¼Œå®é™…è¿˜åŸç”± buffer å¼‚æ­¥å®Œæˆ
				continue
			}
		}

		// å°è¯•ç›´æ¥æ·»åŠ 
		isNew, err := sm.store.Add(block)
		if err != nil {
			addErrs++
			if firstAddErr == nil {
				firstAddErr = err
				firstAddErrBlockID = block.ID
			}
			// æ¥å…¥è¡¥è¯¾æœºåˆ¶ï¼šå¦‚æœæ˜¯æ•°æ®ä¸å®Œæ•´å¯¼è‡´çš„å¤±è´¥ï¼ŒåŠ å…¥ PendingBlockBuffer
			if strings.Contains(err.Error(), "block data incomplete") && sm.pendingBlockBuffer != nil {
				logs.Debug("[SyncManager] Block %s incomplete, queueing for async resolution", block.ID)
				// å°è¯•ä½¿ç”¨å“åº”ä¸­çš„ ShortTxsï¼ˆå¦‚æœæœ‰ï¼‰
				shortTxs := msg.BlocksShortTxs[block.ID]
				sm.pendingBlockBuffer.AddPendingBlockForConsensus(block, shortTxs, types.NodeID(msg.From), 0, nil)
			}
			continue
		}
		if isNew {
			added++
		}
	}

	if added > 0 {
		Logf("[Node %s] ğŸ“¦ Successfully synced %d new blocks (heights %d-%d)\n",
			sm.nodeID, added, msg.FromHeight, msg.ToHeight)
	}
	if addErrs > 0 {
		logs.Warn("[Node %s] Sync received %d blocks, %d failed to add (first=%s err=%v)",
			sm.nodeID, len(msg.Blocks), addErrs, firstAddErrBlockID, firstAddErr)
	}
	if added == 0 && len(msg.Blocks) > 0 && addErrs == 0 {
		// ä¼˜åŒ–ï¼šæ ¹æ®å½“å‰çŠ¶æ€è°ƒæ•´æ—¥å¿—çº§åˆ«ã€‚å¦‚æœæ˜¯æ—©äºæˆ–ç­‰äºå½“å‰é«˜åº¦çš„åŒæ­¥ï¼Œä½¿ç”¨ Debugã€‚
		_, acceptedHeight := sm.store.GetLastAccepted()
		if msg.ToHeight <= acceptedHeight {
			logs.Debug("[Node %s] Sync received %d blocks but none were new (heights %d-%d, current accepted=%d)",
				sm.nodeID, len(msg.Blocks), msg.FromHeight, msg.ToHeight, acceptedHeight)
		} else {
			logs.Warn("[Node %s] Sync received %d blocks but none were new (heights %d-%d, current accepted=%d)",
				sm.nodeID, len(msg.Blocks), msg.FromHeight, msg.ToHeight, acceptedHeight)
		}
	}

	// åŠ é€Ÿè¿½å—ï¼šå¦‚æœæ”¶åˆ°çš„æ˜¯å¯¹æ–¹"å·²æ¥å—é«˜åº¦"èŒƒå›´å†…çš„åŒºå—ï¼Œåˆ™å¯ç›´æ¥æŒ‰çˆ¶é“¾å…³ç³»æ¨è¿›æœ¬åœ° lastAcceptedã€‚
	// è¿™èƒ½è§£å†³"æœ¬åœ°å·²æ‹¥æœ‰åŒºå—ä½†å…±è¯†è¿Ÿè¿Ÿæ— æ³•åœ¨è¯¥é«˜åº¦æ”¶æ•›"å¯¼è‡´çš„é•¿æœŸåœæ»ï¼ˆåå¤ sync added=0ï¼‰ã€‚
	//
	// æ³¨æ„ï¼šæ­¤å¤„åªä½¿ç”¨ sync å“åº”ä¸­çš„åŒºå—ï¼Œä¸æ··å…¥æœ¬åœ° store çš„å€™é€‰ã€‚
	// åŸå› ï¼šsync å“åº”æ¥è‡ªå·²ç»æœ€ç»ˆåŒ–çš„ peerï¼Œä»£è¡¨å®ƒçš„æœ€ç»ˆåŒ–é“¾ã€‚æœ¬åœ° store å¯èƒ½æœ‰
	// æœªè¢«é€‰ä¸­åˆ†æ”¯çš„å€™é€‰åŒºå—ï¼ˆä¸åŒ Window/ä¸åŒ parentï¼‰ï¼Œæ··å…¥ä¼šå¯¼è‡´é€‰æ‹©åˆ°
	// çˆ¶é“¾ä¸å…¼å®¹çš„åŒºå—ï¼Œé€ æˆ SetFinalized å¤±è´¥ï¼Œé“¾æ¡ä¸­æ–­ã€‚
	// é˜²åˆ†å‰ä¿æŠ¤åœ¨æŠ•ç¥¨å±‚ï¼ˆselectBestCandidate åå¥½ä½ Window + sendChits ä¸€è‡´æ€§ï¼‰
	// å’Œç­¾åéªŒè¯å±‚ï¼ˆVerifySignatureSetï¼‰å®ç°ã€‚
	finalized := 0
	acceptedID, acceptedHeight := sm.store.GetLastAccepted()
	blocksByHeight := make(map[uint64][]*types.Block, len(msg.Blocks))
	for _, b := range msg.Blocks {
		if b == nil {
			continue
		}
		blocksByHeight[b.Header.Height] = append(blocksByHeight[b.Header.Height], b)
	}

	for {
		nextHeight := acceptedHeight + 1
		cands := blocksByHeight[nextHeight]
		if len(cands) == 0 {
			break
		}
		// ä» sync å“åº”çš„å€™é€‰ä¸­é€‰æ‹©çˆ¶é“¾åŒ¹é…çš„åŒºå—
		var chosen *types.Block
		for _, c := range cands {
			if c != nil && c.Header.ParentID == acceptedID {
				chosen = c
				break
			}
		}
		// å¦‚æœæ²¡æœ‰çˆ¶é“¾åŒ¹é…çš„ï¼Œé€€åŒ–ä¸ºç¬¬ä¸€ä¸ªå€™é€‰ï¼ˆRealBlockStore.SetFinalized å†…éƒ¨ä¼šåšå®‰å…¨æ£€æŸ¥ï¼‰
		if chosen == nil {
			chosen = cands[0]
		}

		// VRF ç­¾åé›†åˆéªŒè¯ï¼ˆå¦‚æœå¯ç”¨ï¼‰
		if len(msg.SignatureSets) > 0 {
			if sigData, hasSig := msg.SignatureSets[nextHeight]; hasSig {
				var sigSet pb.ConsensusSignatureSet
				if err := proto.Unmarshal(sigData, &sigSet); err != nil {
					logs.Warn("[SyncManager] Failed to decode signature set for height %d: %v", nextHeight, err)
				} else {
					if !VerifySignatureSet(&sigSet, sm.config.SyncAlpha, sm.config.SyncBeta, sm.transport, sm.nodeID) {
						logs.Warn("[SyncManager] âš ï¸ Signature set verification failed for height %d, skipping", nextHeight)
						break
					}
					// éªŒè¯é€šè¿‡ï¼Œå­˜å‚¨åˆ°æœ¬åœ°
					if realStore, ok := sm.store.(*RealBlockStore); ok {
						realStore.SetSignatureSet(nextHeight, &sigSet)
					}
				}
			}
		}

		// å°è¯•æœ€ç»ˆåŒ–è¯¥é«˜åº¦ï¼ˆå¤±è´¥ä¼šä¿æŒ lastAccepted ä¸å˜ï¼‰
		if err := sm.store.SetFinalized(nextHeight, chosen.ID); err != nil {
			logs.Warn("[SyncManager] Failed to finalize block %s at height %d: %v", chosen.ID, nextHeight, err)
			break
		}

		newAcceptedID, newAcceptedHeight := sm.store.GetLastAccepted()
		if newAcceptedHeight != nextHeight || newAcceptedID != chosen.ID {
			break
		}

		finalized++
		acceptedID, acceptedHeight = newAcceptedID, newAcceptedHeight

		// ä¸»åŠ¨å‘å¸ƒæœ€ç»ˆåŒ–äº‹ä»¶ï¼Œé©±åŠ¨ ProposalManager/SnapshotManager ç­‰ç»„ä»¶çŠ¶æ€å‰è¿›
		if blk, ok := sm.store.Get(chosen.ID); ok && blk != nil {
			sm.events.PublishAsync(types.BaseEvent{
				EventType: types.EventBlockFinalized,
				EventData: blk,
			})
		}
	}

	if finalized > 0 {
		logs.Info("[Node %s] âœ… Fast-finalized %d block(s) via sync (accepted=%d)",
			sm.nodeID, finalized, acceptedHeight)
		atomic.StoreUint32(&sm.consecutiveStallCount, 0) // é‡ç½®åœæ»è®¡æ•°
	} else if len(msg.Blocks) > 0 {
		// é«˜é£é™©ä¿®å¤ï¼šæ£€æµ‹åŒæ­¥åœæ»
		// ä¼˜åŒ–ï¼šåªæœ‰å½“åŒæ­¥èŒƒå›´è·¨è¶Šäº†å½“å‰å·²æ¥å—é«˜åº¦ï¼Œä¸”æœªèƒ½æ¨è¿›æ—¶ï¼Œæ‰è®¡å…¥ stall
		if msg.ToHeight > acceptedHeight {
			stalls := atomic.AddUint32(&sm.consecutiveStallCount, 1)
			if stalls >= 3 {
				logs.Debug("[Node %s] âš ï¸ Sync stalled for %d rounds at height %d, breaking pipeline and switching peers",
					sm.nodeID, stalls, acceptedHeight)
				sm.Mu.Lock()
				delete(sm.PeerHeights, types.NodeID(msg.From)) // æ¸…ç†è¯¥ Peer é«˜åº¦ä¿¡æ¯ï¼Œå¼ºåˆ¶é‡æ–°é‡‡æ ·
				sm.Syncing = false
				sm.Mu.Unlock()
				atomic.StoreUint32(&sm.consecutiveStallCount, 0)
				return
			}
		} else {
			logs.Debug("[Node %s] Sync response for heights %d-%d is older than accepted %d, ignoring stall check",
				sm.nodeID, msg.FromHeight, msg.ToHeight, acceptedHeight)
		}
	}

	// ä½é£é™©ä¼˜åŒ–ï¼šä¿¡å·ç²¾å‡†åŒ–
	// åªæœ‰å½“æœ¬åœ°é«˜åº¦ä¸å·²çŸ¥ Peer æœ€å¤§é«˜åº¦å·®è·å°äº BatchSize æ—¶æ‰å‘å¸ƒ EventSyncCompleteã€‚
	// é¿å…åœ¨é•¿è·ç¦»è¿½å—è¿‡ç¨‹ä¸­ï¼Œæ¯ä¸€è½®åŒæ­¥éƒ½å”¤é†’ QueryManager å‘èµ·æ— æ•ˆæŸ¥è¯¢ã€‚
	if added > 0 || finalized > 0 {
		_, curAccepted := sm.store.GetLastAccepted()
		maxPeer := sm.getMaxPeerHeight()
		if maxPeer <= curAccepted+sm.config.BatchSize {
			sm.events.PublishAsync(types.BaseEvent{
				EventType: types.EventSyncComplete,
				EventData: added,
			})
		} else {
			logs.Debug("[Node %s] Skipping EventSyncComplete signal: still behind by %d blocks",
				sm.nodeID, maxPeer-curAccepted)
		}
	}

	// ä¸­é£é™©ä¿®å¤ï¼šå¤„ç†å®Œæˆåå†è§£é”çŠ¶æ€
	sm.Mu.Lock()
	sm.Syncing = false
	sm.Mu.Unlock()

	// æµæ°´çº¿ç»­ä¼ ï¼šå¦‚æœä»è½åï¼Œç«‹å³å‘èµ·ä¸‹ä¸€è½®åŒæ­¥
	if added > 0 || finalized > 0 {
		_, localAccepted := sm.store.GetLastAccepted()
		maxPeer := sm.getMaxPeerHeight()
		if maxPeer > localAccepted+1 {
			go func() {
				time.Sleep(50 * time.Millisecond) // çŸ­æš‚å»¶è¿Ÿé¿å…å¤ªæ¿€è¿›
				sm.requestSync(localAccepted+1, minUint64(localAccepted+sm.config.BatchSize, maxPeer))
			}()
		}
	}
}

// getMaxPeerHeight è¿”å›å·²çŸ¥çš„æœ€å¤§å¯¹ç«¯é«˜åº¦
func (sm *SyncManager) getMaxPeerHeight() uint64 {
	sm.Mu.RLock()
	defer sm.Mu.RUnlock()

	maxHeight := uint64(0)
	for _, h := range sm.PeerHeights {
		if h > maxHeight {
			maxHeight = h
		}
	}
	return maxHeight
}

// VerifySignatureSet éªŒè¯å…±è¯†ç­¾åé›†åˆçš„åˆæ³•æ€§ï¼ˆä¸‰æ­¥éªŒè¯ï¼‰
// 1. è½®æ¬¡å®Œæ•´æ€§ï¼šlen(rounds) >= beta
// 2. æ¯è½®ç­¾åå……è¶³ï¼šlen(signatures) >= alpha
// 3. é‡‡æ ·åˆæ³•æ€§ï¼šé‡æ¼” samplePeersDeterministicï¼Œç¡®è®¤ node_id åœ¨åˆæ³•é‡‡æ ·é›†ä¸­
func VerifySignatureSet(sigSet *pb.ConsensusSignatureSet, alpha, beta int, transport interfaces.Transport, localNodeID types.NodeID) bool {
	if sigSet == nil {
		return false
	}

	// å¦‚æœ alpha/beta æœªé…ç½®ï¼ˆä¸º 0ï¼‰ï¼Œä½¿ç”¨å®½æ¾çš„é»˜è®¤å€¼
	if alpha <= 0 {
		alpha = 1
	}
	if beta <= 0 {
		beta = 1
	}

	// æ­¥éª¤ 1ï¼šè½®æ¬¡å®Œæ•´æ€§
	if len(sigSet.Rounds) < beta {
		logs.Debug("[VerifySignatureSet] Failed: rounds=%d < beta=%d", len(sigSet.Rounds), beta)
		return false
	}

	// æ­¥éª¤ 2ï¼šæ¯è½®ç­¾åå……è¶³
	for i, round := range sigSet.Rounds {
		if len(round.Signatures) < alpha {
			logs.Debug("[VerifySignatureSet] Failed: round %d signatures=%d < alpha=%d",
				i, len(round.Signatures), alpha)
			return false
		}
	}

	// æ­¥éª¤ 3ï¼šé‡‡æ ·åˆæ³•æ€§ï¼ˆé‡æ¼” VRF ç¡®å®šæ€§é‡‡æ ·ï¼Œç¡®è®¤æ¯ä¸ªç­¾åè€…åœ¨åˆæ³•é‡‡æ ·é›†ä¸­ï¼‰
	if transport != nil && len(sigSet.VrfSeed) > 0 {
		allPeers := transport.GetAllPeers(localNodeID)
		if len(allPeers) > 0 {
			// ç”¨ K = len(allPeers) ä½œä¸ºä¸Šé™ï¼ˆé‡‡æ ·é›†ä¸ä¼šè¶…è¿‡å…¨éƒ¨èŠ‚ç‚¹æ•°ï¼‰
			k := len(allPeers)
			for _, round := range sigSet.Rounds {
				sampled := samplePeersDeterministic(sigSet.VrfSeed, round.SeqId, k, allPeers)
				sampledSet := make(map[string]bool, len(sampled))
				for _, p := range sampled {
					sampledSet[string(p)] = true
				}

				for _, sig := range round.Signatures {
					if !sampledSet[sig.NodeId] {
						logs.Debug("[VerifySignatureSet] Failed: node %s not in sampled set for round seq=%d",
							sig.NodeId, round.SeqId)
						return false
					}
				}
			}
		}
	}

	// æ­¥éª¤ 4ï¼šå¯†ç å­¦éªŒç­¾ï¼ˆECDSA ç­¾åéªŒè¯ï¼‰
	// å¯¹æ¯ä¸ª ChitSignature é‡ç®— digest å¹¶éªŒè¯ç­¾å
	// å½“å…¬é’¥æ³¨å†Œè¡¨ä¸ºç©ºï¼ˆå¦‚æ–°èŠ‚ç‚¹é¦–æ¬¡åŒæ­¥ï¼‰æ—¶è·³è¿‡æ­¤æ­¥éª¤
	hasPublicKeys := false
	nodePublicKeysMu.RLock()
	hasPublicKeys = len(nodePublicKeys) > 0
	nodePublicKeysMu.RUnlock()

	if hasPublicKeys {
		for _, round := range sigSet.Rounds {
			for _, sig := range round.Signatures {
				if len(sig.Signature) == 0 {
					continue // å…¼å®¹æ—§ç‰ˆæœ¬æ— ç­¾åçš„è®°å½•
				}
				digest := ComputeChitDigest(sig.PreferredId, sigSet.Height, sigSet.VrfSeed, round.SeqId)
				if !VerifyChitSignature(sig.NodeId, digest, sig.Signature) {
					logs.Debug("[VerifySignatureSet] Failed: ECDSA signature verification failed for node %s round seq=%d",
						sig.NodeId, round.SeqId)
					return false
				}
			}
		}
	}

	return true
}
