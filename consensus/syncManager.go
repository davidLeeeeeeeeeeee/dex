package consensus

import (
	"context"
	"dex/interfaces"
	"dex/logs"
	"dex/types"
	"strings"
	"sync"
	"sync/atomic"
	"time"
)

// ============================================
// åŒæ­¥ç®¡ç†å™¨ - å¢å¼ºç‰ˆï¼ˆæ”¯æŒå¿«ç…§ï¼‰
// ============================================

type SyncManager struct {
	nodeID         types.NodeID
	node           *Node // æ–°å¢
	transport      interfaces.Transport
	store          interfaces.BlockStore
	config         *SyncConfig
	snapshotConfig *SnapshotConfig // æ–°å¢
	events         interfaces.EventBus
	Logger         logs.Logger
	SyncRequests   map[uint32]time.Time
	nextSyncID     uint32
	Syncing        bool
	Mu             sync.RWMutex
	PeerHeights    map[types.NodeID]uint64
	lastPoll       time.Time
	usingSnapshot  bool // æ ‡è®°æ˜¯å¦æ­£åœ¨ä½¿ç”¨å¿«ç…§åŒæ­¥
	// é‡‡æ ·éªŒè¯ç›¸å…³å­—æ®µ
	sampling        bool                    // æ˜¯å¦æ­£åœ¨é‡‡æ ·éªŒè¯
	sampleResponses map[types.NodeID]uint64 // é‡‡æ ·å“åº”: nodeID -> acceptedHeight
	sampleStartTime time.Time               // é‡‡æ ·å¼€å§‹æ—¶é—´
	// äº‹ä»¶é©±åŠ¨åŒæ­¥ç›¸å…³
	pendingBlockBuffer    *PendingBlockBuffer // å¾…å¤„ç†åŒºå—ç¼“å†²åŒºï¼ˆç”¨äºè¡¥è¯¾ï¼‰
	consecutiveStallCount uint32              // è¿ç»­åŒæ­¥åœæ»è®¡æ•°ï¼ˆé«˜é£é™©ä¿®å¤ï¼šæ­»å¾ªç¯ä¿æŠ¤ï¼‰
}

func NewSyncManager(id types.NodeID, transport interfaces.Transport, store interfaces.BlockStore, config *SyncConfig, snapshotConfig *SnapshotConfig, events interfaces.EventBus, logger logs.Logger) *SyncManager {
	return &SyncManager{
		nodeID:          id,
		transport:       transport,
		store:           store,
		config:          config,
		snapshotConfig:  snapshotConfig,
		events:          events,
		Logger:          logger,
		SyncRequests:    make(map[uint32]time.Time),
		PeerHeights:     make(map[types.NodeID]uint64),
		lastPoll:        time.Now(),
		sampleResponses: make(map[types.NodeID]uint64),
	}
}

// SetPendingBlockBuffer è®¾ç½® PendingBlockBufferï¼ˆåœ¨åˆå§‹åŒ–åæ³¨å…¥ï¼‰
func (sm *SyncManager) SetPendingBlockBuffer(buffer *PendingBlockBuffer) {
	sm.pendingBlockBuffer = buffer
}

func (sm *SyncManager) Start(ctx context.Context) {
	go func() {
		logs.SetThreadNodeContext(string(sm.nodeID))
		ticker := time.NewTicker(sm.config.CheckInterval)
		defer ticker.Stop()

		for {
			select {
			case <-ticker.C:
				sm.checkAndSync()
			case <-ctx.Done():
				return
			}
		}
	}()

	go func() {
		ticker := time.NewTicker(sm.config.CheckInterval)
		defer ticker.Stop()

		for {
			select {
			case <-ticker.C:
				sm.pollPeerHeights()
			case <-ctx.Done():
				return
			}
		}
	}()
}

// å®šæ—¶æŸ¥è¯¢å…¶ä»–èŠ‚ç‚¹é«˜åº¦
func (sm *SyncManager) pollPeerHeights() {
	// å¦‚æœæ­£åœ¨åŒæ­¥ä¸­ï¼Œæš‚åœè½®è¯¢é«˜åº¦ï¼Œé¿å…ç½‘ç»œæ‹¥å µ
	sm.Mu.RLock()
	if sm.Syncing {
		sm.Mu.RUnlock()
		return
	}
	sm.Mu.RUnlock()

	// é™åˆ¶è½®è¯¢é¢‘ç‡ï¼Œè½åæ—¶å¢åŠ æ¢æµ‹é¢‘ç‡
	cooldown := 2 * time.Second
	sm.Mu.RLock()
	_, accepted := sm.store.GetLastAccepted()
	maxPeer := uint64(0)
	for _, h := range sm.PeerHeights {
		if h > maxPeer {
			maxPeer = h
		}
	}
	if maxPeer > accepted+2 {
		cooldown = 500 * time.Millisecond // è½åè¾ƒå¤šæ—¶ï¼Œæ¯ 500ms æ¢æµ‹ä¸€æ¬¡
	}
	sm.Mu.RUnlock()

	if time.Since(sm.lastPoll) < cooldown {
		return
	}
	sm.lastPoll = time.Now()

	peers := sm.transport.SamplePeers(sm.nodeID, 10)
	for _, peer := range peers {
		sm.transport.Send(peer, types.Message{
			Type: types.MsgHeightQuery,
			From: sm.nodeID,
		})
	}
}

func (sm *SyncManager) HandleHeightQuery(msg types.Message) {
	_, height := sm.store.GetLastAccepted()
	currentHeight := sm.store.GetCurrentHeight()

	err := sm.transport.Send(types.NodeID(msg.From), types.Message{
		Type:          types.MsgHeightResponse,
		From:          sm.nodeID,
		Height:        height,
		CurrentHeight: currentHeight,
	})
	if err != nil {
		return
	}
}

func (sm *SyncManager) HandleHeightResponse(msg types.Message) {
	sm.Mu.Lock()
	defer sm.Mu.Unlock()
	// åŒæ­¥åº”è¯¥åŸºäºå¯¹æ–¹â€œå·²æœ€ç»ˆåŒ–/å·²æ¥å—â€çš„é«˜åº¦ï¼Œè€Œä¸æ˜¯å…¶å½“å‰æœ€å¤§é«˜åº¦ï¼ˆå¯èƒ½åŒ…å«å¤§é‡æœªæœ€ç»ˆåŒ–å—ï¼‰ã€‚
	// å¦åˆ™ä¼šå¯¼è‡´æœ¬èŠ‚ç‚¹ä¸æ–­å°è¯•åŒæ­¥ä¸€äº›å…¶å®å…¨ç½‘éƒ½è¿˜æ²¡æœ€ç»ˆåŒ–çš„é«˜åº¦ï¼Œå‡ºç°é‡å¤ sync ä¸” added=0 çš„æƒ…å†µã€‚
	sm.PeerHeights[types.NodeID(msg.From)] = msg.Height

	// å¦‚æœæ­£åœ¨é‡‡æ ·éªŒè¯ï¼Œæ”¶é›†å“åº”
	if sm.sampling {
		sm.sampleResponses[types.NodeID(msg.From)] = msg.Height
	}
}

// æ£€æŸ¥æ˜¯å¦æœ‰å¿…è¦å¯åŠ¨åŒæ­¥ç¨‹åº
func (sm *SyncManager) checkAndSync() {
	sm.Mu.Lock()

	// 1. æ£€æŸ¥æ˜¯å¦æœ‰åŒæ­¥è¯·æ±‚è¶…æ—¶
	if sm.Syncing {
		now := time.Now()
		hasTimeout := false
		for syncID, startTime := range sm.SyncRequests {
			if now.Sub(startTime) > sm.config.Timeout {
				Logf("[Node %s] âš ï¸ Sync request %d timed out (started at %v)\n",
					sm.nodeID, syncID, startTime.Format("15:04:05"))
				delete(sm.SyncRequests, syncID)
				hasTimeout = true
			}
		}

		if hasTimeout && len(sm.SyncRequests) == 0 {
			logs.Warn("[Node %s] All sync requests timed out, resetting Syncing flag", sm.nodeID)
			sm.Syncing = false
			sm.usingSnapshot = false
		}

		if sm.Syncing {
			sm.Mu.Unlock()
			return
		}
	}

	// 2. æ£€æŸ¥é‡‡æ ·éªŒè¯çŠ¶æ€
	if sm.sampling {
		// æ£€æŸ¥é‡‡æ ·æ˜¯å¦è¶…æ—¶
		if time.Since(sm.sampleStartTime) > sm.config.SampleTimeout {
			logs.Debug("[Sync] Sample verification timed out, responses=%d", len(sm.sampleResponses))
			sm.sampling = false
			sm.Mu.Unlock()
			return
		}

		// è¯„ä¼° Quorum
		quorumHeight, ok := sm.evaluateSampleQuorum()
		if !ok {
			// å°šæœªè¾¾åˆ° Quorumï¼Œç­‰å¾…æ›´å¤šå“åº”
			sm.Mu.Unlock()
			return
		}

		// Quorum è¾¾æˆï¼Œå¯ä»¥å®‰å…¨åŒæ­¥
		sm.sampling = false
		_, localAcceptedHeight := sm.store.GetLastAccepted()

		if quorumHeight > localAcceptedHeight {
			logs.Info("[Sync] âœ… Quorum verified: target height %d confirmed by %.0f%% nodes",
				quorumHeight, sm.config.QuorumRatio*100)
			sm.Mu.Unlock()

			// åˆ¤æ–­ä½¿ç”¨å¿«ç…§è¿˜æ˜¯æ™®é€šåŒæ­¥
			heightDiff := quorumHeight - localAcceptedHeight
			if sm.snapshotConfig.Enabled && heightDiff > sm.config.SnapshotThreshold {
				sm.requestSnapshotSync(quorumHeight)
			} else {
				sm.requestSync(localAcceptedHeight+1, minUint64(localAcceptedHeight+sm.config.BatchSize, quorumHeight))
			}
			return
		}

		sm.Mu.Unlock()
		return
	}

	// 3. åˆæ­¥æ£€æŸ¥æ˜¯å¦è½å
	maxPeerHeight := uint64(0)
	for _, height := range sm.PeerHeights {
		if height > maxPeerHeight {
			maxPeerHeight = height
		}
	}

	_, localAcceptedHeight := sm.store.GetLastAccepted()
	heightDiff := uint64(0)
	if maxPeerHeight > localAcceptedHeight {
		heightDiff = maxPeerHeight - localAcceptedHeight
	}

	// 4. å¦‚æœè½åè¶…è¿‡é˜ˆå€¼ï¼Œå¯åŠ¨é‡‡æ ·éªŒè¯
	if heightDiff > sm.config.BehindThreshold {
		logs.Debug("[Sync] Detected lag of %d blocks, starting sample verification (target=%d)",
			heightDiff, maxPeerHeight)
		sm.startHeightSampling()
	}

	sm.Mu.Unlock()
}

// TriggerSyncFromChit ç”± Chits æ¶ˆæ¯é©±åŠ¨çš„åŒæ­¥è§¦å‘å…¥å£ï¼ˆäº‹ä»¶é©±åŠ¨æ¨¡å¼ï¼‰
// æ— éœ€å¤æ‚çš„é‡‡æ ·éªŒè¯ï¼Œå› ä¸º Chits æœ¬èº«å°±æ˜¯å…±è¯†é‡‡æ ·çš„ä¸€éƒ¨åˆ†
func (sm *SyncManager) TriggerSyncFromChit(peerAcceptedHeight uint64, from types.NodeID) {
	sm.Mu.Lock()

	// æ›´æ–° PeerHeightsï¼ˆæ— éœ€ä¸“é—¨è½®è¯¢ï¼‰
	if peerAcceptedHeight > sm.PeerHeights[from] {
		sm.PeerHeights[from] = peerAcceptedHeight
	}

	// å¦‚æœå·²åœ¨åŒæ­¥ä¸­æˆ–é‡‡æ ·ä¸­ï¼Œè·³è¿‡
	if sm.Syncing || sm.sampling {
		sm.Mu.Unlock()
		return
	}

	_, localAccepted := sm.store.GetLastAccepted()
	if peerAcceptedHeight <= localAccepted {
		sm.Mu.Unlock()
		return
	}

	heightDiff := peerAcceptedHeight - localAccepted
	sm.Mu.Unlock()

	logs.Debug("[SyncManager] TriggerSyncFromChit: peer=%s peerHeight=%d localAccepted=%d diff=%d",
		from, peerAcceptedHeight, localAccepted, heightDiff)

	// ä¸­é£é™©ä¿®å¤ï¼šè½»é‡çº§é«˜åº¦é‡‡æ ·éªŒè¯
	// é‡‡æ · 2 ä¸ªéšæœºèŠ‚ç‚¹ï¼ˆä¸åŒ…æ‹¬å‘é€è€…ï¼‰æ¥äº¤å‰ç¡®è®¤é«˜åº¦ã€‚
	// è¿™èƒ½é˜²æ­¢è¢«å•ä¸ªæ¶æ„æˆ–æ•…éšœèŠ‚ç‚¹æ‹‰å…¥é”™è¯¯çš„åŒæ­¥è½¨é“ã€‚
	go func() {
		logs.Debug("[SyncManager] Starting lightweight safety sampling for TriggerSyncFromChit (target=%d)", peerAcceptedHeight)

		peers := sm.transport.SamplePeers(sm.nodeID, 2)
		// å¦‚æœç½‘ç»œå¤ªå°æ²¡æ³•é‡‡æ ·ï¼Œåˆ™ä¿¡ä»»è¯¥ Chitï¼ˆæ­¤æ—¶å…±è¯†å®‰å…¨æ€§ç”± BFT ä¿è¯ï¼‰
		if len(peers) <= 1 {
			sm.performTriggeredSync(peerAcceptedHeight, localAccepted, heightDiff)
			return
		}

		// å‘èµ·é«˜åº¦æŸ¥è¯¢
		for _, p := range peers {
			if p == from {
				continue
			}
			sm.transport.Send(p, types.Message{
				Type: types.MsgHeightQuery,
				From: sm.nodeID,
			})
		}

		// ç­‰å¾… 300ms è§‚å¯Ÿå“åº”ï¼ˆé€šè¿‡ HandleHeightResponse æ›´æ–° PeerHeightsï¼‰
		time.Sleep(300 * time.Millisecond)

		// æ£€æŸ¥æ˜¯å¦æœ‰å…¶ä»–èŠ‚ç‚¹ä¹Ÿè¾¾åˆ°äº†è¯¥é«˜åº¦é™„è¿‘
		sm.Mu.RLock()
		maxOtherPeerHeight := uint64(0)
		for pID, h := range sm.PeerHeights {
			if pID != from && h > maxOtherPeerHeight {
				maxOtherPeerHeight = h
			}
		}
		sm.Mu.RUnlock()

		// å®‰å…¨é˜ˆå€¼ï¼šåªè¦æœ‰ä»»ä½•ä¸€ä¸ªå…¶ä»–èŠ‚ç‚¹ä¹Ÿå¤„äºç±»ä¼¼é«˜åº¦ï¼ˆæˆ–æ›´é«˜ï¼‰ï¼Œå³è®¤ä¸ºå®‰å…¨
		if maxOtherPeerHeight >= peerAcceptedHeight-1 {
			sm.performTriggeredSync(peerAcceptedHeight, localAccepted, heightDiff)
		} else {
			logs.Warn("[SyncManager] ğŸ›¡ï¸ Lightweight sampling failed: no other peer confirmed height %d (maxOther=%d). Sync cancelled.",
				peerAcceptedHeight, maxOtherPeerHeight)
		}
	}()
}

// performTriggeredSync æ‰§è¡Œè¢«è§¦å‘çš„åŒæ­¥åŠ¨ä½œ
func (sm *SyncManager) performTriggeredSync(peerAcceptedHeight, localAccepted, heightDiff uint64) {
	if sm.snapshotConfig.Enabled && heightDiff > sm.config.SnapshotThreshold {
		sm.requestSnapshotSync(peerAcceptedHeight)
	} else {
		// ä½¿ç”¨åˆ†ç‰‡å¹¶è¡ŒåŒæ­¥
		sm.requestSyncParallel(localAccepted+1, minUint64(localAccepted+sm.config.BatchSize, peerAcceptedHeight))
	}
}

// requestSyncParallel åˆ†ç‰‡å¹¶è¡ŒåŒæ­¥ï¼šå°†é«˜åº¦èŒƒå›´åˆ†é…ç»™å¤šä¸ªèŠ‚ç‚¹åŒæ—¶è¯·æ±‚
func (sm *SyncManager) requestSyncParallel(fromHeight, toHeight uint64) {
	sm.Mu.Lock()
	if sm.Syncing {
		sm.Mu.Unlock()
		return
	}
	sm.Syncing = true
	sm.Mu.Unlock()

	// è·å–å¯ç”¨èŠ‚ç‚¹
	peers := sm.transport.SamplePeers(sm.nodeID, sm.config.ParallelPeers)
	if len(peers) == 0 {
		sm.Mu.Lock()
		sm.Syncing = false
		sm.Mu.Unlock()
		return
	}

	totalBlocks := toHeight - fromHeight + 1

	// å¦‚æœè¯·æ±‚èŒƒå›´å°æˆ–åªæœ‰1ä¸ªèŠ‚ç‚¹ï¼Œé€€åŒ–åˆ°æ™®é€šåŒæ­¥
	if totalBlocks <= 5 || len(peers) == 1 {
		sm.Mu.Lock()
		sm.Syncing = false
		sm.Mu.Unlock()
		sm.requestSync(fromHeight, toHeight)
		return
	}

	// è®¡ç®—æ¯ä¸ªèŠ‚ç‚¹è´Ÿè´£çš„é«˜åº¦èŒƒå›´
	rangePerPeer := totalBlocks / uint64(len(peers))

	logs.Info("[SyncManager] Starting parallel sync: heights %d-%d across %d peers",
		fromHeight, toHeight, len(peers))

	for i, peer := range peers {
		start := fromHeight + uint64(i)*rangePerPeer
		end := start + rangePerPeer - 1
		if i == len(peers)-1 {
			end = toHeight // æœ€åä¸€ä¸ªèŠ‚ç‚¹è´Ÿè´£å‰©ä½™
		}

		// ä¸ºæ¯ä¸ªåˆ†ç‰‡åˆ›å»ºç‹¬ç«‹çš„ SyncID
		syncID := atomic.AddUint32(&sm.nextSyncID, 1)
		sm.Mu.Lock()
		sm.SyncRequests[syncID] = time.Now()
		sm.Mu.Unlock()

		// åˆ¤æ–­æ˜¯å¦ä½¿ç”¨ ShortTxs æ¨¡å¼ï¼ˆåŸºäºæ€»è½åé‡è€Œéåˆ†ç‰‡å¤§å°ï¼‰
		useShortMode := totalBlocks <= sm.config.ShortSyncThreshold

		go func(p types.NodeID, s, e uint64, id uint32, shortMode bool) {
			logs.Debug("[SyncManager] Parallel shard: peer=%s heights=%d-%d shortMode=%v", p, s, e, shortMode)

			msg := types.Message{
				Type:          types.MsgSyncRequest,
				From:          sm.nodeID,
				SyncID:        id,
				FromHeight:    s,
				ToHeight:      e,
				SyncShortMode: shortMode,
			}
			sm.transport.Send(p, msg)
		}(peer, start, end, syncID, useShortMode)
	}
}

// startHeightSampling å¯åŠ¨é‡‡æ ·éªŒè¯ï¼ˆå¿…é¡»æŒæœ‰å†™é”è°ƒç”¨ï¼‰
func (sm *SyncManager) startHeightSampling() {
	sm.sampling = true
	sm.sampleResponses = make(map[types.NodeID]uint64)
	sm.sampleStartTime = time.Now()

	// é‡‡æ · K ä¸ªèŠ‚ç‚¹
	peers := sm.transport.SamplePeers(sm.nodeID, sm.config.SampleSize)
	for _, peer := range peers {
		sm.transport.Send(peer, types.Message{
			Type: types.MsgHeightQuery,
			From: sm.nodeID,
		})
	}
}

// evaluateSampleQuorum è¯„ä¼°é‡‡æ · Quorumï¼ˆå¿…é¡»æŒæœ‰è¯»é”è°ƒç”¨ï¼‰
// è¿”å›æ»¡è¶³ Quorum çš„æœ€é«˜å·²æœ€ç»ˆåŒ–é«˜åº¦
func (sm *SyncManager) evaluateSampleQuorum() (uint64, bool) {
	responseCount := len(sm.sampleResponses)
	if responseCount == 0 {
		return 0, false
	}

	// è®¡ç®— Quorum é˜ˆå€¼
	required := int(float64(sm.config.SampleSize) * sm.config.QuorumRatio)
	if required < 1 {
		required = 1
	}

	// å¦‚æœå“åº”ä¸è¶³ï¼Œç»§ç»­ç­‰å¾…
	if responseCount < required {
		return 0, false
	}

	// æ”¶é›†æ‰€æœ‰é«˜åº¦å¹¶æ’åº
	heights := make([]uint64, 0, responseCount)
	for _, h := range sm.sampleResponses {
		heights = append(heights, h)
	}

	// ä»é«˜åˆ°ä½æ‰¾åˆ°æ»¡è¶³ Quorum çš„æœ€é«˜é«˜åº¦
	// å¯¹äºæ¯ä¸ªå€™é€‰é«˜åº¦ï¼Œç»Ÿè®¡æœ‰å¤šå°‘èŠ‚ç‚¹çš„ acceptedHeight >= è¯¥é«˜åº¦
	var maxQuorumHeight uint64
	for _, candidateHeight := range heights {
		supportCount := 0
		for _, h := range sm.sampleResponses {
			if h >= candidateHeight {
				supportCount++
			}
		}
		if supportCount >= required && candidateHeight > maxQuorumHeight {
			maxQuorumHeight = candidateHeight
		}
	}

	if maxQuorumHeight > 0 {
		logs.Debug("[Sync] Quorum check: %d/%d nodes support height %d (required=%d)",
			len(sm.sampleResponses), sm.config.SampleSize, maxQuorumHeight, required)
		return maxQuorumHeight, true
	}

	return 0, false
}

// è¯·æ±‚å¿«ç…§åŒæ­¥
func (sm *SyncManager) requestSnapshotSync(targetHeight uint64) {
	sm.Mu.Lock()
	if sm.Syncing {
		sm.Mu.Unlock()
		return
	}
	sm.Syncing = true
	sm.usingSnapshot = true
	syncID := atomic.AddUint32(&sm.nextSyncID, 1)
	sm.SyncRequests[syncID] = time.Now()
	sm.Mu.Unlock()

	// æ‰¾ä¸€ä¸ªé«˜åº¦è¶³å¤Ÿçš„èŠ‚ç‚¹
	sm.Mu.RLock()
	var targetPeer types.NodeID = "-1"
	for peer, height := range sm.PeerHeights {
		if height >= targetHeight {
			targetPeer = peer
			break
		}
	}
	sm.Mu.RUnlock()

	if targetPeer == "-1" {
		peers := sm.transport.SamplePeers(sm.nodeID, 5)
		if len(peers) > 0 {
			targetPeer = peers[0]
		}
	}

	if targetPeer != "-1" {
		Logf("[Node %s] ğŸ“¸ Requesting SNAPSHOT sync from Node %s (behind by %d blocks)\n",
			sm.nodeID, targetPeer, targetHeight-sm.store.GetCurrentHeight())

		msg := types.Message{
			Type:            types.MsgSnapshotRequest,
			From:            sm.nodeID,
			SyncID:          syncID,
			RequestSnapshot: true,
			Height:          targetHeight,
		}
		sm.transport.Send(targetPeer, msg)
	} else {
		sm.Mu.Lock()
		sm.Syncing = false
		sm.usingSnapshot = false
		delete(sm.SyncRequests, syncID)
		sm.Mu.Unlock()
	}
}

func (sm *SyncManager) requestSync(fromHeight, toHeight uint64) {
	sm.Mu.Lock()
	if sm.Syncing {
		sm.Mu.Unlock()
		return
	}
	sm.Syncing = true
	syncID := atomic.AddUint32(&sm.nextSyncID, 1)
	sm.SyncRequests[syncID] = time.Now()
	sm.Mu.Unlock()

	sm.Mu.RLock()
	var targetPeer types.NodeID = "-1"
	for peer, height := range sm.PeerHeights {
		if height >= toHeight {
			targetPeer = peer
			break
		}
	}
	sm.Mu.RUnlock()

	if targetPeer == "-1" {
		peers := sm.transport.SamplePeers(sm.nodeID, 5)
		if len(peers) > 0 {
			targetPeer = peers[0]
		}
	}

	if targetPeer != "-1" {
		Logf("[Node %s] Requesting sync from Node %s for heights %d-%d\n",
			sm.nodeID, targetPeer, fromHeight, toHeight)

		msg := types.Message{
			Type:       types.MsgSyncRequest,
			From:       sm.nodeID,
			SyncID:     syncID,
			FromHeight: fromHeight,
			ToHeight:   toHeight,
		}
		sm.transport.Send(targetPeer, msg)
	} else {
		sm.Mu.Lock()
		sm.Syncing = false
		delete(sm.SyncRequests, syncID)
		sm.Mu.Unlock()
	}
}

// å¤„ç†å¿«ç…§è¯·æ±‚ï¼ˆæ–°å¢ï¼‰
func (sm *SyncManager) HandleSnapshotRequest(msg types.Message) {
	// è·å–æœ€è¿‘çš„å¿«ç…§
	snapshot, exists := sm.store.GetLatestSnapshot()
	if !exists {
		// å¦‚æœæ²¡æœ‰å¿«ç…§ï¼Œé™çº§åˆ°æ™®é€šåŒæ­¥
		sm.HandleSyncRequest(types.Message{
			Type:       types.MsgSyncRequest,
			From:       msg.From,
			SyncID:     msg.SyncID,
			FromHeight: 1,
			ToHeight:   minUint64(100, sm.store.GetCurrentHeight()),
		})
		return
	}

	Logf("[Node %s] ğŸ“¸ Sending snapshot (height %d) to Node %s\n",
		sm.nodeID, snapshot.Height, msg.From)

	// æ›´æ–°ç»Ÿè®¡
	if sm.node != nil {
		sm.node.Stats.Mu.Lock()
		sm.node.Stats.SnapshotsServed++
		sm.node.Stats.Mu.Unlock()
	}

	response := types.Message{
		Type:           types.MsgSnapshotResponse,
		From:           sm.nodeID,
		SyncID:         msg.SyncID,
		Snapshot:       snapshot,
		SnapshotHeight: snapshot.Height,
	}

	sm.transport.Send(types.NodeID(msg.From), response)
}

// å¤„ç†å¿«ç…§å“åº”ï¼ˆæ–°å¢ï¼‰
func (sm *SyncManager) HandleSnapshotResponse(msg types.Message) {
	sm.Mu.Lock()
	defer sm.Mu.Unlock()

	if _, ok := sm.SyncRequests[msg.SyncID]; !ok {
		return
	}

	delete(sm.SyncRequests, msg.SyncID)

	if msg.Snapshot == nil {
		sm.Syncing = false
		sm.usingSnapshot = false
		return
	}

	// åŠ è½½å¿«ç…§
	err := sm.store.LoadSnapshot(msg.Snapshot)
	if err != nil {
		Logf("[Node %s] Failed to load snapshot: %v\n", sm.nodeID, err)
		sm.Syncing = false
		sm.usingSnapshot = false
		return
	}

	// æ›´æ–°ç»Ÿè®¡
	if sm.node != nil {
		sm.node.Stats.Mu.Lock()
		sm.node.Stats.SnapshotsUsed++
		sm.node.Stats.Mu.Unlock()
	}

	Logf("[Node %s] ğŸ“¸ Successfully loaded snapshot at height %d\n",
		sm.nodeID, msg.SnapshotHeight)

	// å‘å¸ƒå¿«ç…§åŠ è½½äº‹ä»¶
	sm.events.PublishAsync(types.BaseEvent{
		EventType: types.EventSnapshotLoaded,
		EventData: msg.Snapshot,
	})

	// ç»§ç»­åŒæ­¥å¿«ç…§ä¹‹åçš„åŒºå—
	currentHeight := sm.store.GetCurrentHeight()
	maxPeerHeight := uint64(0)
	for _, height := range sm.PeerHeights {
		if height > maxPeerHeight {
			maxPeerHeight = height
		}
	}

	sm.Syncing = false
	sm.usingSnapshot = false

	// å¦‚æœè¿˜éœ€è¦æ›´å¤šåŒºå—ï¼Œç»§ç»­æ™®é€šåŒæ­¥
	if maxPeerHeight > currentHeight+1 {
		go func() {
			time.Sleep(100 * time.Millisecond)
			sm.requestSync(currentHeight+1, minUint64(currentHeight+sm.config.BatchSize, maxPeerHeight))
		}()
	}
}

func (sm *SyncManager) HandleSyncRequest(msg types.Message) {
	blocks := sm.store.GetBlocksFromHeight(msg.FromHeight, msg.ToHeight)

	Logf("[Node %s] Received sync request from Node %s for heights %d-%d (found %d blocks, shortMode=%v)\n",
		sm.nodeID, msg.From, msg.FromHeight, msg.ToHeight, len(blocks), msg.SyncShortMode)

	response := types.Message{
		Type:          types.MsgSyncResponse,
		From:          sm.nodeID,
		SyncID:        msg.SyncID,
		Blocks:        blocks,
		FromHeight:    msg.FromHeight,
		ToHeight:      msg.ToHeight,
		SyncShortMode: msg.SyncShortMode,
	}

	// è‡ªé€‚åº”ä¼ è¾“æ¨¡å¼
	if msg.SyncShortMode {
		// çŸ­æœŸè½åæ¨¡å¼ï¼šé™„å¸¦ ShortTxs ç”¨äºå¿«é€Ÿè¿˜åŸ
		response.BlocksShortTxs = make(map[string][]byte)
		for _, block := range blocks {
			if block == nil {
				continue
			}
			if cachedBlock, exists := GetCachedBlock(block.ID); exists && cachedBlock != nil {
				response.BlocksShortTxs[block.ID] = cachedBlock.ShortTxs
			}
		}
	} else {
		// é•¿æœŸè½åæ¨¡å¼ï¼šç›´æ¥å°†å®Œæ•´åŒºå—æ•°æ®æ”¾å…¥ blockCacheï¼ˆæ¥æ”¶æ–¹å¯ç›´æ¥ä½¿ç”¨ï¼‰
		// æ³¨æ„ï¼šè¿™é‡Œçš„ç­–ç•¥æ˜¯åœ¨å‘é€å‰æŠŠå®Œæ•´æ•°æ®ç¼“å­˜é€šçŸ¥å‡ºå»
		// å®é™…ä¸Šéœ€è¦åœ¨æ¥æ”¶ç«¯å¤„ç†ï¼Œè¿™é‡Œåªç¡®ä¿å“åº”ä¸­åŒ…å«å¿…è¦ä¿¡æ¯
		for _, block := range blocks {
			if block == nil {
				continue
			}
			if cachedBlock, exists := GetCachedBlock(block.ID); exists && cachedBlock != nil {
				// å·²æœ‰ç¼“å­˜æ•°æ®ï¼Œæ¥æ”¶æ–¹ä¼šé€šè¿‡å…¶ä»–æœºåˆ¶è·å–
				// å¯ä»¥è€ƒè™‘åœ¨æ¶ˆæ¯ä¸­ç›´æ¥åŒ…å«åºåˆ—åŒ–çš„ pb.Blockï¼Œä½†ä¼šå¢å¤§æ¶ˆæ¯ä½“ç§¯
				// å½“å‰ç­–ç•¥ï¼šåœ¨å“åº”åç”±æ¥æ”¶æ–¹é€šè¿‡ MsgGet æ‹‰å–å®Œæ•´æ•°æ®
			}
		}
	}

	sm.transport.Send(types.NodeID(msg.From), response)
}

func (sm *SyncManager) HandleSyncResponse(msg types.Message) {
	sm.Mu.Lock()
	if _, ok := sm.SyncRequests[msg.SyncID]; !ok {
		sm.Mu.Unlock()
		return
	}
	delete(sm.SyncRequests, msg.SyncID)
	// sm.Syncing = false // ä¸­é£é™©ä¿®å¤ï¼šç§»è‡³å‡½æ•°æœ«å°¾ï¼Œé˜²æ­¢çŠ¶æ€ç«äº‰
	sm.Mu.Unlock()

	added := 0
	addErrs := 0
	var firstAddErr error
	var firstAddErrBlockID string
	for _, block := range msg.Blocks {
		if block == nil {
			continue
		}

		// å¦‚æœæ˜¯ ShortMode ä¸”æœ‰ ShortTxs æ•°æ®ï¼Œæå‰äº¤ç»™ PendingBlockBuffer å¤„ç†
		if msg.SyncShortMode && len(msg.BlocksShortTxs) > 0 {
			shortTxs := msg.BlocksShortTxs[block.ID]
			if len(shortTxs) > 0 && sm.pendingBlockBuffer != nil {
				// ä½¿ç”¨ ShortTxs å°è¯•è¿˜åŸåŒºå—
				sm.pendingBlockBuffer.AddPendingBlockForConsensus(block, shortTxs, types.NodeID(msg.From), 0, nil)
				added++ // ä¹è§‚è®¡æ•°ï¼Œå®é™…è¿˜åŸç”± buffer å¼‚æ­¥å®Œæˆ
				continue
			}
		}

		// å°è¯•ç›´æ¥æ·»åŠ 
		isNew, err := sm.store.Add(block)
		if err != nil {
			addErrs++
			if firstAddErr == nil {
				firstAddErr = err
				firstAddErrBlockID = block.ID
			}
			// æ¥å…¥è¡¥è¯¾æœºåˆ¶ï¼šå¦‚æœæ˜¯æ•°æ®ä¸å®Œæ•´å¯¼è‡´çš„å¤±è´¥ï¼ŒåŠ å…¥ PendingBlockBuffer
			if strings.Contains(err.Error(), "block data incomplete") && sm.pendingBlockBuffer != nil {
				logs.Debug("[SyncManager] Block %s incomplete, queueing for async resolution", block.ID)
				// å°è¯•ä½¿ç”¨å“åº”ä¸­çš„ ShortTxsï¼ˆå¦‚æœæœ‰ï¼‰
				shortTxs := msg.BlocksShortTxs[block.ID]
				sm.pendingBlockBuffer.AddPendingBlockForConsensus(block, shortTxs, types.NodeID(msg.From), 0, nil)
			}
			continue
		}
		if isNew {
			added++
		}
	}

	if added > 0 {
		Logf("[Node %s] ğŸ“¦ Successfully synced %d new blocks (heights %d-%d)\n",
			sm.nodeID, added, msg.FromHeight, msg.ToHeight)
	}
	if addErrs > 0 {
		logs.Warn("[Node %s] Sync received %d blocks, %d failed to add (first=%s err=%v)",
			sm.nodeID, len(msg.Blocks), addErrs, firstAddErrBlockID, firstAddErr)
	}
	if added == 0 && len(msg.Blocks) > 0 && addErrs == 0 {
		logs.Warn("[Node %s] Sync received %d blocks but none were new (heights %d-%d)",
			sm.nodeID, len(msg.Blocks), msg.FromHeight, msg.ToHeight)
	}

	// åŠ é€Ÿè¿½å—ï¼šå¦‚æœæ”¶åˆ°çš„æ˜¯å¯¹æ–¹â€œå·²æ¥å—é«˜åº¦â€èŒƒå›´å†…çš„åŒºå—ï¼Œåˆ™å¯ç›´æ¥æŒ‰çˆ¶é“¾å…³ç³»æ¨è¿›æœ¬åœ° lastAcceptedã€‚
	// è¿™èƒ½è§£å†³â€œæœ¬åœ°å·²æ‹¥æœ‰åŒºå—ä½†å…±è¯†è¿Ÿè¿Ÿæ— æ³•åœ¨è¯¥é«˜åº¦æ”¶æ•›â€å¯¼è‡´çš„é•¿æœŸåœæ»ï¼ˆåå¤ sync added=0ï¼‰ã€‚
	finalized := 0
	acceptedID, acceptedHeight := sm.store.GetLastAccepted()
	blocksByHeight := make(map[uint64][]*types.Block, len(msg.Blocks))
	for _, b := range msg.Blocks {
		if b == nil {
			continue
		}
		blocksByHeight[b.Header.Height] = append(blocksByHeight[b.Header.Height], b)
	}

	for {
		nextHeight := acceptedHeight + 1
		cands := blocksByHeight[nextHeight]
		if len(cands) == 0 {
			break
		}
		chosen := cands[0]
		for _, c := range cands {
			if c != nil && c.Header.ParentID == acceptedID {
				chosen = c
				break
			}
		}

		// å°è¯•æœ€ç»ˆåŒ–è¯¥é«˜åº¦ï¼ˆRealBlockStore å†…éƒ¨ä¼šåšçˆ¶é“¾æ¥å®‰å…¨æ£€æŸ¥ï¼›å¤±è´¥ä¼šä¿æŒ lastAccepted ä¸å˜ï¼‰
		sm.store.SetFinalized(nextHeight, chosen.ID)

		newAcceptedID, newAcceptedHeight := sm.store.GetLastAccepted()
		if newAcceptedHeight != nextHeight || newAcceptedID != chosen.ID {
			break
		}

		finalized++
		acceptedID, acceptedHeight = newAcceptedID, newAcceptedHeight

		// ä¸»åŠ¨å‘å¸ƒæœ€ç»ˆåŒ–äº‹ä»¶ï¼Œé©±åŠ¨ ProposalManager/SnapshotManager ç­‰ç»„ä»¶çŠ¶æ€å‰è¿›
		if blk, ok := sm.store.Get(chosen.ID); ok && blk != nil {
			sm.events.PublishAsync(types.BaseEvent{
				EventType: types.EventBlockFinalized,
				EventData: blk,
			})
		}
	}

	if finalized > 0 {
		logs.Info("[Node %s] âœ… Fast-finalized %d block(s) via sync (accepted=%d)",
			sm.nodeID, finalized, acceptedHeight)
		atomic.StoreUint32(&sm.consecutiveStallCount, 0) // é‡ç½®åœæ»è®¡æ•°
	} else if len(msg.Blocks) > 0 {
		// é«˜é£é™©ä¿®å¤ï¼šæ£€æµ‹åŒæ­¥åœæ»
		stalls := atomic.AddUint32(&sm.consecutiveStallCount, 1)
		if stalls >= 3 {
			logs.Warn("[Node %s] âš ï¸ Sync stalled for %d rounds at height %d, breaking pipeline and switching peers",
				sm.nodeID, stalls, acceptedHeight)
			sm.Mu.Lock()
			delete(sm.PeerHeights, types.NodeID(msg.From)) // æ¸…ç†è¯¥ Peer é«˜åº¦ä¿¡æ¯ï¼Œå¼ºåˆ¶é‡æ–°é‡‡æ ·
			sm.Syncing = false
			sm.Mu.Unlock()
			atomic.StoreUint32(&sm.consecutiveStallCount, 0)
			return
		}
	}

	sm.events.PublishAsync(types.BaseEvent{
		EventType: types.EventSyncComplete,
		EventData: added,
	})

	// ä¸­é£é™©ä¿®å¤ï¼šå¤„ç†å®Œæˆåå†è§£é”çŠ¶æ€
	sm.Mu.Lock()
	sm.Syncing = false
	sm.Mu.Unlock()

	// æµæ°´çº¿ç»­ä¼ ï¼šå¦‚æœä»è½åï¼Œç«‹å³å‘èµ·ä¸‹ä¸€è½®åŒæ­¥
	if added > 0 || finalized > 0 {
		_, localAccepted := sm.store.GetLastAccepted()
		maxPeer := sm.getMaxPeerHeight()
		if maxPeer > localAccepted+1 {
			go func() {
				time.Sleep(50 * time.Millisecond) // çŸ­æš‚å»¶è¿Ÿé¿å…å¤ªæ¿€è¿›
				sm.requestSync(localAccepted+1, minUint64(localAccepted+sm.config.BatchSize, maxPeer))
			}()
		}
	}
}

// getMaxPeerHeight è¿”å›å·²çŸ¥çš„æœ€å¤§å¯¹ç«¯é«˜åº¦
func (sm *SyncManager) getMaxPeerHeight() uint64 {
	sm.Mu.RLock()
	defer sm.Mu.RUnlock()

	maxHeight := uint64(0)
	for _, h := range sm.PeerHeights {
		if h > maxHeight {
			maxHeight = h
		}
	}
	return maxHeight
}
