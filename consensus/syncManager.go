package consensus

import (
	"context"
	"dex/interfaces"
	"dex/logs"
	"dex/pb"
	statedb "dex/stateDB"
	"dex/types"
	"errors"
	"fmt"
	"sort"
	"strings"
	"sync"
	"sync/atomic"
	"time"

	"google.golang.org/protobuf/proto"
)

// ============================================
// åŒæ­¥ç®¡ç†å™¨ - å¢å¼ºç‰ˆï¼ˆæ”¯æŒå¿«ç…§ï¼‰
// ============================================

type SyncManager struct {
	nodeID    types.NodeID
	node      *Node // æ–°å¢
	transport interfaces.Transport
	store     interfaces.BlockStore
	config    *SyncConfig

	events       interfaces.EventBus
	Logger       logs.Logger
	SyncRequests map[uint32]time.Time
	nextSyncID   uint32
	Syncing      bool
	Mu           sync.RWMutex
	PeerHeights  map[types.NodeID]uint64
	lastPoll     time.Time

	// é‡‡æ ·éªŒè¯ç›¸å…³å­—æ®µ
	sampling        bool                    // æ˜¯å¦æ­£åœ¨é‡‡æ ·éªŒè¯
	sampleResponses map[types.NodeID]uint64 // é‡‡æ ·å“åº”: nodeID -> acceptedHeight
	sampleStartTime time.Time               // é‡‡æ ·å¼€å§‹æ—¶é—´
	// äº‹ä»¶é©±åŠ¨åŒæ­¥ç›¸å…³
	pendingBlockBuffer    *PendingBlockBuffer  // å¾…å¤„ç†åŒºå—ç¼“å†²åŒºï¼ˆç”¨äºè¡¥è¯¾ï¼‰
	consecutiveStallCount uint32               // è¿ç»­åŒæ­¥åœæ»è®¡æ•°ï¼ˆé«˜é£é™©ä¿®å¤ï¼šæ­»å¾ªç¯ä¿æŠ¤ï¼‰
	InFlightSyncRanges    map[string]time.Time // æ–°å¢ï¼šæ­£åœ¨è¿›è¡Œçš„åŒæ­¥é«˜åº¦èŒƒå›´ï¼ˆå»é‡ï¼‰

	// Chits-trigger debounce state.
	chitPending          bool
	chitPendingHeight    uint64
	chitPendingFrom      types.NodeID
	chitPendingFirstSeen time.Time
	chitTimerArmed       bool
	lastChitTriggerAt    time.Time
}

type stateSnapshotFetcher interface {
	FetchStateSnapshotShards(peer types.NodeID, targetHeight uint64) (*types.StateSnapshotShardsResponse, error)
	FetchStateSnapshotPage(peer types.NodeID, snapshotHeight uint64, shard string, pageSize int, pageToken string) (*types.StateSnapshotPageResponse, error)
}

func NewSyncManager(id types.NodeID, transport interfaces.Transport, store interfaces.BlockStore, config *SyncConfig, events interfaces.EventBus, logger logs.Logger) *SyncManager {
	return &SyncManager{
		nodeID:             id,
		transport:          transport,
		store:              store,
		config:             config,
		events:             events,
		Logger:             logger,
		SyncRequests:       make(map[uint32]time.Time),
		PeerHeights:        make(map[types.NodeID]uint64),
		lastPoll:           time.Now(),
		sampleResponses:    make(map[types.NodeID]uint64),
		InFlightSyncRanges: make(map[string]time.Time),
	}
}

// SetPendingBlockBuffer è®¾ç½® PendingBlockBufferï¼ˆåœ¨åˆå§‹åŒ–åæ³¨å…¥ï¼‰
func (sm *SyncManager) SetPendingBlockBuffer(buffer *PendingBlockBuffer) {
	sm.pendingBlockBuffer = buffer
}

func (sm *SyncManager) Start(ctx context.Context) {
	// 1. æ¯éš” config.CheckInterval è¿›è¡Œä¸€æ¬¡å¸¸è§„åŒæ­¥æ£€æŸ¥ï¼ˆå…œåº•ï¼‰
	go func() {
		logs.SetThreadNodeContext(string(sm.nodeID))
		ticker := time.NewTicker(sm.config.CheckInterval)
		defer ticker.Stop()

		for {
			select {
			case <-ticker.C:
				sm.checkAndSync()
			case <-ctx.Done():
				return
			}
		}
	}()

	// 2. æ¯éš” config.CheckInterval è¿›è¡Œä¸€æ¬¡å¸¸è§„é«˜åº¦é‡‡æ ·ï¼ˆå…œåº•ï¼‰
	go func() {
		ticker := time.NewTicker(sm.config.CheckInterval)
		defer ticker.Stop()

		for {
			select {
			case <-ticker.C:
				sm.pollPeerHeights()
			case <-ctx.Done():
				return
			}
		}
	}()

	// 3. æ–°å¢ï¼šé«˜é¢‘å¥åº·æ£€æŸ¥å¾ªç¯ï¼ˆæ¯1ç§’ï¼‰ï¼Œç”¨äºå¤„ç†è¶…æ—¶æ¸…ç†å’Œä¸¢åŒ…æ¢å¤
	go func() {
		ticker := time.NewTicker(1 * time.Second)
		defer ticker.Stop()

		for {
			select {
			case <-ticker.C:
				sm.processTimeouts()
			case <-ctx.Done():
				return
			}
		}
	}()
}

// å®šæ—¶æŸ¥è¯¢å…¶ä»–èŠ‚ç‚¹é«˜åº¦
func (sm *SyncManager) pollPeerHeights() {
	// å¦‚æœæ­£åœ¨åŒæ­¥ä¸­ï¼Œæš‚åœè½®è¯¢é«˜åº¦ï¼Œé¿å…ç½‘ç»œæ‹¥å µ
	sm.Mu.RLock()
	if sm.Syncing {
		sm.Mu.RUnlock()
		return
	}
	sm.Mu.RUnlock()

	// é™åˆ¶è½®è¯¢é¢‘ç‡ï¼Œè½åæ—¶å¢åŠ æ¢æµ‹é¢‘ç‡
	cooldown := 2 * time.Second
	sm.Mu.RLock()
	_, accepted := sm.store.GetLastAccepted()
	maxPeer := uint64(0)
	for _, h := range sm.PeerHeights {
		if h > maxPeer {
			maxPeer = h
		}
	}
	if maxPeer > accepted+2 {
		cooldown = 500 * time.Millisecond // è½åè¾ƒå¤šæ—¶ï¼Œæ¯ 500ms æ¢æµ‹ä¸€æ¬¡
	}
	sm.Mu.RUnlock()

	if time.Since(sm.lastPoll) < cooldown {
		return
	}
	sm.lastPoll = time.Now()

	peers := sm.transport.SamplePeers(sm.nodeID, 10)
	for _, peer := range peers {
		sm.transport.Send(peer, types.Message{
			Type: types.MsgHeightQuery,
			From: sm.nodeID,
		})
	}
}

func (sm *SyncManager) HandleHeightQuery(msg types.Message) {
	_, height := sm.store.GetLastAccepted()
	currentHeight := sm.store.GetCurrentHeight()

	err := sm.transport.Send(types.NodeID(msg.From), types.Message{
		Type:          types.MsgHeightResponse,
		From:          sm.nodeID,
		Height:        height,
		CurrentHeight: currentHeight,
	})
	if err != nil {
		return
	}
}

func (sm *SyncManager) HandleHeightResponse(msg types.Message) {
	sm.Mu.Lock()
	defer sm.Mu.Unlock()
	// åŒæ­¥åº”è¯¥åŸºäºå¯¹æ–¹â€œå·²æœ€ç»ˆåŒ–/å·²æ¥å—â€çš„é«˜åº¦ï¼Œè€Œä¸æ˜¯å…¶å½“å‰æœ€å¤§é«˜åº¦ï¼ˆå¯èƒ½åŒ…å«å¤§é‡æœªæœ€ç»ˆåŒ–å—ï¼‰ã€‚
	// å¦åˆ™ä¼šå¯¼è‡´æœ¬èŠ‚ç‚¹ä¸æ–­å°è¯•åŒæ­¥ä¸€äº›å…¶å®å…¨ç½‘éƒ½è¿˜æ²¡æœ€ç»ˆåŒ–çš„é«˜åº¦ï¼Œå‡ºç°é‡å¤ sync ä¸” added=0 çš„æƒ…å†µã€‚
	sm.PeerHeights[types.NodeID(msg.From)] = msg.Height

	// å¦‚æœæ­£åœ¨é‡‡æ ·éªŒè¯ï¼Œæ”¶é›†å“åº”
	if sm.sampling {
		sm.sampleResponses[types.NodeID(msg.From)] = msg.Height
	}
}

// processTimeouts å¿«é€Ÿæ¸…ç†è¶…æ—¶çš„è¯·æ±‚ï¼ˆç”± 1s çš„å¥åº·å¾ªç¯è°ƒç”¨ï¼‰
func (sm *SyncManager) processTimeouts() {
	sm.Mu.Lock()
	defer sm.Mu.Unlock()

	if !sm.Syncing && !sm.sampling {
		return
	}

	now := time.Now()

	// æ£€æŸ¥åŒæ­¥è¯·æ±‚è¶…æ—¶
	if sm.Syncing {
		hasTimeout := false
		for syncID, startTime := range sm.SyncRequests {
			if now.Sub(startTime) > sm.config.Timeout {
				Logf("[Node %s] âš ï¸ Sync request %d timed out (started at %v)\n",
					sm.nodeID, syncID, startTime.Format("15:04:05"))
				delete(sm.SyncRequests, syncID)
				hasTimeout = true
			}
		}

		if hasTimeout && len(sm.SyncRequests) == 0 {
			sm.Logger.Warn("[Node %s] All sync requests timed out, resetting Syncing flag", sm.nodeID)
			sm.Syncing = false

		}
	}

	// æ£€æŸ¥é«˜åº¦é‡‡æ ·è¶…æ—¶
	if sm.sampling {
		if now.Sub(sm.sampleStartTime) > sm.config.SampleTimeout {
			sm.Logger.Debug("[Node %s] Sample verification timed out, resetting sampling flag", sm.nodeID)
			sm.sampling = false
		}
	}

	// æ–°å¢ï¼šæ£€æŸ¥å¹¶æ¸…ç†è¿‡æœŸçš„å¤„ç†ä¸­åŒæ­¥èŒƒå›´ï¼ˆå…œåº•ï¼‰
	for rangeKey, startTime := range sm.InFlightSyncRanges {
		if now.Sub(startTime) > sm.config.Timeout*2 {
			delete(sm.InFlightSyncRanges, rangeKey)
		}
	}
}

// æ£€æŸ¥æ˜¯å¦æœ‰å¿…è¦å¯åŠ¨åŒæ­¥ç¨‹åº
func (sm *SyncManager) checkAndSync() {
	sm.Mu.Lock()

	// æ­£åœ¨åŒæ­¥æœŸé—´ä¸å¯åŠ¨æ–°çš„æ£€æŸ¥
	if sm.Syncing {
		sm.Mu.Unlock()
		return
	}

	// 1. ä¼˜å…ˆæ£€æŸ¥é‡‡æ ·éªŒè¯çŠ¶æ€ï¼ˆé‡‡æ ·å‘å‡ºåéœ€è¦åœ¨æ­¤è¯„ä¼°ç»“æœï¼‰
	if sm.sampling {
		// æ£€æŸ¥é‡‡æ ·æ˜¯å¦è¶…æ—¶
		if time.Since(sm.sampleStartTime) > sm.config.SampleTimeout {
			sm.Logger.Debug("[Sync] Sample verification timed out, responses=%d", len(sm.sampleResponses))
			sm.sampling = false
			sm.Mu.Unlock()
			return
		}

		// è¯„ä¼° Quorum
		quorumHeight, ok := sm.evaluateSampleQuorum()
		if !ok {
			// å°šæœªè¾¾åˆ° Quorumï¼Œç­‰å¾…æ›´å¤šå“åº”
			sm.Mu.Unlock()
			return
		}

		// Quorum è¾¾æˆï¼Œå¯ä»¥å®‰å…¨åŒæ­¥
		sm.sampling = false
		_, localAcceptedHeight := sm.store.GetLastAccepted()

		if quorumHeight > localAcceptedHeight {
			sm.Logger.Info("[Sync] âœ… Quorum verified: target height %d confirmed by %.0f%% nodes",
				quorumHeight, sm.config.QuorumRatio*100)
			sm.Mu.Unlock()

			heightDiff := quorumHeight - localAcceptedHeight
			if heightDiff > sm.deepLagStateSyncThreshold() {
				sm.performStateDBFirstSyncThenCatchUp(quorumHeight, localAcceptedHeight)
				return
			}
			sm.requestSync(localAcceptedHeight+1, minUint64(localAcceptedHeight+sm.config.BatchSize, quorumHeight))
			return
		}

		sm.Mu.Unlock()
		return
	}

	// 2. åˆæ­¥æ£€æŸ¥æ˜¯å¦è½å
	maxPeerHeight := uint64(0)
	for _, height := range sm.PeerHeights {
		if height > maxPeerHeight {
			maxPeerHeight = height
		}
	}

	_, localAcceptedHeight := sm.store.GetLastAccepted()
	heightDiff := uint64(0)
	if maxPeerHeight > localAcceptedHeight {
		heightDiff = maxPeerHeight - localAcceptedHeight
	}

	// 3. å¦‚æœè½åè¶…è¿‡é˜ˆå€¼ï¼Œå¯åŠ¨é‡‡æ ·éªŒè¯
	if heightDiff > sm.config.BehindThreshold {
		sm.Logger.Debug("[Sync] Detected lag of %d blocks, starting sample verification (target=%d)",
			heightDiff, maxPeerHeight)
		sm.startHeightSampling()
	}

	sm.Mu.Unlock()
}

func (sm *SyncManager) chitSoftGap() uint64 {
	if sm == nil || sm.config == nil || sm.config.ChitSoftGap == 0 {
		return 1
	}
	return sm.config.ChitSoftGap
}

func (sm *SyncManager) chitHardGap() uint64 {
	if sm == nil || sm.config == nil || sm.config.ChitHardGap == 0 {
		return 3
	}
	soft := sm.chitSoftGap()
	if sm.config.ChitHardGap < soft {
		return soft
	}
	return sm.config.ChitHardGap
}

func (sm *SyncManager) chitGracePeriod() time.Duration {
	if sm == nil || sm.config == nil || sm.config.ChitGracePeriod <= 0 {
		return time.Second
	}
	return sm.config.ChitGracePeriod
}

func (sm *SyncManager) chitCooldown() time.Duration {
	if sm == nil || sm.config == nil || sm.config.ChitCooldown < 0 {
		return 0
	}
	return sm.config.ChitCooldown
}

func (sm *SyncManager) chitMinConfirmPeers() int {
	if sm == nil || sm.config == nil || sm.config.ChitMinConfirmPeers < 0 {
		return 0
	}
	return sm.config.ChitMinConfirmPeers
}

func (sm *SyncManager) deepLagStateSyncThreshold() uint64 {
	if sm == nil || sm.config == nil || sm.config.DeepLagStateSyncThreshold == 0 {
		return 100
	}
	return sm.config.DeepLagStateSyncThreshold
}

func (sm *SyncManager) stateSyncPeers() int {
	if sm == nil || sm.config == nil || sm.config.StateSyncPeers <= 0 {
		return 4
	}
	return sm.config.StateSyncPeers
}

func (sm *SyncManager) stateSyncShardConcurrency() int {
	if sm == nil || sm.config == nil || sm.config.StateSyncShardConcurrency <= 0 {
		return 8
	}
	return sm.config.StateSyncShardConcurrency
}

func (sm *SyncManager) stateSyncPageSize() int {
	if sm == nil || sm.config == nil || sm.config.StateSyncPageSize <= 0 {
		return 1000
	}
	return sm.config.StateSyncPageSize
}

func (sm *SyncManager) resetStaleSyncStateLocked() bool {
	if !sm.Syncing && !sm.sampling {
		return false
	}
	if len(sm.SyncRequests) > 0 {
		return true
	}
	sm.Logger.Debug("[SyncManager] Chit trigger: resetting stale sync state (Requests=0)")
	sm.Syncing = false
	sm.sampling = false
	return false
}

func (sm *SyncManager) triggerCooldownRemainingLocked(now time.Time) time.Duration {
	cooldown := sm.chitCooldown()
	if cooldown <= 0 || sm.lastChitTriggerAt.IsZero() {
		return 0
	}
	elapsed := now.Sub(sm.lastChitTriggerAt)
	if elapsed >= cooldown {
		return 0
	}
	return cooldown - elapsed
}

func (sm *SyncManager) markPendingChitLocked(height uint64, from types.NodeID) {
	now := time.Now()
	if !sm.chitPending {
		sm.chitPending = true
		sm.chitPendingHeight = height
		sm.chitPendingFrom = from
		sm.chitPendingFirstSeen = now
		return
	}
	if height > sm.chitPendingHeight {
		sm.chitPendingHeight = height
		sm.chitPendingFrom = from
		sm.chitPendingFirstSeen = now
	}
}

func (sm *SyncManager) clearPendingChitLocked() {
	sm.chitPending = false
	sm.chitPendingHeight = 0
	sm.chitPendingFrom = ""
	sm.chitPendingFirstSeen = time.Time{}
}

func (sm *SyncManager) countPeerConfirmationsLocked(targetHeight uint64, exclude types.NodeID) int {
	minConfirmedHeight := targetHeight
	if minConfirmedHeight > 0 {
		minConfirmedHeight--
	}
	confirmations := 0
	for peerID, h := range sm.PeerHeights {
		if peerID == exclude {
			continue
		}
		if h >= minConfirmedHeight {
			confirmations++
		}
	}
	return confirmations
}

func (sm *SyncManager) scheduleChitEvaluationLocked(delay time.Duration) {
	if !sm.chitPending {
		return
	}
	if delay <= 0 {
		delay = 50 * time.Millisecond
	}
	if sm.chitTimerArmed {
		return
	}
	sm.chitTimerArmed = true
	go func(wait time.Duration) {
		timer := time.NewTimer(wait)
		defer timer.Stop()
		<-timer.C
		sm.evaluatePendingChitTrigger()
	}(delay)
}

func (sm *SyncManager) evaluatePendingChitTrigger() {
	sm.Mu.Lock()
	sm.chitTimerArmed = false
	if !sm.chitPending {
		sm.Mu.Unlock()
		return
	}

	targetHeight := sm.chitPendingHeight
	from := sm.chitPendingFrom
	firstSeen := sm.chitPendingFirstSeen
	_, localAccepted := sm.store.GetLastAccepted()
	if targetHeight <= localAccepted {
		sm.clearPendingChitLocked()
		sm.Mu.Unlock()
		return
	}

	if sm.resetStaleSyncStateLocked() {
		sm.scheduleChitEvaluationLocked(200 * time.Millisecond)
		sm.Mu.Unlock()
		return
	}

	now := time.Now()
	if remain := sm.triggerCooldownRemainingLocked(now); remain > 0 {
		sm.scheduleChitEvaluationLocked(remain)
		sm.Mu.Unlock()
		return
	}

	grace := sm.chitGracePeriod()
	if elapsed := now.Sub(firstSeen); elapsed < grace {
		sm.scheduleChitEvaluationLocked(grace - elapsed)
		sm.Mu.Unlock()
		return
	}

	minConfirm := sm.chitMinConfirmPeers()
	if minConfirm > 0 {
		confirmed := sm.countPeerConfirmationsLocked(targetHeight, from)
		if confirmed < minConfirm {
			recheck := grace / 2
			if recheck <= 0 {
				recheck = 200 * time.Millisecond
			}
			sm.scheduleChitEvaluationLocked(recheck)
			sm.Mu.Unlock()
			return
		}
	}

	sm.clearPendingChitLocked()
	sm.lastChitTriggerAt = now
	sm.Mu.Unlock()

	heightDiff := targetHeight - localAccepted
	if heightDiff >= sm.chitHardGap() && heightDiff > sm.deepLagStateSyncThreshold() {
		sm.Logger.Info("[SyncManager] TriggerSyncFromChit: delayed deep lag peer=%s peerHeight=%d localAccepted=%d diff=%d threshold=%d, use stateDB-first catch-up",
			from, targetHeight, localAccepted, heightDiff, sm.deepLagStateSyncThreshold())
		sm.performStateDBFirstSyncThenCatchUp(targetHeight, localAccepted)
		return
	}

	sm.Logger.Debug("[SyncManager] TriggerSyncFromChit: delayed trigger peer=%s peerHeight=%d localAccepted=%d diff=%d",
		from, targetHeight, localAccepted, heightDiff)
	sm.performTriggeredSync(targetHeight, localAccepted, heightDiff)
}

// TriggerSyncFromChit is an event-driven sync trigger path with delay, threshold, and debounce.
func (sm *SyncManager) TriggerSyncFromChit(peerAcceptedHeight uint64, from types.NodeID) {
	sm.Mu.Lock()

	if peerAcceptedHeight > sm.PeerHeights[from] {
		sm.PeerHeights[from] = peerAcceptedHeight
	}

	_, localAccepted := sm.store.GetLastAccepted()
	if peerAcceptedHeight <= localAccepted {
		sm.Mu.Unlock()
		return
	}

	heightDiff := peerAcceptedHeight - localAccepted
	now := time.Now()

	// Hard gap path: trigger fast, still respecting stale-state guard and cooldown debounce.
	if heightDiff >= sm.chitHardGap() {
		if sm.resetStaleSyncStateLocked() {
			sm.markPendingChitLocked(peerAcceptedHeight, from)
			sm.scheduleChitEvaluationLocked(200 * time.Millisecond)
			sm.Mu.Unlock()
			return
		}
		if remain := sm.triggerCooldownRemainingLocked(now); remain > 0 {
			sm.markPendingChitLocked(peerAcceptedHeight, from)
			sm.scheduleChitEvaluationLocked(remain)
			sm.Mu.Unlock()
			return
		}

		sm.clearPendingChitLocked()
		sm.lastChitTriggerAt = now
		sm.Mu.Unlock()

		if heightDiff > sm.deepLagStateSyncThreshold() {
			sm.Logger.Info("[SyncManager] TriggerSyncFromChit: deep lag detected peer=%s peerHeight=%d localAccepted=%d diff=%d threshold=%d, use stateDB-first catch-up",
				from, peerAcceptedHeight, localAccepted, heightDiff, sm.deepLagStateSyncThreshold())
			sm.performStateDBFirstSyncThenCatchUp(peerAcceptedHeight, localAccepted)
			return
		}

		sm.Logger.Debug("[SyncManager] TriggerSyncFromChit: hard trigger peer=%s peerHeight=%d localAccepted=%d diff=%d",
			from, peerAcceptedHeight, localAccepted, heightDiff)
		sm.performTriggeredSync(peerAcceptedHeight, localAccepted, heightDiff)
		return
	}

	// Soft gap path: collect and wait for grace period + confirmations.
	if heightDiff < sm.chitSoftGap() {
		sm.Mu.Unlock()
		return
	}

	sm.markPendingChitLocked(peerAcceptedHeight, from)
	grace := sm.chitGracePeriod()
	if elapsed := now.Sub(sm.chitPendingFirstSeen); elapsed >= grace {
		sm.scheduleChitEvaluationLocked(10 * time.Millisecond)
	} else {
		sm.scheduleChitEvaluationLocked(grace - elapsed)
	}
	sm.Mu.Unlock()
}

// performTriggeredSync æ‰§è¡Œè¢«è§¦å‘çš„åŒæ­¥åŠ¨ä½œ
func (sm *SyncManager) performTriggeredSync(peerAcceptedHeight, localAccepted, heightDiff uint64) {
	// ä½¿ç”¨åˆ†ç‰‡å¹¶è¡ŒåŒæ­¥
	sm.requestSyncParallel(localAccepted+1, minUint64(localAccepted+sm.config.BatchSize, peerAcceptedHeight))
}

// performStateDBFirstSyncThenCatchUp æ·±åº¦è½åæ—¶å…ˆæ‰§è¡Œ stateDB-first è¿½èµ¶ï¼Œå†è¿›å…¥å¸¸è§„è¿½å—æµæ°´çº¿ã€‚
// ä¼˜å…ˆå°è¯•â€œåˆ†ç‰‡ + å¤š peerâ€å¹¶è¡Œä¸‹è½½ stateDB å¿«ç…§ï¼Œåˆ†æ‹…å•èŠ‚ç‚¹å‹åŠ›ï¼›å¤±è´¥æ—¶å›é€€åˆ°ä»…åŒºå—è¿½èµ¶ã€‚
func (sm *SyncManager) performStateDBFirstSyncThenCatchUp(peerAcceptedHeight, localAccepted uint64) {
	if peerAcceptedHeight <= localAccepted {
		return
	}

	if sm.performDistributedStateDBSync(peerAcceptedHeight) {
		sm.Logger.Info("[SyncManager] StateDB-first catch-up: distributed state snapshot synced, continue block catch-up")
	} else {
		sm.Logger.Warn("[SyncManager] StateDB-first catch-up: distributed state snapshot unavailable/failed, fallback to block-only catch-up")
	}

	window := sm.deepLagStateSyncThreshold()
	toHeight := minUint64(localAccepted+window, peerAcceptedHeight)
	if toHeight < localAccepted+1 {
		return
	}

	sm.Logger.Info("[SyncManager] StateDB-first catch-up: syncing heights %d-%d before normal pipeline",
		localAccepted+1, toHeight)
	sm.requestSync(localAccepted+1, toHeight)
}

func (sm *SyncManager) performDistributedStateDBSync(targetHeight uint64) bool {
	fetcher, ok := sm.transport.(stateSnapshotFetcher)
	if !ok {
		return false
	}
	realStore, ok := sm.store.(*RealBlockStore)
	if !ok || realStore == nil || realStore.dbManager == nil {
		return false
	}

	peers := sm.selectStateSyncPeers(targetHeight, sm.stateSyncPeers())
	if len(peers) == 0 {
		sm.Logger.Warn("[SyncManager] StateDB sync: no peers available for snapshot download")
		return false
	}

	var (
		shardResp *types.StateSnapshotShardsResponse
		metaPeer  types.NodeID
	)
	for _, peer := range peers {
		resp, err := fetcher.FetchStateSnapshotShards(peer, targetHeight)
		if err != nil {
			sm.Logger.Warn("[SyncManager] StateDB sync: fetch shards from %s failed: %v", peer, err)
			continue
		}
		if resp == nil {
			continue
		}
		shardResp = resp
		metaPeer = peer
		break
	}
	if shardResp == nil {
		sm.Logger.Warn("[SyncManager] StateDB sync: no shard metadata available")
		return false
	}

	snapshotHeight := shardResp.SnapshotHeight
	if snapshotHeight == 0 {
		snapshotHeight = targetHeight
	}

	type shardTask struct {
		shard string
		count int64
	}
	tasks := make([]shardTask, 0, len(shardResp.Shards))
	for _, shard := range shardResp.Shards {
		if shard.Shard == "" || shard.Count <= 0 {
			continue
		}
		tasks = append(tasks, shardTask{shard: shard.Shard, count: shard.Count})
	}
	sort.Slice(tasks, func(i, j int) bool {
		if tasks[i].count == tasks[j].count {
			return tasks[i].shard < tasks[j].shard
		}
		return tasks[i].count > tasks[j].count
	})

	if len(tasks) == 0 {
		sm.Logger.Info("[SyncManager] StateDB sync: metadata from %s has no shard items, applying empty snapshot at height %d", metaPeer, snapshotHeight)
		if err := realStore.dbManager.BuildStateSnapshotFromUpdates(context.Background(), snapshotHeight, nil); err != nil {
			sm.Logger.Warn("[SyncManager] StateDB sync: apply empty snapshot failed: %v", err)
			return false
		}
		return true
	}

	type shardResult struct {
		shard   string
		peer    types.NodeID
		updates []statedb.KVUpdate
		err     error
	}
	results := make(chan shardResult, len(tasks))
	sem := make(chan struct{}, sm.stateSyncShardConcurrency())
	pageSize := sm.stateSyncPageSize()

	for i, task := range tasks {
		primaryIdx := i % len(peers)
		go func(tk shardTask, startIdx int) {
			sem <- struct{}{}
			defer func() { <-sem }()

			updates, usedPeer, err := sm.fetchSnapshotShardWithFallback(fetcher, peers, startIdx, snapshotHeight, tk.shard, pageSize)
			results <- shardResult{
				shard:   tk.shard,
				peer:    usedPeer,
				updates: updates,
				err:     err,
			}
		}(task, primaryIdx)
	}

	allUpdates := make([]statedb.KVUpdate, 0, 4096)
	perPeerItems := make(map[types.NodeID]int)
	var firstErr error
	failed := 0

	for i := 0; i < len(tasks); i++ {
		res := <-results
		if res.err != nil {
			failed++
			if firstErr == nil {
				firstErr = res.err
			}
			sm.Logger.Warn("[SyncManager] StateDB sync: shard %s failed: %v", res.shard, res.err)
			continue
		}
		allUpdates = append(allUpdates, res.updates...)
		perPeerItems[res.peer] += len(res.updates)
	}

	if failed > 0 {
		sm.Logger.Warn("[SyncManager] StateDB sync: %d/%d shard(s) failed (first err: %v)", failed, len(tasks), firstErr)
		return false
	}

	if err := realStore.dbManager.BuildStateSnapshotFromUpdates(context.Background(), snapshotHeight, allUpdates); err != nil {
		sm.Logger.Warn("[SyncManager] StateDB sync: build local snapshot failed: %v", err)
		return false
	}

	for peer, cnt := range perPeerItems {
		sm.Logger.Info("[SyncManager] StateDB sync: peer %s served %d kv item(s)", peer, cnt)
	}
	sm.Logger.Info("[SyncManager] StateDB sync complete: height=%d shards=%d totalItems=%d metadataPeer=%s",
		snapshotHeight, len(tasks), len(allUpdates), metaPeer)
	return true
}

func (sm *SyncManager) fetchSnapshotShardWithFallback(
	fetcher stateSnapshotFetcher,
	peers []types.NodeID,
	startIdx int,
	snapshotHeight uint64,
	shard string,
	pageSize int,
) ([]statedb.KVUpdate, types.NodeID, error) {
	if len(peers) == 0 {
		return nil, "", fmt.Errorf("no peers available")
	}

	var lastErr error
	for i := 0; i < len(peers); i++ {
		peer := peers[(startIdx+i)%len(peers)]
		token := ""
		updates := make([]statedb.KVUpdate, 0, pageSize)

		for page := 0; page < 1_000_000; page++ {
			resp, err := fetcher.FetchStateSnapshotPage(peer, snapshotHeight, shard, pageSize, token)
			if err != nil {
				lastErr = err
				updates = nil
				break
			}
			if resp == nil {
				lastErr = fmt.Errorf("nil page response")
				updates = nil
				break
			}

			for _, item := range resp.Items {
				if item.Key == "" {
					continue
				}
				valCopy := make([]byte, len(item.Value))
				copy(valCopy, item.Value)
				updates = append(updates, statedb.KVUpdate{
					Key:     item.Key,
					Value:   valCopy,
					Deleted: false,
				})
			}

			if resp.NextPageToken == "" || resp.NextPageToken == token {
				return updates, peer, nil
			}
			token = resp.NextPageToken
		}
	}

	if lastErr == nil {
		lastErr = fmt.Errorf("all peers exhausted")
	}
	return nil, "", fmt.Errorf("fetch shard %s failed: %w", shard, lastErr)
}

func (sm *SyncManager) selectStateSyncPeers(targetHeight uint64, maxPeers int) []types.NodeID {
	if maxPeers <= 0 {
		maxPeers = 1
	}

	type peerHeight struct {
		id     types.NodeID
		height uint64
	}

	sm.Mu.RLock()
	candidates := make([]peerHeight, 0, len(sm.PeerHeights))
	for peerID, h := range sm.PeerHeights {
		if peerID == "" || peerID == sm.nodeID {
			continue
		}
		if targetHeight == 0 || h >= targetHeight {
			candidates = append(candidates, peerHeight{id: peerID, height: h})
		}
	}
	sm.Mu.RUnlock()

	sort.Slice(candidates, func(i, j int) bool {
		if candidates[i].height == candidates[j].height {
			return candidates[i].id < candidates[j].id
		}
		return candidates[i].height > candidates[j].height
	})

	selected := make([]types.NodeID, 0, maxPeers)
	seen := make(map[types.NodeID]struct{}, maxPeers)
	for _, c := range candidates {
		if _, exists := seen[c.id]; exists {
			continue
		}
		seen[c.id] = struct{}{}
		selected = append(selected, c.id)
		if len(selected) >= maxPeers {
			return selected
		}
	}

	extraPeers := sm.transport.SamplePeers(sm.nodeID, maxPeers*2)
	for _, peer := range extraPeers {
		if peer == "" || peer == sm.nodeID {
			continue
		}
		if _, exists := seen[peer]; exists {
			continue
		}
		seen[peer] = struct{}{}
		selected = append(selected, peer)
		if len(selected) >= maxPeers {
			break
		}
	}
	return selected
}

// requestSyncParallel åˆ†ç‰‡å¹¶è¡ŒåŒæ­¥ï¼šå°†é«˜åº¦èŒƒå›´åˆ†é…ç»™å¤šä¸ªèŠ‚ç‚¹åŒæ—¶è¯·æ±‚
func (sm *SyncManager) requestSyncParallel(fromHeight, toHeight uint64) {
	sm.Mu.Lock()
	if sm.Syncing {
		sm.Mu.Unlock()
		return
	}

	// å…¨å±€èŒƒå›´å»é‡
	rangeKey := fmt.Sprintf("%d-%d", fromHeight, toHeight)
	if startTime, exists := sm.InFlightSyncRanges[rangeKey]; exists {
		if time.Since(startTime) < sm.config.Timeout {
			sm.Mu.Unlock()
			return
		}
	}

	sm.Syncing = true
	sm.InFlightSyncRanges[rangeKey] = time.Now()
	sm.Mu.Unlock()

	// è·å–å¯ç”¨èŠ‚ç‚¹
	peers := sm.transport.SamplePeers(sm.nodeID, sm.config.ParallelPeers)
	if len(peers) == 0 {
		sm.Mu.Lock()
		sm.Syncing = false
		sm.Mu.Unlock()
		return
	}

	totalBlocks := toHeight - fromHeight + 1

	// å¦‚æœè¯·æ±‚èŒƒå›´å°æˆ–åªæœ‰1ä¸ªèŠ‚ç‚¹ï¼Œé€€åŒ–åˆ°æ™®é€šåŒæ­¥
	if totalBlocks <= 5 || len(peers) == 1 {
		sm.Mu.Lock()
		sm.Syncing = false
		delete(sm.InFlightSyncRanges, rangeKey) // æ¸…ç† keyï¼Œå¦åˆ™ requestSync ä¼šè¢«åŒä¸€ä¸ª key é˜»å¡
		sm.Mu.Unlock()
		sm.requestSync(fromHeight, toHeight)
		return
	}

	// è®¡ç®—æ¯ä¸ªèŠ‚ç‚¹è´Ÿè´£çš„é«˜åº¦èŒƒå›´
	rangePerPeer := totalBlocks / uint64(len(peers))

	sm.Logger.Info("[SyncManager] Starting parallel sync: heights %d-%d across %d peers",
		fromHeight, toHeight, len(peers))

	for i, peer := range peers {
		start := fromHeight + uint64(i)*rangePerPeer
		end := start + rangePerPeer - 1
		if i == len(peers)-1 {
			end = toHeight // æœ€åä¸€ä¸ªèŠ‚ç‚¹è´Ÿè´£å‰©ä½™
		}

		// ä¸ºæ¯ä¸ªåˆ†ç‰‡åˆ›å»ºç‹¬ç«‹çš„ SyncID
		syncID := atomic.AddUint32(&sm.nextSyncID, 1)
		sm.Mu.Lock()
		sm.SyncRequests[syncID] = time.Now()
		sm.Mu.Unlock()

		// åˆ¤æ–­æ˜¯å¦ä½¿ç”¨ ShortTxs æ¨¡å¼ï¼ˆåŸºäºæ€»è½åé‡è€Œéåˆ†ç‰‡å¤§å°ï¼‰
		useShortMode := totalBlocks <= sm.config.ShortSyncThreshold

		go func(p types.NodeID, s, e uint64, id uint32, shortMode bool) {
			sm.Logger.Debug("[SyncManager] Parallel shard: peer=%s heights=%d-%d shortMode=%v", p, s, e, shortMode)

			msg := types.Message{
				Type:          types.MsgSyncRequest,
				From:          sm.nodeID,
				SyncID:        id,
				FromHeight:    s,
				ToHeight:      e,
				SyncShortMode: shortMode,
			}
			sm.transport.Send(p, msg)
		}(peer, start, end, syncID, useShortMode)
	}
}

// startHeightSampling å¯åŠ¨é‡‡æ ·éªŒè¯ï¼ˆå¿…é¡»æŒæœ‰å†™é”è°ƒç”¨ï¼‰
func (sm *SyncManager) startHeightSampling() {
	sm.sampling = true
	sm.sampleResponses = make(map[types.NodeID]uint64)
	sm.sampleStartTime = time.Now()

	// é‡‡æ · K ä¸ªèŠ‚ç‚¹
	peers := sm.transport.SamplePeers(sm.nodeID, sm.config.SampleSize)
	for _, peer := range peers {
		sm.transport.Send(peer, types.Message{
			Type: types.MsgHeightQuery,
			From: sm.nodeID,
		})
	}
}

// evaluateSampleQuorum è¯„ä¼°é‡‡æ · Quorumï¼ˆå¿…é¡»æŒæœ‰è¯»é”è°ƒç”¨ï¼‰
// è¿”å›æ»¡è¶³ Quorum çš„æœ€é«˜å·²æœ€ç»ˆåŒ–é«˜åº¦
func (sm *SyncManager) evaluateSampleQuorum() (uint64, bool) {
	responseCount := len(sm.sampleResponses)
	if responseCount == 0 {
		return 0, false
	}

	// è®¡ç®— Quorum é˜ˆå€¼
	required := int(float64(sm.config.SampleSize) * sm.config.QuorumRatio)
	if required < 1 {
		required = 1
	}

	// å¦‚æœå“åº”ä¸è¶³ï¼Œç»§ç»­ç­‰å¾…
	if responseCount < required {
		return 0, false
	}

	// æ”¶é›†æ‰€æœ‰é«˜åº¦å¹¶æ’åº
	heights := make([]uint64, 0, responseCount)
	for _, h := range sm.sampleResponses {
		heights = append(heights, h)
	}

	// ä»é«˜åˆ°ä½æ‰¾åˆ°æ»¡è¶³ Quorum çš„æœ€é«˜é«˜åº¦
	// å¯¹äºæ¯ä¸ªå€™é€‰é«˜åº¦ï¼Œç»Ÿè®¡æœ‰å¤šå°‘èŠ‚ç‚¹çš„ acceptedHeight >= è¯¥é«˜åº¦
	var maxQuorumHeight uint64
	for _, candidateHeight := range heights {
		supportCount := 0
		for _, h := range sm.sampleResponses {
			if h >= candidateHeight {
				supportCount++
			}
		}
		if supportCount >= required && candidateHeight > maxQuorumHeight {
			maxQuorumHeight = candidateHeight
		}
	}

	if maxQuorumHeight > 0 {
		sm.Logger.Debug("[Sync] Quorum check: %d/%d nodes support height %d (required=%d)",
			len(sm.sampleResponses), sm.config.SampleSize, maxQuorumHeight, required)
		return maxQuorumHeight, true
	}

	return 0, false
}

func (sm *SyncManager) requestSync(fromHeight, toHeight uint64) {
	sm.Mu.Lock()
	if sm.Syncing {
		sm.Mu.Unlock()
		return
	}

	rangeKey := fmt.Sprintf("%d-%d", fromHeight, toHeight)
	if startTime, exists := sm.InFlightSyncRanges[rangeKey]; exists {
		if time.Since(startTime) < sm.config.Timeout {
			sm.Mu.Unlock()
			return
		}
	}

	sm.Syncing = true
	syncID := atomic.AddUint32(&sm.nextSyncID, 1)
	sm.SyncRequests[syncID] = time.Now()
	sm.InFlightSyncRanges[rangeKey] = time.Now()
	sm.Mu.Unlock()

	sm.Mu.RLock()
	var targetPeer types.NodeID = "-1"
	for peer, height := range sm.PeerHeights {
		if height >= toHeight {
			targetPeer = peer
			break
		}
	}
	sm.Mu.RUnlock()

	if targetPeer == "-1" {
		peers := sm.transport.SamplePeers(sm.nodeID, 5)
		if len(peers) > 0 {
			targetPeer = peers[0]
		}
	}

	if targetPeer != "-1" {
		Logf("[Node %s] Requesting sync from Node %s for heights %d-%d\n",
			sm.nodeID, targetPeer, fromHeight, toHeight)
		sm.transport.Send(targetPeer, types.Message{
			Type: types.MsgSyncRequest, From: sm.nodeID, SyncID: syncID,
			FromHeight: fromHeight, ToHeight: toHeight,
		})
	} else {
		sm.Mu.Lock()
		sm.Syncing = false
		delete(sm.SyncRequests, syncID)
		sm.Mu.Unlock()
	}
}

func (sm *SyncManager) HandleSyncRequest(msg types.Message) {
	blocks := sm.store.GetBlocksFromHeight(msg.FromHeight, msg.ToHeight)

	Logf("[Node %s] Received sync request from Node %s for heights %d-%d (found %d blocks, shortMode=%v)\n",
		sm.nodeID, msg.From, msg.FromHeight, msg.ToHeight, len(blocks), msg.SyncShortMode)

	// å¼ºåˆ¶è¦æ±‚ï¼šåŒæ­¥å“åº”ä¸­çš„åŒºå—å¿…é¡»æºå¸¦å¯éªŒè¯çš„ VRF ç­¾åé›†åˆã€‚
	// å¯¹ç¼ºå°‘ç­¾åé›†åˆæˆ–åºåˆ—åŒ–å¤±è´¥çš„åŒºå—ç›´æ¥å‰”é™¤ï¼Œä¸å‘é€ç»™å¯¹ç«¯ã€‚
	responseBlocks := blocks
	sigSets := make(map[uint64][]byte)
	if realStore, ok := sm.store.(*RealBlockStore); ok {
		filtered := make([]*types.Block, 0, len(blocks))
		for _, block := range blocks {
			if block == nil {
				continue
			}
			sigSet, exists := realStore.GetSignatureSet(block.Header.Height)
			if !exists || sigSet == nil {
				sm.Logger.Warn("[SyncManager] Skip sync block %s at height %d for %s: missing VRF signature set",
					block.ID, block.Header.Height, msg.From)
				continue
			}
			data, err := proto.Marshal(sigSet)
			if err != nil {
				sm.Logger.Warn("[SyncManager] Skip sync block %s at height %d for %s: marshal signature set failed: %v",
					block.ID, block.Header.Height, msg.From, err)
				continue
			}
			sigSets[block.Header.Height] = data
			filtered = append(filtered, block)
		}
		responseBlocks = filtered
	} else {
		sm.Logger.Warn("[SyncManager] Store %T has no signature-set support, returning empty strict sync response", sm.store)
		responseBlocks = nil
	}

	response := types.Message{
		Type:          types.MsgSyncResponse,
		From:          sm.nodeID,
		SyncID:        msg.SyncID,
		Blocks:        responseBlocks,
		FromHeight:    msg.FromHeight,
		ToHeight:      msg.ToHeight,
		SyncShortMode: msg.SyncShortMode,
	}

	if len(sigSets) > 0 {
		response.SignatureSets = sigSets
	}

	// è‡ªé€‚åº”ä¼ è¾“æ¨¡å¼
	if msg.SyncShortMode {
		// çŸ­æœŸè½åæ¨¡å¼ï¼šé™„å¸¦ ShortTxs ç”¨äºå¿«é€Ÿè¿˜åŸ
		response.BlocksShortTxs = make(map[string][]byte)
		for _, block := range responseBlocks {
			if block == nil {
				continue
			}
			if cachedBlock, exists := GetCachedBlock(block.ID); exists && cachedBlock != nil {
				response.BlocksShortTxs[block.ID] = cachedBlock.ShortTxs
			}
		}
	} else {
		for _, block := range responseBlocks {
			if block == nil {
				continue
			}
			if cachedBlock, exists := GetCachedBlock(block.ID); exists && cachedBlock != nil {
				_ = cachedBlock // å·²æœ‰ç¼“å­˜æ•°æ®
			}
		}
	}

	sm.transport.Send(types.NodeID(msg.From), response)
}

func (sm *SyncManager) HandleSyncResponse(msg types.Message) {
	sm.Mu.Lock()
	if _, ok := sm.SyncRequests[msg.SyncID]; !ok {
		sm.Mu.Unlock()
		return
	}
	delete(sm.SyncRequests, msg.SyncID)
	// å¤„ç†å®Œæˆåæ¸…ç† rangeï¼ˆæˆ–è€…ä¿ç•™ä¸€æ®µæ—¶é—´ç”± timeout æ¸…ç†ï¼‰
	rangeKey := fmt.Sprintf("%d-%d", msg.FromHeight, msg.ToHeight)
	// ç”±äºå¹¶è¡ŒåŒæ­¥ä¼šæœ‰å­èŒƒå›´ï¼Œè¿™é‡Œç®€å•å¤„ç†ï¼Œå®é™…æ¨èç”± timeout è‡ªåŠ¨è¿‡æœŸï¼Œè¿™é‡Œä»…ç¤ºä¾‹
	delete(sm.InFlightSyncRanges, rangeKey)
	sm.Mu.Unlock()

	// å¼ºåˆ¶è¦æ±‚ï¼šæ¯ä¸ªå‚ä¸åŒæ­¥å¤„ç†/æœ€ç»ˆåŒ–çš„é«˜åº¦éƒ½å¿…é¡»å…ˆé€šè¿‡ VRF ç­¾åé›†åˆéªŒè¯ã€‚
	verifiedSigSets := make(map[uint64]*pb.ConsensusSignatureSet)
	seenHeights := make(map[uint64]struct{})
	for _, block := range msg.Blocks {
		if block == nil {
			continue
		}
		h := block.Header.Height
		if _, seen := seenHeights[h]; seen {
			continue
		}
		seenHeights[h] = struct{}{}

		sigData, hasSig := msg.SignatureSets[h]
		if !hasSig || len(sigData) == 0 {
			sm.Logger.Warn("[SyncManager] Reject sync height %d from %s: missing VRF signature set", h, msg.From)
			continue
		}

		var sigSet pb.ConsensusSignatureSet
		if err := proto.Unmarshal(sigData, &sigSet); err != nil {
			sm.Logger.Warn("[SyncManager] Reject sync height %d from %s: decode signature set failed: %v", h, msg.From, err)
			continue
		}
		if !VerifySignatureSet(&sigSet, sm.config.SyncAlpha, sm.config.SyncBeta, sm.transport, sm.nodeID) {
			sm.Logger.Warn("[SyncManager] Reject sync height %d from %s: signature set verification failed", h, msg.From)
			continue
		}
		verifiedSigSets[h] = &sigSet
	}

	added := 0
	addErrs := 0
	var firstAddErr error
	var firstAddErrBlockID string
	for _, block := range msg.Blocks {
		if block == nil {
			continue
		}

		if _, ok := verifiedSigSets[block.Header.Height]; !ok {
			addErrs++
			if firstAddErr == nil {
				firstAddErr = fmt.Errorf("missing/invalid VRF signature set")
				firstAddErrBlockID = block.ID
			}
			sm.Logger.Warn("[SyncManager] Skip block %s at height %d from %s: missing/invalid VRF signature set",
				block.ID, block.Header.Height, msg.From)
			continue
		}

		// å¦‚æœæ˜¯ ShortMode ä¸”æœ‰ ShortTxs æ•°æ®ï¼Œæå‰äº¤ç»™ PendingBlockBuffer å¤„ç†
		if msg.SyncShortMode && len(msg.BlocksShortTxs) > 0 {
			shortTxs := msg.BlocksShortTxs[block.ID]
			if len(shortTxs) > 0 && sm.pendingBlockBuffer != nil {
				// ä½¿ç”¨ ShortTxs å°è¯•è¿˜åŸåŒºå—
				sm.pendingBlockBuffer.AddPendingBlockForConsensus(block, shortTxs, types.NodeID(msg.From), 0, nil)
				added++ // ä¹è§‚è®¡æ•°ï¼Œå®é™…è¿˜åŸç”± buffer å¼‚æ­¥å®Œæˆ
				continue
			}
		}

		// å°è¯•ç›´æ¥æ·»åŠ 
		isNew, err := sm.store.Add(block)
		if err != nil {
			addErrs++
			if firstAddErr == nil {
				firstAddErr = err
				firstAddErrBlockID = block.ID
			}
			// æ¥å…¥è¡¥è¯¾æœºåˆ¶ï¼šå¦‚æœæ˜¯æ•°æ®ä¸å®Œæ•´å¯¼è‡´çš„å¤±è´¥ï¼ŒåŠ å…¥ PendingBlockBuffer
			if strings.Contains(err.Error(), "block data incomplete") && sm.pendingBlockBuffer != nil {
				sm.Logger.Debug("[SyncManager] Block %s incomplete, queueing for async resolution", block.ID)
				// å°è¯•ä½¿ç”¨å“åº”ä¸­çš„ ShortTxsï¼ˆå¦‚æœæœ‰ï¼‰
				shortTxs := msg.BlocksShortTxs[block.ID]
				sm.pendingBlockBuffer.AddPendingBlockForConsensus(block, shortTxs, types.NodeID(msg.From), 0, nil)
			}
			continue
		}
		if isNew {
			added++
		}
	}

	if added > 0 {
		Logf("[Node %s] ğŸ“¦ Successfully synced %d new blocks (heights %d-%d)\n",
			sm.nodeID, added, msg.FromHeight, msg.ToHeight)
	}
	if addErrs > 0 {
		sm.Logger.Warn("[Node %s] Sync received %d blocks, %d failed to add (first=%s err=%v)",
			sm.nodeID, len(msg.Blocks), addErrs, firstAddErrBlockID, firstAddErr)
	}
	if added == 0 && len(msg.Blocks) > 0 && addErrs == 0 {
		// ä¼˜åŒ–ï¼šæ ¹æ®å½“å‰çŠ¶æ€è°ƒæ•´æ—¥å¿—çº§åˆ«ã€‚å¦‚æœæ˜¯æ—©äºæˆ–ç­‰äºå½“å‰é«˜åº¦çš„åŒæ­¥ï¼Œä½¿ç”¨ Debugã€‚
		_, acceptedHeight := sm.store.GetLastAccepted()
		if msg.ToHeight <= acceptedHeight {
			sm.Logger.Debug("[Node %s] Sync received %d blocks but none were new (heights %d-%d, current accepted=%d)",
				sm.nodeID, len(msg.Blocks), msg.FromHeight, msg.ToHeight, acceptedHeight)
		} else {
			sm.Logger.Warn("[Node %s] Sync received %d blocks but none were new (heights %d-%d, current accepted=%d)",
				sm.nodeID, len(msg.Blocks), msg.FromHeight, msg.ToHeight, acceptedHeight)
		}
	}

	// åŠ é€Ÿè¿½å—ï¼šå¦‚æœæ”¶åˆ°çš„æ˜¯å¯¹æ–¹"å·²æ¥å—é«˜åº¦"èŒƒå›´å†…çš„åŒºå—ï¼Œåˆ™å¯ç›´æ¥æŒ‰çˆ¶é“¾å…³ç³»æ¨è¿›æœ¬åœ° lastAcceptedã€‚
	// è¿™èƒ½è§£å†³"æœ¬åœ°å·²æ‹¥æœ‰åŒºå—ä½†å…±è¯†è¿Ÿè¿Ÿæ— æ³•åœ¨è¯¥é«˜åº¦æ”¶æ•›"å¯¼è‡´çš„é•¿æœŸåœæ»ï¼ˆåå¤ sync added=0ï¼‰ã€‚
	//
	// æ³¨æ„ï¼šæ­¤å¤„åªä½¿ç”¨ sync å“åº”ä¸­çš„åŒºå—ï¼Œä¸æ··å…¥æœ¬åœ° store çš„å€™é€‰ã€‚
	// åŸå› ï¼šsync å“åº”æ¥è‡ªå·²ç»æœ€ç»ˆåŒ–çš„ peerï¼Œä»£è¡¨å®ƒçš„æœ€ç»ˆåŒ–é“¾ã€‚æœ¬åœ° store å¯èƒ½æœ‰
	// æœªè¢«é€‰ä¸­åˆ†æ”¯çš„å€™é€‰åŒºå—ï¼ˆä¸åŒ Window/ä¸åŒ parentï¼‰ï¼Œæ··å…¥ä¼šå¯¼è‡´é€‰æ‹©åˆ°
	// çˆ¶é“¾ä¸å…¼å®¹çš„åŒºå—ï¼Œé€ æˆ SetFinalized å¤±è´¥ï¼Œé“¾æ¡ä¸­æ–­ã€‚
	// é˜²åˆ†å‰ä¿æŠ¤åœ¨æŠ•ç¥¨å±‚ï¼ˆselectBestCandidate åå¥½ä½ Window + sendChits ä¸€è‡´æ€§ï¼‰
	// å’Œç­¾åéªŒè¯å±‚ï¼ˆVerifySignatureSetï¼‰å®ç°ã€‚
	finalized := 0
	acceptedID, acceptedHeight := sm.store.GetLastAccepted()
	blocksByHeight := make(map[uint64][]*types.Block, len(msg.Blocks))
	for _, b := range msg.Blocks {
		if b == nil {
			continue
		}
		blocksByHeight[b.Header.Height] = append(blocksByHeight[b.Header.Height], b)
	}

	for {
		nextHeight := acceptedHeight + 1
		cands := blocksByHeight[nextHeight]
		if len(cands) == 0 {
			break
		}
		// ä» sync å“åº”çš„å€™é€‰ä¸­é€‰æ‹©çˆ¶é“¾åŒ¹é…çš„åŒºå—
		var chosen *types.Block
		for _, c := range cands {
			if c != nil && c.Header.ParentID == acceptedID {
				chosen = c
				break
			}
		}
		// å¦‚æœæ²¡æœ‰çˆ¶é“¾åŒ¹é…çš„ï¼Œé€€åŒ–ä¸ºç¬¬ä¸€ä¸ªå€™é€‰ï¼ˆRealBlockStore.SetFinalized å†…éƒ¨ä¼šåšå®‰å…¨æ£€æŸ¥ï¼‰
		if chosen == nil {
			chosen = cands[0]
		}

		// å¼ºåˆ¶è¦æ±‚ï¼šæœ€ç»ˆåŒ–å‰å¿…é¡»æœ‰å·²éªŒè¯çš„ç­¾åé›†åˆã€‚
		sigSet, hasSig := verifiedSigSets[nextHeight]
		if !hasSig || sigSet == nil {
			sm.Logger.Warn("[SyncManager] Missing verified signature set for height %d, stop fast-finalize", nextHeight)
			break
		}
		// éªŒè¯é€šè¿‡ï¼Œå­˜å‚¨åˆ°æœ¬åœ°
		if realStore, ok := sm.store.(*RealBlockStore); ok {
			realStore.SetSignatureSet(nextHeight, sigSet)
		}

		// å°è¯•æœ€ç»ˆåŒ–è¯¥é«˜åº¦ï¼ˆå¤±è´¥ä¼šä¿æŒ lastAccepted ä¸å˜ï¼‰
		if err := sm.store.SetFinalized(nextHeight, chosen.ID); err != nil {
			if errors.Is(err, ErrAlreadyFinalized) {
				acceptedID, acceptedHeight = sm.store.GetLastAccepted()
				continue
			}
			sm.Logger.Warn("[SyncManager] Failed to finalize block %s at height %d: %v", chosen.ID, nextHeight, err)
			break
		}

		newAcceptedID, newAcceptedHeight := sm.store.GetLastAccepted()
		if newAcceptedHeight != nextHeight || newAcceptedID != chosen.ID {
			break
		}

		finalized++
		acceptedID, acceptedHeight = newAcceptedID, newAcceptedHeight

		// ä¸»åŠ¨å‘å¸ƒæœ€ç»ˆåŒ–äº‹ä»¶ï¼Œé©±åŠ¨ ProposalManager/SnapshotManager ç­‰ç»„ä»¶çŠ¶æ€å‰è¿›
		if blk, ok := sm.store.Get(chosen.ID); ok && blk != nil {
			sm.events.PublishAsync(types.BaseEvent{
				EventType: types.EventBlockFinalized,
				EventData: blk,
			})
		}
	}

	if finalized > 0 {
		sm.Logger.Info("[Node %s] âœ… Fast-finalized %d block(s) via sync (accepted=%d)",
			sm.nodeID, finalized, acceptedHeight)
		atomic.StoreUint32(&sm.consecutiveStallCount, 0) // é‡ç½®åœæ»è®¡æ•°
	} else if len(msg.Blocks) > 0 {
		// é«˜é£é™©ä¿®å¤ï¼šæ£€æµ‹åŒæ­¥åœæ»
		// ä¼˜åŒ–ï¼šåªæœ‰å½“åŒæ­¥èŒƒå›´è·¨è¶Šäº†å½“å‰å·²æ¥å—é«˜åº¦ï¼Œä¸”æœªèƒ½æ¨è¿›æ—¶ï¼Œæ‰è®¡å…¥ stall
		if msg.ToHeight > acceptedHeight {
			stalls := atomic.AddUint32(&sm.consecutiveStallCount, 1)
			if stalls >= 3 {
				sm.Logger.Debug("[Node %s] âš ï¸ Sync stalled for %d rounds at height %d, breaking pipeline and switching peers",
					sm.nodeID, stalls, acceptedHeight)
				sm.Mu.Lock()
				delete(sm.PeerHeights, types.NodeID(msg.From)) // æ¸…ç†è¯¥ Peer é«˜åº¦ä¿¡æ¯ï¼Œå¼ºåˆ¶é‡æ–°é‡‡æ ·
				sm.Syncing = false
				sm.Mu.Unlock()
				atomic.StoreUint32(&sm.consecutiveStallCount, 0)
				return
			}
		} else {
			sm.Logger.Debug("[Node %s] Sync response for heights %d-%d is older than accepted %d, ignoring stall check",
				sm.nodeID, msg.FromHeight, msg.ToHeight, acceptedHeight)
		}
	}

	// ä½é£é™©ä¼˜åŒ–ï¼šä¿¡å·ç²¾å‡†åŒ–
	// åªæœ‰å½“æœ¬åœ°é«˜åº¦ä¸å·²çŸ¥ Peer æœ€å¤§é«˜åº¦å·®è·å°äº BatchSize æ—¶æ‰å‘å¸ƒ EventSyncCompleteã€‚
	// é¿å…åœ¨é•¿è·ç¦»è¿½å—è¿‡ç¨‹ä¸­ï¼Œæ¯ä¸€è½®åŒæ­¥éƒ½å”¤é†’ QueryManager å‘èµ·æ— æ•ˆæŸ¥è¯¢ã€‚
	if added > 0 || finalized > 0 {
		_, curAccepted := sm.store.GetLastAccepted()
		maxPeer := sm.getMaxPeerHeight()
		if maxPeer <= curAccepted+sm.config.BatchSize {
			sm.events.PublishAsync(types.BaseEvent{
				EventType: types.EventSyncComplete,
				EventData: added,
			})
		} else {
			sm.Logger.Debug("[Node %s] Skipping EventSyncComplete signal: still behind by %d blocks",
				sm.nodeID, maxPeer-curAccepted)
		}
	}

	// ä¸­é£é™©ä¿®å¤ï¼šå¤„ç†å®Œæˆåå†è§£é”çŠ¶æ€
	sm.Mu.Lock()
	sm.Syncing = false
	sm.Mu.Unlock()

	// æµæ°´çº¿ç»­ä¼ ï¼šå¦‚æœä»è½åï¼Œç«‹å³å‘èµ·ä¸‹ä¸€è½®åŒæ­¥
	if added > 0 || finalized > 0 {
		_, localAccepted := sm.store.GetLastAccepted()
		maxPeer := sm.getMaxPeerHeight()
		if maxPeer > localAccepted+1 {
			go func() {
				time.Sleep(50 * time.Millisecond) // çŸ­æš‚å»¶è¿Ÿé¿å…å¤ªæ¿€è¿›
				sm.requestSync(localAccepted+1, minUint64(localAccepted+sm.config.BatchSize, maxPeer))
			}()
		}
	}
}

// getMaxPeerHeight è¿”å›å·²çŸ¥çš„æœ€å¤§å¯¹ç«¯é«˜åº¦
func (sm *SyncManager) getMaxPeerHeight() uint64 {
	sm.Mu.RLock()
	defer sm.Mu.RUnlock()

	maxHeight := uint64(0)
	for _, h := range sm.PeerHeights {
		if h > maxHeight {
			maxHeight = h
		}
	}
	return maxHeight
}

// VerifySignatureSet éªŒè¯å…±è¯†ç­¾åé›†åˆçš„åˆæ³•æ€§ï¼ˆä¸‰æ­¥éªŒè¯ï¼‰
// 1. è½®æ¬¡å®Œæ•´æ€§ï¼šlen(rounds) >= beta
// 2. æ¯è½®ç­¾åå……è¶³ï¼šlen(signatures) >= alpha
// 3. é‡‡æ ·åˆæ³•æ€§ï¼šé‡æ¼” samplePeersDeterministicï¼Œç¡®è®¤ node_id åœ¨åˆæ³•é‡‡æ ·é›†ä¸­
func VerifySignatureSet(sigSet *pb.ConsensusSignatureSet, alpha, beta int, transport interfaces.Transport, localNodeID types.NodeID) bool {
	if sigSet == nil {
		return false
	}

	// å¦‚æœ alpha/beta æœªé…ç½®ï¼ˆä¸º 0ï¼‰ï¼Œä½¿ç”¨å®½æ¾çš„é»˜è®¤å€¼
	if alpha <= 0 {
		alpha = 1
	}
	if beta <= 0 {
		beta = 1
	}

	// æ­¥éª¤ 1ï¼šè½®æ¬¡å®Œæ•´æ€§
	if len(sigSet.Rounds) < beta {
		logs.Debug("[VerifySignatureSet] Failed: rounds=%d < beta=%d", len(sigSet.Rounds), beta)
		return false
	}

	// æ­¥éª¤ 2ï¼šæ¯è½®ç­¾åå……è¶³
	for i, round := range sigSet.Rounds {
		if len(round.Signatures) < alpha {
			logs.Debug("[VerifySignatureSet] Failed: round %d signatures=%d < alpha=%d",
				i, len(round.Signatures), alpha)
			return false
		}
	}

	// æ­¥éª¤ 3ï¼šé‡‡æ ·åˆæ³•æ€§ï¼ˆé‡æ¼” VRF ç¡®å®šæ€§é‡‡æ ·ï¼Œç¡®è®¤æ¯ä¸ªç­¾åè€…åœ¨åˆæ³•é‡‡æ ·é›†ä¸­ï¼‰
	if transport != nil && len(sigSet.VrfSeed) > 0 {
		allPeers := transport.GetAllPeers(localNodeID)
		if len(allPeers) > 0 {
			// ç”¨ K = len(allPeers) ä½œä¸ºä¸Šé™ï¼ˆé‡‡æ ·é›†ä¸ä¼šè¶…è¿‡å…¨éƒ¨èŠ‚ç‚¹æ•°ï¼‰
			k := len(allPeers)
			for _, round := range sigSet.Rounds {
				sampled := samplePeersDeterministic(sigSet.VrfSeed, round.SeqId, k, allPeers)
				sampledSet := make(map[string]bool, len(sampled))
				for _, p := range sampled {
					sampledSet[string(p)] = true
				}

				for _, sig := range round.Signatures {
					if !sampledSet[sig.NodeId] {
						logs.Debug("[VerifySignatureSet] Failed: node %s not in sampled set for round seq=%d",
							sig.NodeId, round.SeqId)
						return false
					}
				}
			}
		}
	}

	// æ­¥éª¤ 4ï¼šå¯†ç å­¦éªŒç­¾ï¼ˆECDSA ç­¾åéªŒè¯ï¼‰
	// å¯¹æ¯ä¸ª ChitSignature é‡ç®— digest å¹¶éªŒè¯ç­¾å
	// å½“å…¬é’¥æ³¨å†Œè¡¨ä¸ºç©ºï¼ˆå¦‚æ–°èŠ‚ç‚¹é¦–æ¬¡åŒæ­¥ï¼‰æ—¶è·³è¿‡æ­¤æ­¥éª¤
	hasPublicKeys := false
	nodePublicKeysMu.RLock()
	hasPublicKeys = len(nodePublicKeys) > 0
	nodePublicKeysMu.RUnlock()

	if hasPublicKeys {
		for _, round := range sigSet.Rounds {
			for _, sig := range round.Signatures {
				if len(sig.Signature) == 0 {
					continue // å…¼å®¹æ—§ç‰ˆæœ¬æ— ç­¾åçš„è®°å½•
				}
				digest := ComputeChitDigest(sig.PreferredId, sigSet.Height, sigSet.VrfSeed, round.SeqId)
				if !VerifyChitSignature(sig.NodeId, digest, sig.Signature) {
					logs.Debug("[VerifySignatureSet] Failed: ECDSA signature verification failed for node %s round seq=%d",
						sig.NodeId, round.SeqId)
					return false
				}
			}
		}
	}

	return true
}
