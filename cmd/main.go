package main

import (
	"context"
	"crypto/ecdsa"
	"crypto/elliptic"
	"crypto/rand"
	"crypto/tls"
	"crypto/x509"
	"dex/config"
	"dex/consensus"
	"dex/db"
	"dex/handlers"
	"dex/logs"
	"dex/middleware"
	"dex/sender"
	"dex/txpool"
	"dex/types"
	"dex/utils"
	"encoding/hex"
	"encoding/pem"
	"fmt"
	"math/big"
	"net/http"
	"os"
	"os/signal"
	"sort"
	"strings"
	"sync"
	"syscall"
	"time"

	"github.com/quic-go/quic-go"
	"github.com/quic-go/quic-go/http3"
)

// è¡¨ç¤ºä¸€ä¸ªèŠ‚ç‚¹å®ä¾‹
type NodeInstance struct {
	ID               int
	PrivateKey       string
	Address          string
	Port             string
	DataPath         string
	Server           *http.Server
	ConsensusManager *consensus.ConsensusNodeManager
	DBManager        *db.Manager
	Cancel           context.CancelFunc
	TxPool           *txpool.TxPool
	SenderManager    *sender.SenderManager
	HandlerManager   *handlers.HandlerManager // æ–°å¢
}

// TestValidator ç®€å•çš„äº¤æ˜“éªŒè¯å™¨
type TestValidator struct{}

// åœ¨ cmd/main.go æ–‡ä»¶çš„importåé¢ï¼ŒNodeInstanceç»“æ„ä½“å‰é¢æ·»åŠ ï¼š

// æ¥å£è°ƒç”¨ç»Ÿè®¡ç»“æ„ä½“
type APICallStats struct {
	sync.RWMutex
	// è®°å½•æ¯ä¸ªæ¥å£çš„ç´¯è®¡è°ƒç”¨æ¬¡æ•°
	CallCounts map[string]uint64
	// è®°å½•æ¯ä¸ªèŠ‚ç‚¹æ¯ä¸ªæ¥å£çš„è°ƒç”¨æ¬¡æ•°
	NodeCallCounts map[int]map[string]uint64
}

// å…¨å±€æ¥å£è°ƒç”¨ç»Ÿè®¡
var globalAPIStats = &APICallStats{
	CallCounts:     make(map[string]uint64),
	NodeCallCounts: make(map[int]map[string]uint64),
}

func monitorMetrics(nodes []*NodeInstance) {
	ticker := time.NewTicker(20 * time.Second)
	defer ticker.Stop()

	// ç”¨äºè®°å½•æ¯ä¸ªèŠ‚ç‚¹ä¸Šæ¬¡çš„è°ƒç”¨æ¬¡æ•°ï¼Œè®¡ç®—å¢é‡
	lastCallCounts := make(map[int]map[string]uint64)

	for range ticker.C {
		// ä¸´æ—¶å­˜å‚¨å½“å‰å‘¨æœŸçš„ç»Ÿè®¡æ•°æ®
		currentStats := make(map[string]uint64)
		nodeStats := make(map[int]map[string]uint64)

		for _, node := range nodes {
			if node == nil || node.SenderManager == nil {
				continue
			}

			// 1. å‘é€é˜Ÿåˆ—é•¿åº¦
			sendQueueLen := len(node.SenderManager.SendQueue.TaskChan)

			// 2. æ¯ç›®æ ‡åœ¨é€”è¯·æ±‚
			node.SenderManager.SendQueue.InflightMutex.RLock()
			inflightCopy := make(map[string]int32)
			totalInflight := int32(0)
			for k, v := range node.SenderManager.SendQueue.InflightMap {
				inflightCopy[k] = v
				totalInflight += v
			}
			node.SenderManager.SendQueue.InflightMutex.RUnlock()

			// 3. æ¥æ”¶é˜Ÿåˆ—é•¿åº¦
			recvQueueLen := 0
			if node.ConsensusManager != nil && node.ConsensusManager.Transport != nil {
				// æ ¹æ®ä½ çš„å®é™…Transportç±»å‹è°ƒæ•´
				// if rt, ok := node.ConsensusManager.Transport.(*consensus.ReliableTransport); ok {
				//     recvQueueLen = rt.GetRecvQueueLength()
				// }
			}

			// 4. æ¥å£è°ƒç”¨ç»Ÿè®¡ï¼ˆæ–°å¢ï¼‰
			if node.HandlerManager != nil {
				apiStats := node.HandlerManager.Stats.GetAPICallStats()

				// è®°å½•å½“å‰èŠ‚ç‚¹çš„APIè°ƒç”¨ç»Ÿè®¡
				nodeStats[node.ID] = apiStats

				// è®¡ç®—å¢é‡å¹¶æ›´æ–°å…¨å±€ç»Ÿè®¡
				if lastCallCounts[node.ID] == nil {
					lastCallCounts[node.ID] = make(map[string]uint64)
				}

				for apiName, currentCount := range apiStats {
					// è®¡ç®—è¿™ä¸ªå‘¨æœŸçš„å¢é‡
					delta := currentCount
					if lastCount, exists := lastCallCounts[node.ID][apiName]; exists {
						delta = currentCount - lastCount
					}

					// æ›´æ–°å…¨å±€ç»Ÿè®¡
					currentStats[apiName] += delta

					// æ›´æ–°ä¸Šæ¬¡è®°å½•
					lastCallCounts[node.ID][apiName] = currentCount
				}
			}

			// æ‰“å°èŠ‚ç‚¹æŒ‡æ ‡
			if sendQueueLen > 0 || totalInflight > 0 || recvQueueLen > 0 {
				fmt.Printf("[Metrics] Node %d: SendQ=%d, Inflight=%d, RecvQ=%d\n",
					node.ID, sendQueueLen, totalInflight, recvQueueLen)
			}
		}

		// æ›´æ–°å…¨å±€APIè°ƒç”¨ç»Ÿè®¡
		globalAPIStats.Lock()
		for apiName, delta := range currentStats {
			globalAPIStats.CallCounts[apiName] += delta
		}
		for nodeID, apis := range nodeStats {
			globalAPIStats.NodeCallCounts[nodeID] = apis
		}
		globalAPIStats.Unlock()

		printAPICallStatistics()
	}
}

// æ‰“å°APIè°ƒç”¨ç»Ÿè®¡
func printAPICallStatistics() {
	globalAPIStats.RLock()
	defer globalAPIStats.RUnlock()

	if len(globalAPIStats.CallCounts) == 0 {
		return
	}

	fmt.Println("\n========== API Call Statistics ==========")
	fmt.Println("Global API Call Counts:")

	// æŒ‰æ¥å£åç§°æ’åº
	var apiNames []string
	for apiName := range globalAPIStats.CallCounts {
		apiNames = append(apiNames, apiName)
	}
	sort.Strings(apiNames)

	// æ‰“å°å…¨å±€ç»Ÿè®¡
	totalCalls := uint64(0)
	for _, apiName := range apiNames {
		count := globalAPIStats.CallCounts[apiName]
		totalCalls += count
		fmt.Printf("  %-30s: %10d calls\n", apiName, count)
	}
	fmt.Printf("  %-30s: %10d calls\n", "TOTAL", totalCalls)

	// æ‰“å°æ¯ä¸ªèŠ‚ç‚¹çš„ç»Ÿè®¡ï¼ˆå¯é€‰ï¼‰
	if len(globalAPIStats.NodeCallCounts) > 0 {
		//fmt.Println("\nPer-Node API Call Distribution:")

		// æŒ‰èŠ‚ç‚¹IDæ’åº
		var nodeIDs []int
		for nodeID := range globalAPIStats.NodeCallCounts {
			nodeIDs = append(nodeIDs, nodeID)
		}
		sort.Ints(nodeIDs)

		for _, nodeID := range nodeIDs {
			apis := globalAPIStats.NodeCallCounts[nodeID]
			if len(apis) == 0 {
				continue
			}

			nodeTotalCalls := uint64(0)
			//fmt.Printf("\n  Node %d:\n", nodeID)

			// æŒ‰æ¥å£åç§°æ’åº
			var nodeAPINames []string
			for apiName := range apis {
				nodeAPINames = append(nodeAPINames, apiName)
			}
			sort.Strings(nodeAPINames)

			for _, apiName := range nodeAPINames {
				count := apis[apiName]
				nodeTotalCalls += count
				//fmt.Printf("    %-28s: %8d\n", apiName, count)
			}
			//fmt.Printf("    %-28s: %8d\n", "Node Total", nodeTotalCalls)
		}
	}

	// æ‰“å°APIè°ƒç”¨é¢‘ç‡åˆ†æ
	fmt.Println("\nAPI Call Frequency Analysis:")
	if totalCalls > 0 {
		for _, apiName := range apiNames {
			count := globalAPIStats.CallCounts[apiName]
			percentage := float64(count) * 100.0 / float64(totalCalls)

			// åˆ›å»ºä¸€ä¸ªç®€å•çš„æ¡å½¢å›¾
			barLength := int(percentage / 2)
			if barLength > 40 {
				barLength = 40
			}
			bar := strings.Repeat("â–ˆ", barLength)

			fmt.Printf("  %-25s: %6.2f%% %s\n", apiName, percentage, bar)
		}
	}

	fmt.Println("==========================================\n")
}
func (v *TestValidator) CheckAnyTx(tx *db.AnyTx) error {
	if tx == nil {
		return fmt.Errorf("nil transaction")
	}
	base := tx.GetBase()
	if base == nil {
		return fmt.Errorf("missing base message")
	}
	if base.TxId == "" {
		return fmt.Errorf("empty tx id")
	}
	return nil
}
func main() {
	// åŠ è½½é…ç½®
	cfg := config.DefaultConfig()
	// é…ç½®å‚æ•°
	numNodes := cfg.Network.DefaultNumNodes
	basePort := cfg.Network.BasePort

	fmt.Printf("ğŸš€ Starting %d real consensus nodes...\n", numNodes)

	// ç”ŸæˆèŠ‚ç‚¹ç§é’¥
	privateKeys := generatePrivateKeys(numNodes)

	// åˆ›å»ºæ‰€æœ‰èŠ‚ç‚¹å®ä¾‹
	nodes := make([]*NodeInstance, numNodes)
	var wg sync.WaitGroup

	// ç¬¬ä¸€é˜¶æ®µï¼šåˆå§‹åŒ–æ‰€æœ‰èŠ‚ç‚¹ï¼ˆåˆ›å»ºæ•°æ®åº“å’ŒåŸºç¡€è®¾æ–½ï¼‰
	fmt.Println("ğŸ“¦ Phase 1: Initializing all nodes...")
	for i := 0; i < numNodes; i++ {
		node := &NodeInstance{
			Address:    fmt.Sprintf("0x000%d", i),
			ID:         i,
			PrivateKey: privateKeys[i],
			Port:       fmt.Sprintf("%d", basePort+i),
			DataPath:   fmt.Sprintf("./data/data_node_%d", i),
		}

		// æ¸…ç†æ—§æ•°æ®
		os.RemoveAll(node.DataPath)

		// åˆå§‹åŒ–èŠ‚ç‚¹
		if err := initializeNode(node); err != nil {
			logs.Error("Failed to initialize node %d: %v", i, err)
			continue
		}

		nodes[i] = node
		fmt.Printf("  âœ” Node %d initialized (port %s)\n", i, node.Port)
	}

	// ç­‰å¾…ä¸€ä¸‹è®©æ‰€æœ‰æ•°æ®åº“å®Œæˆåˆå§‹åŒ–
	time.Sleep(2 * time.Second)

	// ç¬¬äºŒé˜¶æ®µï¼šæ³¨å†Œæ‰€æœ‰èŠ‚ç‚¹åˆ°æ•°æ®åº“ï¼ˆè®©èŠ‚ç‚¹äº’ç›¸çŸ¥é“ï¼‰
	fmt.Println("ğŸ”— Phase 2: Registering all nodes...")
	registerAllNodes(nodes)

	// ç¬¬ä¸‰é˜¶æ®µï¼šå¯åŠ¨æ‰€æœ‰HTTPæœåŠ¡å™¨
	fmt.Println("ğŸŒ Phase 3: Starting HTTP servers...")

	// åˆ›å»ºä¸€ä¸ªchannelæ¥æ”¶é›†æœåŠ¡å™¨å¯åŠ¨å®Œæˆçš„ä¿¡å·
	serverReadyChan := make(chan int, numNodes)
	serverErrorChan := make(chan error, numNodes)

	for _, node := range nodes {
		if node == nil {
			continue
		}
		wg.Add(1)
		go func(n *NodeInstance) {
			defer wg.Done()

			// å¯åŠ¨HTTPæœåŠ¡å™¨å¹¶å‘é€å°±ç»ªä¿¡å·
			if err := startHTTPServerWithSignal(n, serverReadyChan, serverErrorChan); err != nil {
				serverErrorChan <- fmt.Errorf("node %d failed to start: %v", n.ID, err)
			}
		}(node)

		// ç¨å¾®é”™å¼€å¯åŠ¨æ—¶é—´ï¼Œé¿å…èµ„æºäº‰æŠ¢
		time.Sleep(50 * time.Millisecond)
	}

	// ç­‰å¾…æ‰€æœ‰HTTPæœåŠ¡å™¨å¯åŠ¨å®Œæˆ
	fmt.Println("â³ Waiting for all HTTP/3 servers to be ready...")
	readyCount := 0
	successfulNodes := 0

	// è®¾ç½®è¶…æ—¶æ—¶é—´
	timeout := time.After(30 * time.Second)

	for readyCount < len(nodes) {
		select {
		case nodeID := <-serverReadyChan:
			successfulNodes++
			fmt.Printf("  âœ… Node %d HTTP/3 server is ready (%d/%d)\n",
				nodeID, successfulNodes, len(nodes))

		case err := <-serverErrorChan:
			fmt.Printf("  âŒ Error: %v\n", err)

		case <-timeout:
			fmt.Printf("  âš ï¸ Timeout waiting for servers. %d/%d started successfully\n",
				successfulNodes, len(nodes))
			goto ContinueWithConsensus
		}

		readyCount++
	}

	fmt.Printf("âœ… All %d HTTP/3 servers are ready!\n", successfulNodes)

ContinueWithConsensus:
	// é¢å¤–ç­‰å¾…ä¸€å°æ®µæ—¶é—´ç¡®ä¿æœåŠ¡å™¨å®Œå…¨ç¨³å®š
	time.Sleep(1 * time.Second)
	// æ–°å¢ï¼šç¬¬3.5é˜¶æ®µ - å¯åŠ¨å…±è¯†å¼•æ“
	fmt.Println("ğŸ”§ Phase 3.5: Starting consensus engines...")
	for _, node := range nodes {
		if node != nil && node.ConsensusManager != nil {
			node.ConsensusManager.Start()
			fmt.Printf("  âœ“ Node %d consensus engine started\n", node.ID)
		}
	}
	// Create initial transactions
	fmt.Println("ğŸ“ Creating initial transactions...")
	for _, node := range nodes {
		if node != nil && node.ConsensusManager != nil {
			generateTransactions(node)
		}
	}
	time.Sleep(2 * time.Second)

	// ç¬¬å››é˜¶æ®µï¼šå¯åŠ¨å…±è¯†
	fmt.Println("ğŸ¯ Phase 4: Starting consensus engines...")
	for _, node := range nodes {
		if node != nil && node.ConsensusManager != nil {
			// è§¦å‘åˆå§‹æŸ¥è¯¢
			go func(n *NodeInstance) {
				time.Sleep(time.Duration(n.ID*100) * time.Millisecond) // é”™å¼€å¯åŠ¨
				n.ConsensusManager.StartQuery()
			}(node)
		}
	}
	// å¯åŠ¨æŒ‡æ ‡ç›‘æ§
	go monitorMetrics(nodes)
	// ç›‘æ§è¿›åº¦
	go monitorProgress(nodes)

	// ç­‰å¾…ä¿¡å·é€€å‡º
	sigChan := make(chan os.Signal, 1)
	signal.Notify(sigChan, syscall.SIGINT, syscall.SIGTERM)

	fmt.Println("\nâœ… All nodes started! Press Ctrl+C to stop...")
	fmt.Println("ğŸ“Š Monitoring consensus progress...")

	<-sigChan

	// ä¼˜é›…å…³é—­
	fmt.Println("\nğŸ›‘ Shutting down all nodes...")
	shutdownAllNodes(nodes)

	wg.Wait()
	fmt.Println("ğŸ‘‹ All nodes stopped. Goodbye!")
}

// æ–°å¢ï¼šå¸¦ä¿¡å·çš„HTTPæœåŠ¡å™¨å¯åŠ¨å‡½æ•°
func startHTTPServerWithSignal(node *NodeInstance, readyChan chan<- int, errorChan chan<- error) error {
	// åˆ›å»ºHTTPè·¯ç”±
	mux := http.NewServeMux()

	// ä½¿ç”¨HandlerManageræ³¨å†Œè·¯ç”±
	node.HandlerManager.RegisterRoutes(mux)

	// åº”ç”¨ä¸­é—´ä»¶
	handler := middleware.RateLimit(mux)

	// ç”Ÿæˆè‡ªç­¾åè¯ä¹¦
	certFile := fmt.Sprintf("server_%d.crt", node.ID)
	keyFile := fmt.Sprintf("server_%d.key", node.ID)

	if err := generateSelfSignedCert(certFile, keyFile); err != nil {
		errorChan <- fmt.Errorf("Node %d: Failed to generate certificate: %v", node.ID, err)
		return err
	}

	// åˆ›å»ºTLSé…ç½®
	tlsConfig := &tls.Config{
		Certificates: []tls.Certificate{},
		MinVersion:   tls.VersionTLS13,
		MaxVersion:   tls.VersionTLS13,
		// æ·»åŠ ALPNåè®®æ”¯æŒ - è¿™æ˜¯å…³é”®ä¿®å¤
		NextProtos: []string{"h3", "h3-29", "h3-28", "h3-27"}, // HTTP/3åè®®æ ‡è¯†ç¬¦
	}

	cert, err := tls.LoadX509KeyPair(certFile, keyFile)
	if err != nil {
		errorChan <- fmt.Errorf("Node %d: Failed to load certificate: %v", node.ID, err)
		return err
	}
	tlsConfig.Certificates = append(tlsConfig.Certificates, cert)

	// åˆ›å»ºQUICé…ç½®
	quicConfig := &quic.Config{
		KeepAlivePeriod: 10 * time.Second,
		MaxIdleTimeout:  5 * time.Minute,
		Allow0RTT:       true,
	}

	// åˆ›å»ºHTTP/3æœåŠ¡å™¨
	server := &http3.Server{
		Addr:       ":" + node.Port,
		Handler:    handler,
		TLSConfig:  tlsConfig,
		QUICConfig: quicConfig,
	}

	node.Server = &http.Server{
		Addr:    ":" + node.Port,
		Handler: handler,
	}

	// åˆ›å»ºQUICç›‘å¬å™¨
	listener, err := quic.ListenAddr(":"+node.Port, tlsConfig, quicConfig)
	if err != nil {
		errorChan <- fmt.Errorf("Node %d: Failed to create QUIC listener: %v", node.ID, err)
		return err
	}

	logs.Info("Node %d: Starting HTTP/3 server on port %s", node.ID, node.Port)

	// æœåŠ¡å™¨æˆåŠŸåˆ›å»ºç›‘å¬å™¨ï¼Œå‘é€å°±ç»ªä¿¡å·
	readyChan <- node.ID

	// å¯åŠ¨æœåŠ¡å™¨ï¼ˆè¿™æ˜¯é˜»å¡è°ƒç”¨ï¼‰
	if err := server.ServeListener(listener); err != nil {
		logs.Error("Node %d: HTTP/3 Server error: %v", node.ID, err)
		return err
	}

	return nil
}

// generatePrivateKeys ç”ŸæˆæŒ‡å®šæ•°é‡çš„ç§é’¥
func generatePrivateKeys(count int) []string {
	keys := make([]string, count)
	for i := 0; i < count; i++ {
		priv, err := ecdsa.GenerateKey(elliptic.P256(), rand.Reader)
		if err != nil {
			logs.Error("Failed to generate key %d: %v", i, err)
			continue
		}

		// è½¬æ¢ä¸ºhexæ ¼å¼
		privBytes := priv.D.Bytes()
		keys[i] = hex.EncodeToString(privBytes)
	}
	return keys
}

// åˆå§‹åŒ–å•ä¸ªèŠ‚ç‚¹
func initializeNode(node *NodeInstance) error {
	// 1. åˆå§‹åŒ–å¯†é’¥ç®¡ç†å™¨
	keyMgr := utils.GetKeyManager()
	if err := keyMgr.InitKey(node.PrivateKey); err != nil {
		return fmt.Errorf("failed to init key: %v", err)
	}
	node.Address = keyMgr.GetAddress()

	// 2. è®¾ç½®ç¯å¢ƒå˜é‡ï¼ˆæŸäº›æ¨¡å—å¯èƒ½éœ€è¦ï¼‰
	utils.Port = node.Port

	// 3. åˆå§‹åŒ–æ•°æ®åº“
	dbManager, err := db.NewManager(node.DataPath)
	if err != nil {
		return fmt.Errorf("failed to init db: %v", err)
	}
	node.DBManager = dbManager

	// åˆå§‹åŒ–æ•°æ®åº“å†™é˜Ÿåˆ—
	dbManager.InitWriteQueue(100, 200*time.Millisecond)

	// 4. åˆ›å»ºéªŒè¯å™¨
	validator := &TestValidator{}

	// 5. åˆ›å»ºå¹¶å¯åŠ¨TxPoolï¼ˆä¸å†ä½¿ç”¨å•ä¾‹ï¼‰
	txPool, err := txpool.NewTxPool(dbManager, validator)
	if err != nil {
		return fmt.Errorf("failed to create TxPool: %v", err)
	}
	if err := txPool.Start(); err != nil {
		return fmt.Errorf("failed to start TxPool: %v", err)
	}
	node.TxPool = txPool

	// 6. åˆ›å»ºå‘é€ç®¡ç†å™¨
	senderManager := sender.NewSenderManager(dbManager, node.Address, txPool, node.ID)
	node.SenderManager = senderManager

	// 7. åˆå§‹åŒ–å…±è¯†ç³»ç»Ÿ
	nodeID := types.NodeID(node.Address)
	config := consensus.DefaultConfig()

	// è°ƒæ•´é…ç½®
	config.Network.NumNodes = 100
	config.Network.NumByzantineNodes = 0
	config.Consensus.NumHeights = 10     // è¿è¡Œ10ä¸ªé«˜åº¦
	config.Consensus.BlocksPerHeight = 3 // æ¯ä¸ªé«˜åº¦3ä¸ªå€™é€‰å—
	config.Consensus.K = 10              // é‡‡æ ·10ä¸ªèŠ‚ç‚¹
	config.Consensus.Alpha = 7           // éœ€è¦7ä¸ªå›åº”
	config.Consensus.Beta = 5            // 5æ¬¡è¿ç»­æŠ•ç¥¨ç¡®è®¤
	config.Node.ProposalInterval = 5 * time.Second

	// åˆ›å»ºå…±è¯†ç®¡ç†å™¨
	consensusManager := consensus.InitConsensusManager(
		nodeID,
		dbManager,
		config,
		senderManager,
		txPool,
	)
	node.ConsensusManager = consensusManager

	// 8. åˆ›å»ºHandlerç®¡ç†å™¨
	handlerManager := handlers.NewHandlerManager(
		dbManager,
		consensusManager,
		node.Port,
		node.Address,
		senderManager,
		txPool,
	)
	node.HandlerManager = handlerManager

	// ä¿å­˜èŠ‚ç‚¹ä¿¡æ¯åˆ°æ•°æ®åº“
	nodeInfo := &db.NodeInfo{
		PublicKey: keyMgr.GetPublicKey(),
		Ip:        fmt.Sprintf("127.0.0.1:%s", node.Port),
		IsOnline:  true,
	}

	if err := dbManager.SaveNodeInfo(nodeInfo); err != nil {
		return fmt.Errorf("failed to save node info: %v", err)
	}

	// åˆ›å»ºè´¦æˆ·
	account := &db.Account{
		Address:   node.Address,
		PublicKey: keyMgr.GetPublicKey(),
		Ip:        fmt.Sprintf("127.0.0.1:%s", node.Port),
		Index:     uint64(node.ID),
		IsMiner:   true,
		Balances:  make(map[string]*db.TokenBalance),
	}

	// åˆå§‹åŒ–FBä»£å¸ä½™é¢
	account.Balances["FB"] = &db.TokenBalance{
		Balance:            "1000000",
		MinerLockedBalance: "100000",
	}

	if err := dbManager.SaveAccount(account); err != nil {
		return fmt.Errorf("failed to save account: %v", err)
	}
	// ä¿å­˜ç´¢å¼•æ˜ å°„
	indexKey := db.KeyIndexToAccount(account.Index)
	accountKey := fmt.Sprintf("account_%s", account.Address)
	dbManager.EnqueueSet(indexKey, accountKey)
	// Force flush to ensure miner registration is persisted
	dbManager.ForceFlush()
	return nil
}

// Option 2: Generate transactions continuously
func generateTransactions(node *NodeInstance) {
	go func() {
		ticker := time.NewTicker(200 * time.Millisecond)
		defer ticker.Stop()

		txID := 0
		for range ticker.C {
			// Generate multiple transactions
			for i := 0; i < 2; i++ {
				tx := &db.Transaction{
					Base: &db.BaseMessage{
						TxId:        fmt.Sprintf("%d%d", i, time.Now().UnixNano()),
						FromAddress: node.Address,
						Status:      db.Status_PENDING,
						Nonce:       uint64(i),
					},
					To:           node.Address,
					TokenAddress: "FB",
					Amount:       "100",
				}
				anyTx := &db.AnyTx{
					Content: &db.AnyTx_Transaction{Transaction: tx},
				}
				// Add to transaction pool
				if err := node.TxPool.StoreAnyTx(anyTx); err == nil {
					logs.Trace("Added transaction %s to pool", tx.Base.TxId)
				}
			}
			txID++
		}
	}()
}

// æ³¨å†Œæ‰€æœ‰èŠ‚ç‚¹ä¿¡æ¯åˆ°æ¯ä¸ªèŠ‚ç‚¹çš„æ•°æ®åº“
func registerAllNodes(nodes []*NodeInstance) {
	for i, node := range nodes {
		if node == nil || node.DBManager == nil {
			continue
		}

		// åœ¨å½“å‰èŠ‚ç‚¹çš„æ•°æ®åº“ä¸­æ³¨å†Œæ‰€æœ‰å…¶ä»–èŠ‚ç‚¹
		for j, otherNode := range nodes {
			if otherNode == nil || i == j {
				continue
			}

			// ä¿å­˜å…¶ä»–èŠ‚ç‚¹çš„è´¦æˆ·ä¿¡æ¯
			account := &db.Account{
				Address:   otherNode.Address,
				PublicKey: utils.GetKeyManager().GetPublicKey(), // è¿™é‡Œç®€åŒ–å¤„ç†
				Ip:        fmt.Sprintf("127.0.0.1:%s", otherNode.Port),
				Index:     uint64(j),
				IsMiner:   true,
				Balances:  make(map[string]*db.TokenBalance),
			}

			account.Balances["FB"] = &db.TokenBalance{
				Balance:            "1000000",
				MinerLockedBalance: "100000",
			}

			node.DBManager.SaveAccount(account)

			// ä¿å­˜èŠ‚ç‚¹ä¿¡æ¯
			nodeInfo := &db.NodeInfo{
				PublicKey: fmt.Sprintf("node_%d_pub", j),
				Ip:        fmt.Sprintf("127.0.0.1:%s", otherNode.Port),
				IsOnline:  true,
			}
			node.DBManager.SaveNodeInfo(nodeInfo)
			// ä¿å­˜ç´¢å¼•æ˜ å°„
			indexKey := fmt.Sprintf(db.KeyIndexToAccount(uint64(j)))
			accountKey := fmt.Sprintf("account_%s", otherNode.Address)
			node.DBManager.EnqueueSet(indexKey, accountKey)

		}
		// Force flush to ensure all registrations are persisted
		node.DBManager.ForceFlush()
		time.Sleep(100 * time.Millisecond) // ç¡®ä¿å†™å…¥å®Œæˆ

		// é‡æ–°æ‰«ææ•°æ®åº“é‡å»º bitmap
		if err := node.DBManager.IndexMgr.RebuildBitmapFromDB(); err != nil {
			logs.Error("Failed to rebuild bitmap: %v", err)
		}
	}
}

// ç”Ÿæˆè‡ªç­¾åè¯ä¹¦
func generateSelfSignedCert(certFile, keyFile string) error {
	priv, err := ecdsa.GenerateKey(elliptic.P256(), rand.Reader)
	if err != nil {
		return err
	}

	template := x509.Certificate{
		SerialNumber: big.NewInt(1),
		DNSNames:     []string{"localhost"},
	}

	certDER, err := x509.CreateCertificate(
		rand.Reader,
		&template,
		&template,
		&priv.PublicKey,
		priv,
	)
	if err != nil {
		return err
	}

	// ä¿å­˜è¯ä¹¦
	certOut, err := os.Create(certFile)
	if err != nil {
		return err
	}
	defer certOut.Close()

	pem.Encode(certOut, &pem.Block{Type: "CERTIFICATE", Bytes: certDER})

	// ä¿å­˜ç§é’¥
	keyOut, err := os.Create(keyFile)
	if err != nil {
		return err
	}
	defer keyOut.Close()

	privBytes, err := x509.MarshalECPrivateKey(priv)
	if err != nil {
		return err
	}

	pem.Encode(keyOut, &pem.Block{Type: "EC PRIVATE KEY", Bytes: privBytes})

	return nil
}

// ç›‘æ§å…±è¯†è¿›åº¦
func monitorProgress(nodes []*NodeInstance) {
	ticker := time.NewTicker(20 * time.Second)
	defer ticker.Stop()

	for range ticker.C {
		fmt.Println("\n========== Progress Monitor ==========")
		fmt.Printf("Time: %s\n", time.Now().Format("15:04:05"))

		var minHeight, maxHeight uint64
		activeNodes := 0

		for _, node := range nodes {
			if node == nil || node.ConsensusManager == nil {
				continue
			}
			activeNodes++
			_, height := node.ConsensusManager.GetLastAccepted()
			if minHeight == 0 || height < minHeight {
				minHeight = height
			}
			if height > maxHeight {
				maxHeight = height
			}
		}

		fmt.Printf("\nğŸ“ˆ Progress: Active=%d, MinHeight=%d, MaxHeight=%d\n",
			activeNodes, minHeight, maxHeight)

		// æ‰“å°æ¯ä¸ªèŠ‚ç‚¹çš„å®Œæ•´ç»Ÿè®¡ä¿¡æ¯
		fmt.Println("\nNode Statistics:")
		for i, node := range nodes {
			if node == nil || node.ConsensusManager == nil {
				fmt.Printf("Node %d: inactive\n", i)
				continue
			}

			stats := node.ConsensusManager.GetStats()
			if stats == nil {
				fmt.Printf("Node %d: no stats\n", i)
				continue
			}

			lastAccepted, height := node.ConsensusManager.GetLastAccepted()

			// è·å–æ‰€æœ‰ç»Ÿè®¡æ•°æ®
			stats.Mu.Lock()
			fmt.Printf("\nNode %d:\n", i)
			fmt.Printf("  Last Accepted: %s (height=%d)\n", lastAccepted, height)
			fmt.Printf("  Queries Sent: %d\n", stats.QueriesSent)
			fmt.Printf("  Queries Received: %d\n", stats.QueriesReceived)
			fmt.Printf("  Chits Responded: %d\n", stats.ChitsResponded)
			fmt.Printf("  Blocks Proposed: %d\n", stats.BlocksProposed)
			fmt.Printf("  Gossips Received: %d\n", stats.GossipsReceived)
			fmt.Printf("  Snapshots Used: %d\n", stats.SnapshotsUsed)
			fmt.Printf("  Snapshots Served: %d\n", stats.SnapshotsServed)
			fmt.Printf("  GetPreferenceSwitchHistory: %+v\n", stats.GetPreferenceSwitchHistory())
			fmt.Printf("  Consensus API handler: %+v\n", node.HandlerManager.Stats.GetAPICallStats())
			rt, ok := node.ConsensusManager.Transport.(*consensus.RealTransport) // å®‰å…¨æ–­è¨€
			if !ok {
				return
			}
			if rt == nil {
				return
			}
			fmt.Printf("  Consensus API send: %+v\n", rt.Stats.GetAPICallStats())
			// æ˜¾ç¤ºæ¯ä¸ªé«˜åº¦çš„æŸ¥è¯¢æ•°
			if len(stats.QueriesPerHeight) > 0 {
				fmt.Printf("  Queries Per Height:\n")
				for h, count := range stats.QueriesPerHeight {
					fmt.Printf("    Height %d: %d\n", h, count)
				}
			}
			stats.Mu.Unlock()
		}

		fmt.Println()
	}
}

// å…³é—­æ‰€æœ‰èŠ‚ç‚¹
func shutdownAllNodes(nodes []*NodeInstance) {
	var wg sync.WaitGroup

	for _, node := range nodes {
		if node == nil {
			continue
		}

		wg.Add(1)
		go func(n *NodeInstance) {
			defer wg.Done()

			// åœæ­¢å…±è¯†
			if n.ConsensusManager != nil {
				n.ConsensusManager.Stop()
			}

			// åœæ­¢Handlerç®¡ç†å™¨ï¼ˆæ–°å¢ï¼‰
			if n.HandlerManager != nil {
				n.HandlerManager.Stop()
			}

			// åœæ­¢Senderç®¡ç†å™¨
			if n.SenderManager != nil {
				n.SenderManager.Stop()
			}

			// åœæ­¢TxPool
			if n.TxPool != nil {
				n.TxPool.Stop()
			}

			// åœæ­¢HTTPæœåŠ¡å™¨
			if n.Server != nil {
				ctx, cancel := context.WithTimeout(context.Background(), 5*time.Second)
				defer cancel()
				n.Server.Shutdown(ctx)
			}

			// å…³é—­æ•°æ®åº“
			if n.DBManager != nil {
				n.DBManager.Close()
			}

			fmt.Printf("  âœ“ Node %d stopped\n", n.ID)
		}(node)
	}

	wg.Wait()
}
