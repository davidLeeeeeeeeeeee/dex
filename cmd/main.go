package main

import (
	"context"
	"crypto/ecdsa"
	"crypto/elliptic"
	"crypto/rand"
	"crypto/x509"
	"dex/consensus"
	"dex/db"
	"dex/handlers"
	"dex/logs"
	"dex/middleware"
	"dex/sender"
	"dex/txpool"
	"dex/types"
	"dex/utils"
	"encoding/hex"
	"encoding/pem"
	"fmt"
	"math/big"
	"net/http"
	"os"
	"os/signal"
	"sync"
	"syscall"
	"time"
)

// NodeInstance è¡¨ç¤ºä¸€ä¸ªèŠ‚ç‚¹å®ä¾‹
type NodeInstance struct {
	ID               int
	PrivateKey       string
	Address          string
	Port             string
	DataPath         string
	Server           *http.Server
	ConsensusManager *consensus.ConsensusNodeManager
	DBManager        *db.Manager
	TxPoolQueue      *txpool.TxPoolQueue
	SenderQueue      *sender.SendQueue
	Cancel           context.CancelFunc
}

// TestValidator ç®€å•çš„äº¤æ˜“éªŒè¯å™¨
type TestValidator struct{}

func (v *TestValidator) CheckAnyTx(tx *db.AnyTx) error {
	if tx == nil {
		return fmt.Errorf("nil transaction")
	}
	base := tx.GetBase()
	if base == nil {
		return fmt.Errorf("missing base message")
	}
	if base.TxId == "" {
		return fmt.Errorf("empty tx id")
	}
	return nil
}

func main() {
	// é…ç½®å‚æ•°
	const numNodes = 100
	basePort := 6000

	fmt.Printf("ğŸš€ Starting %d real consensus nodes...\n", numNodes)

	// ç”ŸæˆèŠ‚ç‚¹ç§é’¥
	privateKeys := generatePrivateKeys(numNodes)

	// åˆ›å»ºæ‰€æœ‰èŠ‚ç‚¹å®ä¾‹
	nodes := make([]*NodeInstance, numNodes)
	var wg sync.WaitGroup

	// ç¬¬ä¸€é˜¶æ®µï¼šåˆå§‹åŒ–æ‰€æœ‰èŠ‚ç‚¹ï¼ˆåˆ›å»ºæ•°æ®åº“å’ŒåŸºç¡€è®¾æ–½ï¼‰
	fmt.Println("ğŸ“¦ Phase 1: Initializing all nodes...")
	for i := 0; i < numNodes; i++ {
		node := &NodeInstance{
			ID:         i,
			PrivateKey: privateKeys[i],
			Port:       fmt.Sprintf("%d", basePort+i),
			DataPath:   fmt.Sprintf("./data_node_%d", i),
		}

		// æ¸…ç†æ—§æ•°æ®
		os.RemoveAll(node.DataPath)

		// åˆå§‹åŒ–èŠ‚ç‚¹
		if err := initializeNode(node); err != nil {
			logs.Error("Failed to initialize node %d: %v", i, err)
			continue
		}

		nodes[i] = node
		fmt.Printf("  âœ“ Node %d initialized (port %s)\n", i, node.Port)
	}

	// ç­‰å¾…ä¸€ä¸‹è®©æ‰€æœ‰æ•°æ®åº“å®Œæˆåˆå§‹åŒ–
	time.Sleep(2 * time.Second)

	// ç¬¬äºŒé˜¶æ®µï¼šæ³¨å†Œæ‰€æœ‰èŠ‚ç‚¹åˆ°æ•°æ®åº“ï¼ˆè®©èŠ‚ç‚¹äº’ç›¸çŸ¥é“ï¼‰
	fmt.Println("ğŸ”— Phase 2: Registering all nodes...")
	registerAllNodes(nodes)

	// ç¬¬ä¸‰é˜¶æ®µï¼šå¯åŠ¨æ‰€æœ‰HTTPæœåŠ¡å™¨
	fmt.Println("ğŸŒ Phase 3: Starting HTTP servers...")
	for _, node := range nodes {
		if node == nil {
			continue
		}
		wg.Add(1)
		go func(n *NodeInstance) {
			defer wg.Done()
			startHTTPServer(n)
		}(node)
		time.Sleep(50 * time.Millisecond) // é¿å…åŒæ—¶å¯åŠ¨å¤ªå¤š
	}

	// ç­‰å¾…æœåŠ¡å™¨å¯åŠ¨
	time.Sleep(5 * time.Second)

	// ç¬¬å››é˜¶æ®µï¼šå¯åŠ¨å…±è¯†
	fmt.Println("ğŸ¯ Phase 4: Starting consensus engines...")
	for _, node := range nodes {
		if node != nil && node.ConsensusManager != nil {
			// è§¦å‘åˆå§‹æŸ¥è¯¢
			go func(n *NodeInstance) {
				time.Sleep(time.Duration(n.ID*100) * time.Millisecond) // é”™å¼€å¯åŠ¨
				n.ConsensusManager.StartQuery()
			}(node)
		}
	}

	// ç›‘æ§è¿›åº¦
	go monitorProgress(nodes)

	// ç­‰å¾…ä¿¡å·é€€å‡º
	sigChan := make(chan os.Signal, 1)
	signal.Notify(sigChan, syscall.SIGINT, syscall.SIGTERM)

	fmt.Println("\nâœ… All nodes started! Press Ctrl+C to stop...")
	fmt.Println("ğŸ“Š Monitoring consensus progress...")

	<-sigChan

	// ä¼˜é›…å…³é—­
	fmt.Println("\nğŸ›‘ Shutting down all nodes...")
	shutdownAllNodes(nodes)

	wg.Wait()
	fmt.Println("ğŸ‘‹ All nodes stopped. Goodbye!")
}

// generatePrivateKeys ç”ŸæˆæŒ‡å®šæ•°é‡çš„ç§é’¥
func generatePrivateKeys(count int) []string {
	keys := make([]string, count)
	for i := 0; i < count; i++ {
		priv, err := ecdsa.GenerateKey(elliptic.P256(), rand.Reader)
		if err != nil {
			logs.Error("Failed to generate key %d: %v", i, err)
			continue
		}

		// è½¬æ¢ä¸ºhexæ ¼å¼
		privBytes := priv.D.Bytes()
		keys[i] = hex.EncodeToString(privBytes)
	}
	return keys
}

// initializeNode åˆå§‹åŒ–å•ä¸ªèŠ‚ç‚¹
func initializeNode(node *NodeInstance) error {
	// 1. åˆå§‹åŒ–å¯†é’¥ç®¡ç†å™¨
	keyMgr := utils.GetKeyManager()
	if err := keyMgr.InitKey(node.PrivateKey); err != nil {
		return fmt.Errorf("failed to init key: %v", err)
	}
	node.Address = keyMgr.GetAddress()

	// 2. è®¾ç½®ç¯å¢ƒå˜é‡ï¼ˆæŸäº›æ¨¡å—å¯èƒ½éœ€è¦ï¼‰
	utils.Port = node.Port

	// 3. åˆå§‹åŒ–æ•°æ®åº“
	dbManager, err := db.NewManager(node.DataPath)
	if err != nil {
		return fmt.Errorf("failed to init db: %v", err)
	}
	node.DBManager = dbManager

	// åˆå§‹åŒ–æ•°æ®åº“å†™é˜Ÿåˆ—
	dbManager.InitWriteQueue(100, 200*time.Millisecond)

	// 4. åˆå§‹åŒ–TxPoolé˜Ÿåˆ—
	validator := &TestValidator{}
	txpool.InitTxPoolQueue(dbManager, validator)

	// 5. åˆå§‹åŒ–å‘é€é˜Ÿåˆ—
	sender.InitQueue(10, 10000)

	// 6. åˆå§‹åŒ–å…±è¯†ç³»ç»Ÿ
	nodeID := types.NodeID(fmt.Sprintf("%d", node.ID))
	config := consensus.DefaultConfig()

	// è°ƒæ•´é…ç½®
	config.Network.NumNodes = 100
	config.Network.NumByzantineNodes = 0
	config.Consensus.NumHeights = 10     // è¿è¡Œ10ä¸ªé«˜åº¦
	config.Consensus.BlocksPerHeight = 3 // æ¯ä¸ªé«˜åº¦3ä¸ªå€™é€‰å—
	config.Consensus.K = 10              // é‡‡æ ·10ä¸ªèŠ‚ç‚¹
	config.Consensus.Alpha = 7           // éœ€è¦7ä¸ªå›åº”
	config.Consensus.Beta = 5            // 5æ¬¡è¿ç»­æŠ•ç¥¨ç¡®è®¤
	config.Node.ProposalInterval = 5 * time.Second

	// åˆ›å»ºå…±è¯†ç®¡ç†å™¨
	consensusManager := consensus.InitConsensusManager(nodeID, dbManager, config)
	node.ConsensusManager = consensusManager

	// ä¿å­˜èŠ‚ç‚¹ä¿¡æ¯åˆ°æ•°æ®åº“
	nodeInfo := &db.NodeInfo{
		PublicKey: keyMgr.GetPublicKey(),
		Ip:        fmt.Sprintf("127.0.0.1:%s", node.Port),
		IsOnline:  true,
	}

	if err := db.SaveNodeInfo(dbManager, nodeInfo); err != nil {
		return fmt.Errorf("failed to save node info: %v", err)
	}

	// åˆ›å»ºè´¦æˆ·
	account := &db.Account{
		Address:   node.Address,
		PublicKey: keyMgr.GetPublicKey(),
		Ip:        fmt.Sprintf("127.0.0.1:%s", node.Port),
		Index:     uint64(node.ID),
		IsMiner:   true,
		Balances:  make(map[string]*db.TokenBalance),
	}

	// åˆå§‹åŒ–FBä»£å¸ä½™é¢
	account.Balances["FB"] = &db.TokenBalance{
		Balance:            "1000000",
		MinerLockedBalance: "100000",
	}

	if err := db.SaveAccount(dbManager, account); err != nil {
		return fmt.Errorf("failed to save account: %v", err)
	}

	return nil
}

// registerAllNodes æ³¨å†Œæ‰€æœ‰èŠ‚ç‚¹ä¿¡æ¯åˆ°æ¯ä¸ªèŠ‚ç‚¹çš„æ•°æ®åº“
func registerAllNodes(nodes []*NodeInstance) {
	for i, node := range nodes {
		if node == nil || node.DBManager == nil {
			continue
		}

		// åœ¨å½“å‰èŠ‚ç‚¹çš„æ•°æ®åº“ä¸­æ³¨å†Œæ‰€æœ‰å…¶ä»–èŠ‚ç‚¹
		for j, otherNode := range nodes {
			if otherNode == nil || i == j {
				continue
			}

			// ä¿å­˜å…¶ä»–èŠ‚ç‚¹çš„è´¦æˆ·ä¿¡æ¯
			account := &db.Account{
				Address:   otherNode.Address,
				PublicKey: utils.GetKeyManager().GetPublicKey(), // è¿™é‡Œç®€åŒ–å¤„ç†
				Ip:        fmt.Sprintf("127.0.0.1:%s", otherNode.Port),
				Index:     uint64(j),
				IsMiner:   true,
				Balances:  make(map[string]*db.TokenBalance),
			}

			account.Balances["FB"] = &db.TokenBalance{
				Balance:            "1000000",
				MinerLockedBalance: "100000",
			}

			db.SaveAccount(node.DBManager, account)

			// ä¿å­˜èŠ‚ç‚¹ä¿¡æ¯
			nodeInfo := &db.NodeInfo{
				PublicKey: fmt.Sprintf("node_%d_pub", j),
				Ip:        fmt.Sprintf("127.0.0.1:%s", otherNode.Port),
				IsOnline:  true,
			}
			db.SaveNodeInfo(node.DBManager, nodeInfo)
		}
	}
}

// startHTTPServer å¯åŠ¨HTTPæœåŠ¡å™¨
func startHTTPServer(node *NodeInstance) {
	// åˆ›å»ºHTTPè·¯ç”±
	mux := http.NewServeMux()

	// è®¾ç½®handlersçš„å…±è¯†ç®¡ç†å™¨
	handlers.ConsensusManager = node.ConsensusManager

	// æ³¨å†Œè·¯ç”±
	mux.HandleFunc("/pushquery", handlers.HandlePushQuery)
	mux.HandleFunc("/pullquery", handlers.HandlePullQuery)
	mux.HandleFunc("/pullcontainer", handlers.HandlePullContainer)
	mux.HandleFunc("/status", handlers.HandleStatus)
	mux.HandleFunc("/tx", handlers.HandleTx)
	mux.HandleFunc("/getblock", handlers.HandleGetBlock)
	mux.HandleFunc("/getdata", handlers.HandleGetData)
	mux.HandleFunc("/batchgetdata", handlers.HandleBatchGetTx)
	mux.HandleFunc("/nodes", handlers.HandleNodes)

	// åº”ç”¨ä¸­é—´ä»¶
	handler := middleware.RateLimit(mux)

	// åˆ›å»ºæœåŠ¡å™¨
	server := &http.Server{
		Addr:         ":" + node.Port,
		Handler:      handler,
		ReadTimeout:  30 * time.Second,
		WriteTimeout: 30 * time.Second,
		IdleTimeout:  120 * time.Second,
	}

	node.Server = server

	// ç”Ÿæˆè‡ªç­¾åè¯ä¹¦
	certFile := fmt.Sprintf("server_%d.crt", node.ID)
	keyFile := fmt.Sprintf("server_%d.key", node.ID)

	if err := generateSelfSignedCert(certFile, keyFile); err != nil {
		logs.Error("Node %d: Failed to generate certificate: %v", node.ID, err)
		return
	}

	// å¯åŠ¨HTTPSæœåŠ¡
	logs.Info("Node %d: Starting HTTPS server on port %s", node.ID, node.Port)
	if err := server.ListenAndServeTLS(certFile, keyFile); err != http.ErrServerClosed {
		logs.Error("Node %d: Server error: %v", node.ID, err)
	}
}

// generateSelfSignedCert ç”Ÿæˆè‡ªç­¾åè¯ä¹¦
func generateSelfSignedCert(certFile, keyFile string) error {
	priv, err := ecdsa.GenerateKey(elliptic.P256(), rand.Reader)
	if err != nil {
		return err
	}

	template := x509.Certificate{
		SerialNumber: big.NewInt(1),
		DNSNames:     []string{"localhost"},
	}

	certDER, err := x509.CreateCertificate(
		rand.Reader,
		&template,
		&template,
		&priv.PublicKey,
		priv,
	)
	if err != nil {
		return err
	}

	// ä¿å­˜è¯ä¹¦
	certOut, err := os.Create(certFile)
	if err != nil {
		return err
	}
	defer certOut.Close()

	pem.Encode(certOut, &pem.Block{Type: "CERTIFICATE", Bytes: certDER})

	// ä¿å­˜ç§é’¥
	keyOut, err := os.Create(keyFile)
	if err != nil {
		return err
	}
	defer keyOut.Close()

	privBytes, err := x509.MarshalECPrivateKey(priv)
	if err != nil {
		return err
	}

	pem.Encode(keyOut, &pem.Block{Type: "EC PRIVATE KEY", Bytes: privBytes})

	return nil
}

// monitorProgress ç›‘æ§å…±è¯†è¿›åº¦
func monitorProgress(nodes []*NodeInstance) {
	ticker := time.NewTicker(10 * time.Second)
	defer ticker.Stop()

	for range ticker.C {
		var minHeight, maxHeight uint64
		activeNodes := 0

		for _, node := range nodes {
			if node == nil || node.ConsensusManager == nil {
				continue
			}

			activeNodes++
			_, height := node.ConsensusManager.GetLastAccepted()

			if minHeight == 0 || height < minHeight {
				minHeight = height
			}
			if height > maxHeight {
				maxHeight = height
			}
		}

		fmt.Printf("\nğŸ“ˆ Progress: Active=%d, MinHeight=%d, MaxHeight=%d\n",
			activeNodes, minHeight, maxHeight)

		// æ‰“å°ä¸€äº›èŠ‚ç‚¹çš„è¯¦ç»†çŠ¶æ€
		for i := 0; i < 3 && i < len(nodes); i++ {
			if nodes[i] != nil && nodes[i].ConsensusManager != nil {
				accepted, height := nodes[i].ConsensusManager.GetLastAccepted()
				fmt.Printf("  Node %d: height=%d, block=%s\n", i, height, accepted)
			}
		}
	}
}

// shutdownAllNodes å…³é—­æ‰€æœ‰èŠ‚ç‚¹
func shutdownAllNodes(nodes []*NodeInstance) {
	var wg sync.WaitGroup

	for _, node := range nodes {
		if node == nil {
			continue
		}

		wg.Add(1)
		go func(n *NodeInstance) {
			defer wg.Done()

			// åœæ­¢å…±è¯†
			if n.ConsensusManager != nil {
				n.ConsensusManager.Stop()
			}

			// åœæ­¢HTTPæœåŠ¡å™¨
			if n.Server != nil {
				ctx, cancel := context.WithTimeout(context.Background(), 5*time.Second)
				defer cancel()
				n.Server.Shutdown(ctx)
			}

			// å…³é—­æ•°æ®åº“
			if n.DBManager != nil {
				n.DBManager.Close()
			}

			fmt.Printf("  âœ“ Node %d stopped\n", n.ID)
		}(node)
	}

	wg.Wait()
}
